{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch classifier notebook\n",
    "\n",
    "V1 : only 1 split. First implementation  \n",
    "All folds V1 : with all folds  \n",
    "All folds V2 : add activation stats plot  \n",
    "All folds V3 : try 2 outputs regression and classification. Activation stats plot fixed.  \n",
    "All folds V4 : still 2 outputs regression and classification, but 2 variables on last layer, instead of 2 last layers.  \n",
    "All folds V5 : Model from kaggle : https://www.kaggle.com/c/jane-street-market-prediction/discussion/214321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import PIL.Image\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET_INPUT_FILE = 'train.csv'\n",
    "\n",
    "#FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)] + ['cross_41_42_43', 'cross_1_2']\n",
    "FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)]\n",
    "\n",
    "# For custom non-overlaped folds generation\n",
    "TRAIN_PERCENT = 0.70  \n",
    "TEST_PERCENT = 0.30\n",
    "\n",
    "#pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Behavior\n",
    "seed = 42\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Torch RNG\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)\n",
    "# CuDA Determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 50000\n",
    "#BATCH_SIZE = 4096 # Gave once better results than 50000\n",
    "#BATCH_SIZE = 2048\n",
    "\n",
    "#BATCH_SIZE = 300000\n",
    "\n",
    "#BATCH_SIZE = 500000\n",
    "\n",
    "#BATCH_SIZE = 100000\n",
    "BATCH_SIZE = 50000\n",
    "\n",
    "#BATCH_SIZE = 8192\n",
    "#BATCH_SIZE = 40960\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "#MODEL_COMMENT = \"All folds, 3 layers 2000, 1000 and 1, batch size 4096, lr=1e-4, patience 5, standard scale, 0.7 dropout, activ stats, CLASSIF+REG without sigmoid on reg\"\n",
    "MODEL_COMMENT = \"All folds, kaggle resnet, without cross features, classif, batch size 8192, weight decay 1e-5, lr=1e-4, patience 5, no standard scale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyStandardScale(tensor, mean, std):\n",
    "    return((tensor - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# this is code slightly modified from the sklearn docs here:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cv_indices_custom(cv_custom, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv_custom):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accounts for variable instance counts in each split by dividing utility_pi by number of instances (but this has been removed)\n",
    "# It also does some copy of dataframe to prevent memory overwrite\n",
    "def utility_function(df_test, df_test_predictions):\n",
    "    df_test_copy = df_test.copy(deep=True)\n",
    "    df_test_copy.loc[:, 'utility_pj'] = df_test_copy['weight'] * df_test_copy['resp'] * df_test_predictions\n",
    "    #df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum() / df_test_copy.groupby('date')['utility_pj'].count()\n",
    "    df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum()\n",
    "\n",
    "    nb_unique_dates = df_test_utility_pi.shape[0]\n",
    "    \n",
    "    if (np.sqrt(df_test_utility_pi.pow(2).sum()) == 0):\n",
    "        t = 0\n",
    "\n",
    "    else:\n",
    "        t = (df_test_utility_pi.sum() / np.sqrt(df_test_utility_pi.pow(2).sum())) * (np.sqrt(250 / np.abs(nb_unique_dates)))\n",
    "        \n",
    "    u = min(max(t, 0), 6) * df_test_utility_pi.sum()\n",
    "    del df_test_copy\n",
    "    \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def take_closest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutputActivationStats:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        #self.outputs.append(module_out)\n",
    "        self.outputs.append({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#\n",
    "#plot_cv_indices(cv, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.device"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 3090'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "    \n",
    "df = pd.read_csv(DATASET_INPUT_FILE)\n",
    "df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cross_41_42_43'] = df['feature_41'] + df['feature_42'] + df['feature_43']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cross_1_2'] = df['feature_1'] / (df['feature_2'] + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non overlap fold generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_indexes_list = df.groupby('date')['ts_id'].first().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_split_size = int((df.shape[0] // 5) * TRAIN_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_split_size = int((df.shape[0] // 5) * TEST_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 477711, 958233, 1435933, 1913985]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_start_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FOLDS = 5\n",
    "last_index = df.shape[0] - 1\n",
    "\n",
    "cv_table = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_train_start_index = train_split_start_indexes[fold_indice]\n",
    "    \n",
    "    if (fold_indice == NB_FOLDS - 1):    \n",
    "        nextfold_train_start_index = last_index\n",
    "        \n",
    "    else:\n",
    "        nextfold_train_start_index = train_split_start_indexes[fold_indice + 1]\n",
    "    \n",
    "    fold_test_start_index = take_closest(date_indexes_list, int(TRAIN_PERCENT * (nextfold_train_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    \n",
    "    cv_table.append(fold_train_start_index)\n",
    "    cv_table.append(fold_test_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_table.append(last_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples = []\n",
    "\n",
    "for i in range(0, NB_FOLDS*2, 2):\n",
    "    cv_tuples.append([df.loc[cv_table[i]:cv_table[i+1]-1, :].index.to_list(), df.loc[cv_table[i+1]:cv_table[i+2]-1, :].index.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#plot_cv_indices_custom(cv_tuples_generator, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20); \n",
    "\n",
    "#cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of training set :\n",
    "#train_sets_table =  [cv_tuples[i][0] for i in range(5)]\n",
    "#sum([len(train_set_table) for train_set_table in train_sets_table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our old time series split (with overlap : required 1 neural network trained per split)\n",
    "# But in this script it's not needed because we're training 1 unique network, with a different fold strategy (non overlaped)\n",
    "#cv = PurgedGroupTimeSeriesSplit(\n",
    "#    n_splits=5,\n",
    "#    max_train_group_size=180,\n",
    "#    group_gap=20,\n",
    "#    max_test_group_size=60\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index, test_index = next(cv.split(df, (df['resp'] > 0)*1, df['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.loc[train_index, 'resp'] > 0).astype(np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(f_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Sum of model parameters:')\n",
    "#[print(p.sum()) for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter()\n",
    "\n",
    "#writer.add_text('test', 'test:'  + str(model).replace('\\n', '<BR>'))\n",
    "\n",
    "#writer.flush()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(cv_tuples_generator):\n",
    "    folds_list.append((train_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_train = [folds_list[i][0] for i in range(5)]\n",
    "folds_list_train_flat = [folds_list_train_item for sublist in folds_list_train for folds_list_train_item in sublist]\n",
    "folds_list_train_unique = list(set(folds_list_train_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677155"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677155"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train_item) for folds_list_train_item in folds_list_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677155"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_test = [folds_list[i][1] for i in range(5)]\n",
    "folds_list_test_flat = [folds_list_test_item for sublist in folds_list_test for folds_list_test_item in sublist]\n",
    "folds_list_test_unique = set(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_test_item) for folds_list_test_item in folds_list_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train_flat) + len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141980, 130)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[folds_list_test[4], FEATURES_LIST_TOTRAIN].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 141)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Model created\n",
      "Epoch(0) - Training Loss: 0.7076\n",
      "Epoch(0) - Fold 0 - Validation Loss : 0.7062\n",
      "Epoch(0) - Fold 0 - Validation Accuracy : 0.5027\n",
      "Epoch(0) - Fold 0 - Validation Utility score : 250.5495\n",
      "Epoch(0) - Fold 1 - Validation Loss : 0.7026\n",
      "Epoch(0) - Fold 1 - Validation Accuracy : 0.4971\n",
      "Epoch(0) - Fold 1 - Validation Utility score : 33.6540\n",
      "Epoch(0) - Fold 2 - Validation Loss : 0.7057\n",
      "Epoch(0) - Fold 2 - Validation Accuracy : 0.4985\n",
      "Epoch(0) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 3 - Validation Loss : 0.7065\n",
      "Epoch(0) - Fold 3 - Validation Accuracy : 0.5008\n",
      "Epoch(0) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 4 - Validation Loss : 0.7044\n",
      "Epoch(0) - Fold 4 - Validation Accuracy : 0.5048\n",
      "Epoch(0) - Fold 4 - Validation Utility score : 237.4072\n",
      "Epoch(0) - GLOBAL - Validation Loss: 0.7051\n",
      "Epoch(0) - GLOBAL - Validation Accuracy: 0.5008\n",
      "Epoch(0) - GLOBAL - Validation Utility score: 521.6107\n",
      "Saving model corresponding to last_utility_score == 521.6106974392718\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.6973\n",
      "Epoch(1) - Fold 0 - Validation Loss : 0.6979\n",
      "Epoch(1) - Fold 0 - Validation Accuracy : 0.5020\n",
      "Epoch(1) - Fold 0 - Validation Utility score : 123.6581\n",
      "Epoch(1) - Fold 1 - Validation Loss : 0.6952\n",
      "Epoch(1) - Fold 1 - Validation Accuracy : 0.4958\n",
      "Epoch(1) - Fold 1 - Validation Utility score : 25.6666\n",
      "Epoch(1) - Fold 2 - Validation Loss : 0.6978\n",
      "Epoch(1) - Fold 2 - Validation Accuracy : 0.4997\n",
      "Epoch(1) - Fold 2 - Validation Utility score : 60.4534\n",
      "Epoch(1) - Fold 3 - Validation Loss : 0.6984\n",
      "Epoch(1) - Fold 3 - Validation Accuracy : 0.4995\n",
      "Epoch(1) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(1) - Fold 4 - Validation Loss : 0.6965\n",
      "Epoch(1) - Fold 4 - Validation Accuracy : 0.5021\n",
      "Epoch(1) - Fold 4 - Validation Utility score : 53.3124\n",
      "Epoch(1) - GLOBAL - Validation Loss: 0.6972\n",
      "Epoch(1) - GLOBAL - Validation Accuracy: 0.4998\n",
      "Epoch(1) - GLOBAL - Validation Utility score: 263.0905\n",
      "Saving model corresponding to last_utility_score == 263.0904641221415\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.6942\n",
      "Epoch(2) - Fold 0 - Validation Loss : 0.6955\n",
      "Epoch(2) - Fold 0 - Validation Accuracy : 0.5015\n",
      "Epoch(2) - Fold 0 - Validation Utility score : 59.7113\n",
      "Epoch(2) - Fold 1 - Validation Loss : 0.6933\n",
      "Epoch(2) - Fold 1 - Validation Accuracy : 0.4956\n",
      "Epoch(2) - Fold 1 - Validation Utility score : 18.4161\n",
      "Epoch(2) - Fold 2 - Validation Loss : 0.6956\n",
      "Epoch(2) - Fold 2 - Validation Accuracy : 0.4996\n",
      "Epoch(2) - Fold 2 - Validation Utility score : 92.8345\n",
      "Epoch(2) - Fold 3 - Validation Loss : 0.6959\n",
      "Epoch(2) - Fold 3 - Validation Accuracy : 0.4995\n",
      "Epoch(2) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(2) - Fold 4 - Validation Loss : 0.6942\n",
      "Epoch(2) - Fold 4 - Validation Accuracy : 0.5019\n",
      "Epoch(2) - Fold 4 - Validation Utility score : 11.0265\n",
      "Epoch(2) - GLOBAL - Validation Loss: 0.6949\n",
      "Epoch(2) - GLOBAL - Validation Accuracy: 0.4996\n",
      "Epoch(2) - GLOBAL - Validation Utility score: 181.9884\n",
      "Saving model corresponding to last_utility_score == 181.9884216165685\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.6931\n",
      "Epoch(3) - Fold 0 - Validation Loss : 0.6946\n",
      "Epoch(3) - Fold 0 - Validation Accuracy : 0.5018\n",
      "Epoch(3) - Fold 0 - Validation Utility score : 37.0287\n",
      "Epoch(3) - Fold 1 - Validation Loss : 0.6927\n",
      "Epoch(3) - Fold 1 - Validation Accuracy : 0.4957\n",
      "Epoch(3) - Fold 1 - Validation Utility score : 26.7109\n",
      "Epoch(3) - Fold 2 - Validation Loss : 0.6950\n",
      "Epoch(3) - Fold 2 - Validation Accuracy : 0.4996\n",
      "Epoch(3) - Fold 2 - Validation Utility score : 91.8008\n",
      "Epoch(3) - Fold 3 - Validation Loss : 0.6952\n",
      "Epoch(3) - Fold 3 - Validation Accuracy : 0.4995\n",
      "Epoch(3) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(3) - Fold 4 - Validation Loss : 0.6935\n",
      "Epoch(3) - Fold 4 - Validation Accuracy : 0.5020\n",
      "Epoch(3) - Fold 4 - Validation Utility score : 6.3174\n",
      "Epoch(3) - GLOBAL - Validation Loss: 0.6942\n",
      "Epoch(3) - GLOBAL - Validation Accuracy: 0.4997\n",
      "Epoch(3) - GLOBAL - Validation Utility score: 161.8578\n",
      "Saving model corresponding to last_utility_score == 161.8577860911036\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.6926\n",
      "Epoch(4) - Fold 0 - Validation Loss : 0.6944\n",
      "Epoch(4) - Fold 0 - Validation Accuracy : 0.5016\n",
      "Epoch(4) - Fold 0 - Validation Utility score : 49.7204\n",
      "Epoch(4) - Fold 1 - Validation Loss : 0.6925\n",
      "Epoch(4) - Fold 1 - Validation Accuracy : 0.4958\n",
      "Epoch(4) - Fold 1 - Validation Utility score : 14.9578\n",
      "Epoch(4) - Fold 2 - Validation Loss : 0.6947\n",
      "Epoch(4) - Fold 2 - Validation Accuracy : 0.4999\n",
      "Epoch(4) - Fold 2 - Validation Utility score : 111.6137\n",
      "Epoch(4) - Fold 3 - Validation Loss : 0.6950\n",
      "Epoch(4) - Fold 3 - Validation Accuracy : 0.4997\n",
      "Epoch(4) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(4) - Fold 4 - Validation Loss : 0.6931\n",
      "Epoch(4) - Fold 4 - Validation Accuracy : 0.5023\n",
      "Epoch(4) - Fold 4 - Validation Utility score : 16.0891\n",
      "Epoch(4) - GLOBAL - Validation Loss: 0.6939\n",
      "Epoch(4) - GLOBAL - Validation Accuracy: 0.4998\n",
      "Epoch(4) - GLOBAL - Validation Utility score: 192.3809\n",
      "Saving model corresponding to last_utility_score == 192.38091592956567\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 0.6923\n",
      "Epoch(5) - Fold 0 - Validation Loss : 0.6938\n",
      "Epoch(5) - Fold 0 - Validation Accuracy : 0.5013\n",
      "Epoch(5) - Fold 0 - Validation Utility score : 12.4951\n",
      "Epoch(5) - Fold 1 - Validation Loss : 0.6921\n",
      "Epoch(5) - Fold 1 - Validation Accuracy : 0.4957\n",
      "Epoch(5) - Fold 1 - Validation Utility score : 25.7736\n",
      "Epoch(5) - Fold 2 - Validation Loss : 0.6944\n",
      "Epoch(5) - Fold 2 - Validation Accuracy : 0.4996\n",
      "Epoch(5) - Fold 2 - Validation Utility score : 104.8301\n",
      "Epoch(5) - Fold 3 - Validation Loss : 0.6946\n",
      "Epoch(5) - Fold 3 - Validation Accuracy : 0.4994\n",
      "Epoch(5) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(5) - Fold 4 - Validation Loss : 0.6927\n",
      "Epoch(5) - Fold 4 - Validation Accuracy : 0.5021\n",
      "Epoch(5) - Fold 4 - Validation Utility score : 24.0726\n",
      "Epoch(5) - GLOBAL - Validation Loss: 0.6935\n",
      "Epoch(5) - GLOBAL - Validation Accuracy: 0.4996\n",
      "Epoch(5) - GLOBAL - Validation Utility score: 167.1713\n",
      "Saving model corresponding to last_utility_score == 167.17128410453785\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 0.6921\n",
      "Epoch(6) - Fold 0 - Validation Loss : 0.6938\n",
      "Epoch(6) - Fold 0 - Validation Accuracy : 0.5010\n",
      "Epoch(6) - Fold 0 - Validation Utility score : 13.9585\n",
      "Epoch(6) - Fold 1 - Validation Loss : 0.6920\n",
      "Epoch(6) - Fold 1 - Validation Accuracy : 0.4960\n",
      "Epoch(6) - Fold 1 - Validation Utility score : 33.9983\n",
      "Epoch(6) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(6) - Fold 2 - Validation Accuracy : 0.4997\n",
      "Epoch(6) - Fold 2 - Validation Utility score : 112.0805\n",
      "Epoch(6) - Fold 3 - Validation Loss : 0.6944\n",
      "Epoch(6) - Fold 3 - Validation Accuracy : 0.4999\n",
      "Epoch(6) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(6) - Fold 4 - Validation Loss : 0.6925\n",
      "Epoch(6) - Fold 4 - Validation Accuracy : 0.5023\n",
      "Epoch(6) - Fold 4 - Validation Utility score : 14.4915\n",
      "Epoch(6) - GLOBAL - Validation Loss: 0.6934\n",
      "Epoch(6) - GLOBAL - Validation Accuracy: 0.4998\n",
      "Epoch(6) - GLOBAL - Validation Utility score: 174.5289\n",
      "Saving model corresponding to last_utility_score == 174.52885123761763\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 0.6919\n",
      "Epoch(7) - Fold 0 - Validation Loss : 0.6936\n",
      "Epoch(7) - Fold 0 - Validation Accuracy : 0.5009\n",
      "Epoch(7) - Fold 0 - Validation Utility score : 0.3430\n",
      "Epoch(7) - Fold 1 - Validation Loss : 0.6919\n",
      "Epoch(7) - Fold 1 - Validation Accuracy : 0.4959\n",
      "Epoch(7) - Fold 1 - Validation Utility score : 65.6795\n",
      "Epoch(7) - Fold 2 - Validation Loss : 0.6942\n",
      "Epoch(7) - Fold 2 - Validation Accuracy : 0.4994\n",
      "Epoch(7) - Fold 2 - Validation Utility score : 109.0703\n",
      "Epoch(7) - Fold 3 - Validation Loss : 0.6942\n",
      "Epoch(7) - Fold 3 - Validation Accuracy : 0.5000\n",
      "Epoch(7) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(7) - Fold 4 - Validation Loss : 0.6924\n",
      "Epoch(7) - Fold 4 - Validation Accuracy : 0.5022\n",
      "Epoch(7) - Fold 4 - Validation Utility score : 0.9823\n",
      "Epoch(7) - GLOBAL - Validation Loss: 0.6932\n",
      "Epoch(7) - GLOBAL - Validation Accuracy: 0.4997\n",
      "Epoch(7) - GLOBAL - Validation Utility score: 176.0751\n",
      "Saving model corresponding to last_utility_score == 176.07505496714126\n",
      "\n",
      "\n",
      "Epoch(8) - Training Loss: 0.6918\n",
      "Epoch(8) - Fold 0 - Validation Loss : 0.6936\n",
      "Epoch(8) - Fold 0 - Validation Accuracy : 0.5006\n",
      "Epoch(8) - Fold 0 - Validation Utility score : 0.1457\n",
      "Epoch(8) - Fold 1 - Validation Loss : 0.6919\n",
      "Epoch(8) - Fold 1 - Validation Accuracy : 0.4962\n",
      "Epoch(8) - Fold 1 - Validation Utility score : 55.4038\n",
      "Epoch(8) - Fold 2 - Validation Loss : 0.6942\n",
      "Epoch(8) - Fold 2 - Validation Accuracy : 0.4993\n",
      "Epoch(8) - Fold 2 - Validation Utility score : 103.3137\n",
      "Epoch(8) - Fold 3 - Validation Loss : 0.6942\n",
      "Epoch(8) - Fold 3 - Validation Accuracy : 0.4998\n",
      "Epoch(8) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(8) - Fold 4 - Validation Loss : 0.6922\n",
      "Epoch(8) - Fold 4 - Validation Accuracy : 0.5024\n",
      "Epoch(8) - Fold 4 - Validation Utility score : 31.1790\n",
      "Epoch(8) - GLOBAL - Validation Loss: 0.6932\n",
      "Epoch(8) - GLOBAL - Validation Accuracy: 0.4997\n",
      "Epoch(8) - GLOBAL - Validation Utility score: 190.0422\n",
      "Saving model corresponding to last_utility_score == 190.04220233906054\n",
      "\n",
      "\n",
      "Epoch(9) - Training Loss: 0.6917\n",
      "Epoch(9) - Fold 0 - Validation Loss : 0.6937\n",
      "Epoch(9) - Fold 0 - Validation Accuracy : 0.5008\n",
      "Epoch(9) - Fold 0 - Validation Utility score : 1.3760\n",
      "Epoch(9) - Fold 1 - Validation Loss : 0.6919\n",
      "Epoch(9) - Fold 1 - Validation Accuracy : 0.4965\n",
      "Epoch(9) - Fold 1 - Validation Utility score : 59.6316\n",
      "Epoch(9) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(9) - Fold 2 - Validation Accuracy : 0.4993\n",
      "Epoch(9) - Fold 2 - Validation Utility score : 90.5449\n",
      "Epoch(9) - Fold 3 - Validation Loss : 0.6942\n",
      "Epoch(9) - Fold 3 - Validation Accuracy : 0.5002\n",
      "Epoch(9) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(9) - Fold 4 - Validation Loss : 0.6923\n",
      "Epoch(9) - Fold 4 - Validation Accuracy : 0.5028\n",
      "Epoch(9) - Fold 4 - Validation Utility score : 59.0591\n",
      "Epoch(9) - GLOBAL - Validation Loss: 0.6933\n",
      "Epoch(9) - GLOBAL - Validation Accuracy: 0.4999\n",
      "Epoch(9) - GLOBAL - Validation Utility score: 210.6115\n",
      "Intermediate early stopping : vepoch_loss = 0.6933, the_last_loss=0.6932\n",
      "\n",
      "\n",
      "Epoch(10) - Training Loss: 0.6916\n",
      "Epoch(10) - Fold 0 - Validation Loss : 0.6936\n",
      "Epoch(10) - Fold 0 - Validation Accuracy : 0.5008\n",
      "Epoch(10) - Fold 0 - Validation Utility score : 1.5000\n",
      "Epoch(10) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(10) - Fold 1 - Validation Accuracy : 0.4966\n",
      "Epoch(10) - Fold 1 - Validation Utility score : 58.3918\n",
      "Epoch(10) - Fold 2 - Validation Loss : 0.6942\n",
      "Epoch(10) - Fold 2 - Validation Accuracy : 0.4994\n",
      "Epoch(10) - Fold 2 - Validation Utility score : 116.0782\n",
      "Epoch(10) - Fold 3 - Validation Loss : 0.6941\n",
      "Epoch(10) - Fold 3 - Validation Accuracy : 0.5002\n",
      "Epoch(10) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(10) - Fold 4 - Validation Loss : 0.6921\n",
      "Epoch(10) - Fold 4 - Validation Accuracy : 0.5030\n",
      "Epoch(10) - Fold 4 - Validation Utility score : 69.2654\n",
      "Epoch(10) - GLOBAL - Validation Loss: 0.6932\n",
      "Epoch(10) - GLOBAL - Validation Accuracy: 0.5000\n",
      "Epoch(10) - GLOBAL - Validation Utility score: 245.2353\n",
      "Saving model corresponding to last_utility_score == 245.23528475175743\n",
      "\n",
      "\n",
      "Epoch(11) - Training Loss: 0.6915\n",
      "Epoch(11) - Fold 0 - Validation Loss : 0.6934\n",
      "Epoch(11) - Fold 0 - Validation Accuracy : 0.5011\n",
      "Epoch(11) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(11) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(11) - Fold 1 - Validation Accuracy : 0.4965\n",
      "Epoch(11) - Fold 1 - Validation Utility score : 39.2194\n",
      "Epoch(11) - Fold 2 - Validation Loss : 0.6942\n",
      "Epoch(11) - Fold 2 - Validation Accuracy : 0.4993\n",
      "Epoch(11) - Fold 2 - Validation Utility score : 114.7499\n",
      "Epoch(11) - Fold 3 - Validation Loss : 0.6939\n",
      "Epoch(11) - Fold 3 - Validation Accuracy : 0.5006\n",
      "Epoch(11) - Fold 3 - Validation Utility score : 1.4578\n",
      "Epoch(11) - Fold 4 - Validation Loss : 0.6920\n",
      "Epoch(11) - Fold 4 - Validation Accuracy : 0.5032\n",
      "Epoch(11) - Fold 4 - Validation Utility score : 71.4280\n",
      "Epoch(11) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(11) - GLOBAL - Validation Accuracy: 0.5001\n",
      "Epoch(11) - GLOBAL - Validation Utility score: 226.8551\n",
      "Saving model corresponding to last_utility_score == 226.855077892942\n",
      "\n",
      "\n",
      "Epoch(12) - Training Loss: 0.6915\n",
      "Epoch(12) - Fold 0 - Validation Loss : 0.6935\n",
      "Epoch(12) - Fold 0 - Validation Accuracy : 0.5009\n",
      "Epoch(12) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(12) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(12) - Fold 1 - Validation Accuracy : 0.4966\n",
      "Epoch(12) - Fold 1 - Validation Utility score : 48.3794\n",
      "Epoch(12) - Fold 2 - Validation Loss : 0.6942\n",
      "Epoch(12) - Fold 2 - Validation Accuracy : 0.4993\n",
      "Epoch(12) - Fold 2 - Validation Utility score : 139.2093\n",
      "Epoch(12) - Fold 3 - Validation Loss : 0.6940\n",
      "Epoch(12) - Fold 3 - Validation Accuracy : 0.5004\n",
      "Epoch(12) - Fold 3 - Validation Utility score : 0.0415\n",
      "Epoch(12) - Fold 4 - Validation Loss : 0.6920\n",
      "Epoch(12) - Fold 4 - Validation Accuracy : 0.5034\n",
      "Epoch(12) - Fold 4 - Validation Utility score : 87.5358\n",
      "Epoch(12) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(12) - GLOBAL - Validation Accuracy: 0.5001\n",
      "Epoch(12) - GLOBAL - Validation Utility score: 275.1661\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6931\n",
      "\n",
      "\n",
      "Epoch(13) - Training Loss: 0.6914\n",
      "Epoch(13) - Fold 0 - Validation Loss : 0.6935\n",
      "Epoch(13) - Fold 0 - Validation Accuracy : 0.5008\n",
      "Epoch(13) - Fold 0 - Validation Utility score : 0.0126\n",
      "Epoch(13) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(13) - Fold 1 - Validation Accuracy : 0.4966\n",
      "Epoch(13) - Fold 1 - Validation Utility score : 32.7830\n",
      "Epoch(13) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(13) - Fold 2 - Validation Accuracy : 0.4993\n",
      "Epoch(13) - Fold 2 - Validation Utility score : 84.9026\n",
      "Epoch(13) - Fold 3 - Validation Loss : 0.6940\n",
      "Epoch(13) - Fold 3 - Validation Accuracy : 0.5007\n",
      "Epoch(13) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(13) - Fold 4 - Validation Loss : 0.6919\n",
      "Epoch(13) - Fold 4 - Validation Accuracy : 0.5040\n",
      "Epoch(13) - Fold 4 - Validation Utility score : 135.2040\n",
      "Epoch(13) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(13) - GLOBAL - Validation Accuracy: 0.5003\n",
      "Epoch(13) - GLOBAL - Validation Utility score: 252.9023\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6931\n",
      "\n",
      "\n",
      "Epoch(14) - Training Loss: 0.6914\n",
      "Epoch(14) - Fold 0 - Validation Loss : 0.6935\n",
      "Epoch(14) - Fold 0 - Validation Accuracy : 0.5010\n",
      "Epoch(14) - Fold 0 - Validation Utility score : 1.0290\n",
      "Epoch(14) - Fold 1 - Validation Loss : 0.6917\n",
      "Epoch(14) - Fold 1 - Validation Accuracy : 0.4967\n",
      "Epoch(14) - Fold 1 - Validation Utility score : 58.1813\n",
      "Epoch(14) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(14) - Fold 2 - Validation Accuracy : 0.4990\n",
      "Epoch(14) - Fold 2 - Validation Utility score : 75.7101\n",
      "Epoch(14) - Fold 3 - Validation Loss : 0.6939\n",
      "Epoch(14) - Fold 3 - Validation Accuracy : 0.5008\n",
      "Epoch(14) - Fold 3 - Validation Utility score : 0.0044\n",
      "Epoch(14) - Fold 4 - Validation Loss : 0.6919\n",
      "Epoch(14) - Fold 4 - Validation Accuracy : 0.5039\n",
      "Epoch(14) - Fold 4 - Validation Utility score : 106.7507\n",
      "Epoch(14) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(14) - GLOBAL - Validation Accuracy: 0.5003\n",
      "Epoch(14) - GLOBAL - Validation Utility score: 241.6756\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6931\n",
      "\n",
      "\n",
      "Epoch(15) - Training Loss: 0.6913\n",
      "Epoch(15) - Fold 0 - Validation Loss : 0.6934\n",
      "Epoch(15) - Fold 0 - Validation Accuracy : 0.5014\n",
      "Epoch(15) - Fold 0 - Validation Utility score : 0.0482\n",
      "Epoch(15) - Fold 1 - Validation Loss : 0.6917\n",
      "Epoch(15) - Fold 1 - Validation Accuracy : 0.4968\n",
      "Epoch(15) - Fold 1 - Validation Utility score : 37.3876\n",
      "Epoch(15) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(15) - Fold 2 - Validation Accuracy : 0.4993\n",
      "Epoch(15) - Fold 2 - Validation Utility score : 100.3709\n",
      "Epoch(15) - Fold 3 - Validation Loss : 0.6940\n",
      "Epoch(15) - Fold 3 - Validation Accuracy : 0.5006\n",
      "Epoch(15) - Fold 3 - Validation Utility score : 0.0530\n",
      "Epoch(15) - Fold 4 - Validation Loss : 0.6919\n",
      "Epoch(15) - Fold 4 - Validation Accuracy : 0.5042\n",
      "Epoch(15) - Fold 4 - Validation Utility score : 136.9074\n",
      "Epoch(15) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(15) - GLOBAL - Validation Accuracy: 0.5005\n",
      "Epoch(15) - GLOBAL - Validation Utility score: 274.7671\n",
      "Saving model corresponding to last_utility_score == 274.76711038512474\n",
      "\n",
      "\n",
      "Epoch(16) - Training Loss: 0.6912\n",
      "Epoch(16) - Fold 0 - Validation Loss : 0.6934\n",
      "Epoch(16) - Fold 0 - Validation Accuracy : 0.5014\n",
      "Epoch(16) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(16) - Fold 1 - Validation Loss : 0.6917\n",
      "Epoch(16) - Fold 1 - Validation Accuracy : 0.4969\n",
      "Epoch(16) - Fold 1 - Validation Utility score : 73.1238\n",
      "Epoch(16) - Fold 2 - Validation Loss : 0.6944\n",
      "Epoch(16) - Fold 2 - Validation Accuracy : 0.4988\n",
      "Epoch(16) - Fold 2 - Validation Utility score : 56.0912\n",
      "Epoch(16) - Fold 3 - Validation Loss : 0.6939\n",
      "Epoch(16) - Fold 3 - Validation Accuracy : 0.5008\n",
      "Epoch(16) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(16) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(16) - Fold 4 - Validation Accuracy : 0.5045\n",
      "Epoch(16) - Fold 4 - Validation Utility score : 129.3068\n",
      "Epoch(16) - GLOBAL - Validation Loss: 0.6930\n",
      "Epoch(16) - GLOBAL - Validation Accuracy: 0.5005\n",
      "Epoch(16) - GLOBAL - Validation Utility score: 258.5218\n",
      "Saving model corresponding to last_utility_score == 258.52184189121755\n",
      "\n",
      "\n",
      "Epoch(17) - Training Loss: 0.6912\n",
      "Epoch(17) - Fold 0 - Validation Loss : 0.6934\n",
      "Epoch(17) - Fold 0 - Validation Accuracy : 0.5017\n",
      "Epoch(17) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(17) - Fold 1 - Validation Loss : 0.6917\n",
      "Epoch(17) - Fold 1 - Validation Accuracy : 0.4971\n",
      "Epoch(17) - Fold 1 - Validation Utility score : 85.3413\n",
      "Epoch(17) - Fold 2 - Validation Loss : 0.6944\n",
      "Epoch(17) - Fold 2 - Validation Accuracy : 0.4990\n",
      "Epoch(17) - Fold 2 - Validation Utility score : 72.3629\n",
      "Epoch(17) - Fold 3 - Validation Loss : 0.6940\n",
      "Epoch(17) - Fold 3 - Validation Accuracy : 0.5005\n",
      "Epoch(17) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(17) - Fold 4 - Validation Loss : 0.6919\n",
      "Epoch(17) - Fold 4 - Validation Accuracy : 0.5047\n",
      "Epoch(17) - Fold 4 - Validation Utility score : 144.9928\n",
      "Epoch(17) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(17) - GLOBAL - Validation Accuracy: 0.5006\n",
      "Epoch(17) - GLOBAL - Validation Utility score: 302.6970\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6930\n",
      "\n",
      "\n",
      "Epoch(18) - Training Loss: 0.6911\n",
      "Epoch(18) - Fold 0 - Validation Loss : 0.6936\n",
      "Epoch(18) - Fold 0 - Validation Accuracy : 0.5017\n",
      "Epoch(18) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(18) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(18) - Fold 1 - Validation Accuracy : 0.4972\n",
      "Epoch(18) - Fold 1 - Validation Utility score : 49.7060\n",
      "Epoch(18) - Fold 2 - Validation Loss : 0.6945\n",
      "Epoch(18) - Fold 2 - Validation Accuracy : 0.4988\n",
      "Epoch(18) - Fold 2 - Validation Utility score : 60.3982\n",
      "Epoch(18) - Fold 3 - Validation Loss : 0.6938\n",
      "Epoch(18) - Fold 3 - Validation Accuracy : 0.5010\n",
      "Epoch(18) - Fold 3 - Validation Utility score : 0.7740\n",
      "Epoch(18) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(18) - Fold 4 - Validation Accuracy : 0.5050\n",
      "Epoch(18) - Fold 4 - Validation Utility score : 113.2640\n",
      "Epoch(18) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(18) - GLOBAL - Validation Accuracy: 0.5007\n",
      "Epoch(18) - GLOBAL - Validation Utility score: 224.1421\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6930\n",
      "\n",
      "\n",
      "Epoch(19) - Training Loss: 0.6911\n",
      "Epoch(19) - Fold 0 - Validation Loss : 0.6935\n",
      "Epoch(19) - Fold 0 - Validation Accuracy : 0.5016\n",
      "Epoch(19) - Fold 0 - Validation Utility score : 1.8637\n",
      "Epoch(19) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(19) - Fold 1 - Validation Accuracy : 0.4971\n",
      "Epoch(19) - Fold 1 - Validation Utility score : 55.4526\n",
      "Epoch(19) - Fold 2 - Validation Loss : 0.6946\n",
      "Epoch(19) - Fold 2 - Validation Accuracy : 0.4987\n",
      "Epoch(19) - Fold 2 - Validation Utility score : 56.4028\n",
      "Epoch(19) - Fold 3 - Validation Loss : 0.6939\n",
      "Epoch(19) - Fold 3 - Validation Accuracy : 0.5011\n",
      "Epoch(19) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(19) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(19) - Fold 4 - Validation Accuracy : 0.5056\n",
      "Epoch(19) - Fold 4 - Validation Utility score : 188.4604\n",
      "Epoch(19) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(19) - GLOBAL - Validation Accuracy: 0.5008\n",
      "Epoch(19) - GLOBAL - Validation Utility score: 302.1794\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6930\n",
      "\n",
      "\n",
      "Epoch(20) - Training Loss: 0.6911\n",
      "Epoch(20) - Fold 0 - Validation Loss : 0.6933\n",
      "Epoch(20) - Fold 0 - Validation Accuracy : 0.5020\n",
      "Epoch(20) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(20) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(20) - Fold 1 - Validation Accuracy : 0.4973\n",
      "Epoch(20) - Fold 1 - Validation Utility score : 81.7280\n",
      "Epoch(20) - Fold 2 - Validation Loss : 0.6944\n",
      "Epoch(20) - Fold 2 - Validation Accuracy : 0.4988\n",
      "Epoch(20) - Fold 2 - Validation Utility score : 67.4361\n",
      "Epoch(20) - Fold 3 - Validation Loss : 0.6937\n",
      "Epoch(20) - Fold 3 - Validation Accuracy : 0.5009\n",
      "Epoch(20) - Fold 3 - Validation Utility score : 0.4104\n",
      "Epoch(20) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(20) - Fold 4 - Validation Accuracy : 0.5047\n",
      "Epoch(20) - Fold 4 - Validation Utility score : 92.9560\n",
      "Epoch(20) - GLOBAL - Validation Loss: 0.6930\n",
      "Epoch(20) - GLOBAL - Validation Accuracy: 0.5008\n",
      "Epoch(20) - GLOBAL - Validation Utility score: 242.5305\n",
      "Saving model corresponding to last_utility_score == 242.53050704757175\n",
      "\n",
      "\n",
      "Epoch(21) - Training Loss: 0.6910\n",
      "Epoch(21) - Fold 0 - Validation Loss : 0.6936\n",
      "Epoch(21) - Fold 0 - Validation Accuracy : 0.5019\n",
      "Epoch(21) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(21) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(21) - Fold 1 - Validation Accuracy : 0.4976\n",
      "Epoch(21) - Fold 1 - Validation Utility score : 59.7939\n",
      "Epoch(21) - Fold 2 - Validation Loss : 0.6946\n",
      "Epoch(21) - Fold 2 - Validation Accuracy : 0.4988\n",
      "Epoch(21) - Fold 2 - Validation Utility score : 57.8927\n",
      "Epoch(21) - Fold 3 - Validation Loss : 0.6939\n",
      "Epoch(21) - Fold 3 - Validation Accuracy : 0.5010\n",
      "Epoch(21) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(21) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(21) - Fold 4 - Validation Accuracy : 0.5055\n",
      "Epoch(21) - Fold 4 - Validation Utility score : 127.9024\n",
      "Epoch(21) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(21) - GLOBAL - Validation Accuracy: 0.5010\n",
      "Epoch(21) - GLOBAL - Validation Utility score: 245.5890\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6930\n",
      "\n",
      "\n",
      "Epoch(22) - Training Loss: 0.6910\n",
      "Epoch(22) - Fold 0 - Validation Loss : 0.6937\n",
      "Epoch(22) - Fold 0 - Validation Accuracy : 0.5019\n",
      "Epoch(22) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(22) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(22) - Fold 1 - Validation Accuracy : 0.4979\n",
      "Epoch(22) - Fold 1 - Validation Utility score : 46.7758\n",
      "Epoch(22) - Fold 2 - Validation Loss : 0.6947\n",
      "Epoch(22) - Fold 2 - Validation Accuracy : 0.4990\n",
      "Epoch(22) - Fold 2 - Validation Utility score : 55.5449\n",
      "Epoch(22) - Fold 3 - Validation Loss : 0.6939\n",
      "Epoch(22) - Fold 3 - Validation Accuracy : 0.5010\n",
      "Epoch(22) - Fold 3 - Validation Utility score : 0.0049\n",
      "Epoch(22) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(22) - Fold 4 - Validation Accuracy : 0.5062\n",
      "Epoch(22) - Fold 4 - Validation Utility score : 200.4484\n",
      "Epoch(22) - GLOBAL - Validation Loss: 0.6932\n",
      "Epoch(22) - GLOBAL - Validation Accuracy: 0.5012\n",
      "Epoch(22) - GLOBAL - Validation Utility score: 302.7739\n",
      "Intermediate early stopping : vepoch_loss = 0.6932, the_last_loss=0.6930\n",
      "\n",
      "\n",
      "Epoch(23) - Training Loss: 0.6909\n",
      "Epoch(23) - Fold 0 - Validation Loss : 0.6934\n",
      "Epoch(23) - Fold 0 - Validation Accuracy : 0.5017\n",
      "Epoch(23) - Fold 0 - Validation Utility score : -0.0000\n",
      "Epoch(23) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(23) - Fold 1 - Validation Accuracy : 0.4974\n",
      "Epoch(23) - Fold 1 - Validation Utility score : 47.7195\n",
      "Epoch(23) - Fold 2 - Validation Loss : 0.6946\n",
      "Epoch(23) - Fold 2 - Validation Accuracy : 0.4983\n",
      "Epoch(23) - Fold 2 - Validation Utility score : 54.5025\n",
      "Epoch(23) - Fold 3 - Validation Loss : 0.6937\n",
      "Epoch(23) - Fold 3 - Validation Accuracy : 0.5010\n",
      "Epoch(23) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(23) - Fold 4 - Validation Loss : 0.6917\n",
      "Epoch(23) - Fold 4 - Validation Accuracy : 0.5054\n",
      "Epoch(23) - Fold 4 - Validation Utility score : 142.7389\n",
      "Epoch(23) - GLOBAL - Validation Loss: 0.6930\n",
      "Epoch(23) - GLOBAL - Validation Accuracy: 0.5007\n",
      "Epoch(23) - GLOBAL - Validation Utility score: 244.9609\n",
      "Intermediate early stopping : vepoch_loss = 0.6930, the_last_loss=0.6930\n",
      "\n",
      "\n",
      "Epoch(24) - Training Loss: 0.6909\n",
      "Epoch(24) - Fold 0 - Validation Loss : 0.6936\n",
      "Epoch(24) - Fold 0 - Validation Accuracy : 0.5019\n",
      "Epoch(24) - Fold 0 - Validation Utility score : 9.5564\n",
      "Epoch(24) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(24) - Fold 1 - Validation Accuracy : 0.4978\n",
      "Epoch(24) - Fold 1 - Validation Utility score : 83.4809\n",
      "Epoch(24) - Fold 2 - Validation Loss : 0.6947\n",
      "Epoch(24) - Fold 2 - Validation Accuracy : 0.4985\n",
      "Epoch(24) - Fold 2 - Validation Utility score : 34.0115\n",
      "Epoch(24) - Fold 3 - Validation Loss : 0.6939\n",
      "Epoch(24) - Fold 3 - Validation Accuracy : 0.5011\n",
      "Epoch(24) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(24) - Fold 4 - Validation Loss : 0.6917\n",
      "Epoch(24) - Fold 4 - Validation Accuracy : 0.5060\n",
      "Epoch(24) - Fold 4 - Validation Utility score : 197.2146\n",
      "Epoch(24) - GLOBAL - Validation Loss: 0.6931\n",
      "Epoch(24) - GLOBAL - Validation Accuracy: 0.5011\n",
      "Epoch(24) - GLOBAL - Validation Utility score: 324.2634\n",
      "Intermediate early stopping : vepoch_loss = 0.6931, the_last_loss=0.6930\n",
      "\n",
      "\n",
      "Epoch(25) - Training Loss: 0.6908\n",
      "Epoch(25) - Fold 0 - Validation Loss : 0.6936\n",
      "Epoch(25) - Fold 0 - Validation Accuracy : 0.5017\n",
      "Epoch(25) - Fold 0 - Validation Utility score : 8.1931\n",
      "Epoch(25) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(25) - Fold 1 - Validation Accuracy : 0.4981\n",
      "Epoch(25) - Fold 1 - Validation Utility score : 95.5697\n",
      "Epoch(25) - Fold 2 - Validation Loss : 0.6949\n",
      "Epoch(25) - Fold 2 - Validation Accuracy : 0.4980\n",
      "Epoch(25) - Fold 2 - Validation Utility score : 9.7968\n",
      "Epoch(25) - Fold 3 - Validation Loss : 0.6938\n",
      "Epoch(25) - Fold 3 - Validation Accuracy : 0.5010\n",
      "Epoch(25) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(25) - Fold 4 - Validation Loss : 0.6916\n",
      "Epoch(25) - Fold 4 - Validation Accuracy : 0.5063\n",
      "Epoch(25) - Fold 4 - Validation Utility score : 202.7185\n",
      "Epoch(25) - GLOBAL - Validation Loss: 0.6932\n",
      "Epoch(25) - GLOBAL - Validation Accuracy: 0.5010\n",
      "Epoch(25) - GLOBAL - Validation Utility score: 316.2781\n",
      "Intermediate early stopping : vepoch_loss = 0.6932, the_last_loss=0.6930\n",
      "Meet Early stopping!\n",
      "Training summary:\n",
      "{'utility_score': 242.53050704757175, 'utility_scores': [-0.0, 81.72797964221571, 67.43607880289662, 0.41043319667192407, 92.95601540578745], 'utility_score_std': 40.25883074926143, 'accuracy_scores': [0.5019772930220692, 0.4973043173043173, 0.49880879636940356, 0.5009426529348866, 0.5047471474855614]}\n"
     ]
    }
   ],
   "source": [
    "print('Training started')\n",
    "patience=5\n",
    "label_smoothing = 1e-2\n",
    "\n",
    "utility_scores = [None] * 5\n",
    "accuracy_scores = [None] * 5\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "ts_train = torch.tensor(df.loc[folds_list_train_unique, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "ts_train_y = torch.tensor((df.loc[folds_list_train_unique, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "ts_train_y_reg = torch.tensor(df.loc[folds_list_train_unique, 'resp'].to_numpy(), device='cuda')\n",
    "\n",
    "# Normalize data\n",
    "ts_train_mean = torch.mean(ts_train, axis=0)\n",
    "ts_train_std = torch.std(ts_train, axis=0)\n",
    "#ts_train = pyStandardScale(ts_train, ts_train_mean, ts_train_std)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(ts_train, ts_train_y, ts_train_y_reg)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) # pin_memory : VOIR RESULTAT\n",
    "\n",
    "ts_test = [None] * 5\n",
    "ts_test_y = [None] * 5    \n",
    "ts_test_y_reg = [None] * 5   \n",
    "test_dataset = [None] * 5\n",
    "test_loader = [None] * 5\n",
    "\n",
    "for fold_indice in range(5):\n",
    "    ts_test[fold_indice] = torch.tensor(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    ts_test_y[fold_indice] = torch.tensor((df.loc[folds_list_test[fold_indice], 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "    ts_test_y_reg[fold_indice] = torch.tensor(df.loc[folds_list_test[fold_indice], 'resp'].to_numpy(), device='cuda')\n",
    "\n",
    "    # Normalize\n",
    "    #ts_test[fold_indice] = pyStandardScale(ts_test[fold_indice], ts_train_mean, ts_train_std)\n",
    "    \n",
    "    test_dataset[fold_indice] = torch.utils.data.TensorDataset(ts_test[fold_indice], ts_test_y[fold_indice], ts_test_y_reg[fold_indice])\n",
    "    test_loader[fold_indice] = torch.utils.data.DataLoader(test_dataset[fold_indice], batch_size=BATCH_SIZE)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "'''\n",
    "model = nn.Sequential(\n",
    "        #nn.Dropout(0.2),\n",
    "        nn.Linear(len(FEATURES_LIST_TOTRAIN), 200),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.7),\n",
    "\n",
    "        nn.Linear(200, 100),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.7),\n",
    "    \n",
    "        nn.Linear(100, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).double().to('cuda')\n",
    "'''\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    #def __init__(self, n_feature, n_hidden): \n",
    "    def __init__(self): \n",
    "        super(MLP, self).__init__() \n",
    "\n",
    "        #nn.Dropout(0.2),\n",
    "        self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN), 130)\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(130)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer2 = nn.Linear(130, 60)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(130)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer3 = nn.Linear(60, 1)\n",
    "        \n",
    "        #self.layer11_classif = nn.Linear(30, 1)\n",
    "        #self.act11_classif = nn.Sigmoid()  \n",
    "\n",
    "        #self.layer11_reg = nn.Linear(30, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.act1(self.layer1(x)))\n",
    "        x = self.dropout2(self.act2(self.layer2(x)))\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        #x_out_classif = nn.Sigmoid()(x[:, 0]).unsqueeze(-1)\n",
    "        #x_out_reg = x[:, 1].unsqueeze(-1)\n",
    "\n",
    "        #x_out_classif = nn.Sigmoid()(x[:, 0])\n",
    "        #x_out_reg = x[:, 1]\n",
    "        x_out_reg = x\n",
    "\n",
    "        #return x_out_classif, x_out_reg\n",
    "        return x_out_reg\n",
    "\n",
    "class Model_Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_Resnet, self).__init__()\n",
    "        self.batch_norm0 = nn.BatchNorm1d(len(FEATURES_LIST_TOTRAIN))\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "\n",
    "        dropout_rate = 0.2\n",
    "        hidden_size = 256\n",
    "        self.dense1 = nn.Linear(len(FEATURES_LIST_TOTRAIN), hidden_size)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense2 = nn.Linear(hidden_size+len(FEATURES_LIST_TOTRAIN), hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        #self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n",
    "        self.dense5 = nn.Linear(hidden_size+hidden_size, 1)\n",
    "\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.act4 = nn.LeakyReLU()\n",
    "        self.actfinal = nn.Sigmoid()\n",
    "        \n",
    "        ##self.Relu = nn.ReLU(inplace=True)\n",
    "        ##self.PReLU = nn.PReLU()\n",
    "        ##self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.GeLU = nn.GELU()\n",
    "        ##self.RReLU = nn.RReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x1 = self.dense1(x)\n",
    "        x1 = self.batch_norm1(x1)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        x1 = self.act1(x1)\n",
    "        x1 = self.dropout1(x1)\n",
    "\n",
    "        x = torch.cat([x, x1], 1)\n",
    "\n",
    "        x2 = self.dense2(x)\n",
    "        x2 = self.batch_norm2(x2)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        x2 = self.act2(x2)\n",
    "        x2 = self.dropout2(x2)\n",
    "\n",
    "        x = torch.cat([x1, x2], 1)\n",
    "\n",
    "        x3 = self.dense3(x)\n",
    "        x3 = self.batch_norm3(x3)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        x3 = self.act3(x3)\n",
    "        x3 = self.dropout3(x3)\n",
    "\n",
    "        x = torch.cat([x2, x3], 1)\n",
    "\n",
    "        x4 = self.dense4(x)\n",
    "        x4 = self.batch_norm4(x4)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.PReLU(x)\n",
    "        x4 = self.act4(x4)\n",
    "        x4 = self.dropout4(x4)\n",
    "\n",
    "        x = torch.cat([x3, x4], 1)\n",
    "\n",
    "        x = self.actfinal(self.dense5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "#model = MLP().double().to('cuda')\n",
    "model = Model_Resnet().double().to('cuda')\n",
    "    \n",
    "print('Model created')\n",
    "\n",
    "#print('Number of model parameters :')\n",
    "#numel_list = [p.numel() for p in model.parameters()]\n",
    "#sum(numel_list), numel_list\n",
    "\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "#loss_fn = nn.BCELoss().to('cuda')\n",
    "loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n",
    "\n",
    "\n",
    "#loss_fn_reg = nn.MSELoss().to('cuda')\n",
    "\n",
    "#def MSE_SignedLoss(output, target):\n",
    "    #return((nn.LeakyReLU()(-(output*target)) / torch.abs(output*target)) * loss_fn_reg(output, target))\n",
    "\n",
    "def MSE_SignedLoss(output, target):\n",
    "    return(\n",
    "        torch.sqrt(\n",
    "            torch.mean(\n",
    "                (nn.LeakyReLU()(-(output*target)) / torch.abs(output*target)) * torch.pow(output - target, 2)\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) \n",
    "\n",
    "model.eval()\n",
    "#start_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "#start_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "#print('Start Validation Accuracy: {:.4f}'.format(start_accuracy))\n",
    "#print('Start Validation Utility: {:.4f}'.format(start_utility_score))\n",
    "\n",
    "### Call back to save activation stats (mean, std dev and near 0 values after activation functions)\n",
    "\n",
    "Val_Loss = 0\n",
    "N_Samples = 0\n",
    "\n",
    "the_last_loss = 100\n",
    "the_last_utility_score = 0\n",
    "the_last_accuracy = 0\n",
    "trigger_times=0\n",
    "early_stopping_met = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    running_loss = 0.0        \n",
    "    \n",
    "    # Setting hook for activation layers stats\n",
    "    hook_handles = []\n",
    "    save_output_activation_stats = []\n",
    "\n",
    "    for layer in model.modules():\n",
    "        if ('activation' in str(type(layer))):\n",
    "            save_output_activation_stats_1layer = SaveOutputActivationStats()\n",
    "            handle = layer.register_forward_hook(save_output_activation_stats_1layer)\n",
    "            save_output_activation_stats.append(save_output_activation_stats_1layer)\n",
    "            hook_handles.append(handle)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        #inputs, labels = batch[0], batch[1]\n",
    "        inputs, labels, labels_reg = batch[0].to('cuda'), batch[1].to('cuda'), batch[2].to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            #outputs_classif, outputs_reg = model(inputs)\n",
    "            #outputs_reg = model(inputs)\n",
    "            outputs_classif = model(inputs)\n",
    "            loss_classif = loss_fn(outputs_classif, labels.unsqueeze(-1).double())\n",
    "            \n",
    "            #loss_classif = 0\n",
    "            #loss_reg = MSE_SignedLoss(outputs_reg, labels_reg.unsqueeze(-1).double()) * torch.tensor(864.625) # Coefficient to balance reg loss which is much smaller\n",
    "            #loss_reg = MSE_SignedLoss(outputs_reg, labels_reg.unsqueeze(-1).double()) # Coefficient to balance reg loss which is much smaller\n",
    "            #loss_reg = loss_fn_reg(outputs_reg.unsqueeze(-1), labels_reg.unsqueeze(-1).double()) # Coefficient to balance reg loss which is much smaller\n",
    "            \n",
    "            #loss = loss_classif + loss_reg\n",
    "            #loss = loss_reg\n",
    "            loss = loss_classif\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # update local train loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # update global train loss\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print('Epoch({}) - Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "    writer.add_scalar(f\"Global train/loss\", epoch_loss, epoch)\n",
    "\n",
    "    # Write activation stats graphs\n",
    "    for layer_number,save_output_activation_stats_layer in enumerate(save_output_activation_stats):\n",
    "        df_stats_layer = pd.DataFrame(save_output_activation_stats_layer.outputs)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(25, 4))\n",
    "\n",
    "        ax[0].set_title(f'Layer {layer_number} : Mean activation value', fontsize=16)\n",
    "        ax[0].set_xlabel('Batch instances')\n",
    "        ax[0].set_ylabel('Mean')\n",
    "        ax[0].plot(range(df_stats_layer.shape[0]), df_stats_layer['mean'])\n",
    "\n",
    "        ax[1].set_title(f'Layer {layer_number} : Std deviation activation value', fontsize=16)\n",
    "        ax[1].set_xlabel('Batch instances')\n",
    "        ax[1].set_ylabel('Standard deviation')\n",
    "        ax[1].plot(range(df_stats_layer.shape[0]), df_stats_layer['std'])\n",
    "\n",
    "        ax[2].set_title(f'Layer {layer_number} : Percentage of activation values near zero', fontsize=16)\n",
    "        ax[2].set_xlabel('Batch instances')\n",
    "        ax[2].set_ylabel('Percentage')\n",
    "        ax[2].plot(range(df_stats_layer.shape[0]), df_stats_layer['near_zero']);\n",
    "        \n",
    "        plot_buf = io.BytesIO()\n",
    "        plt.savefig(plot_buf, format='jpeg')\n",
    "        plt.close()\n",
    "        \n",
    "        plot_buf.seek(0)\n",
    "        image = PIL.Image.open(plot_buf)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        writer.add_image(\"Train activation stats/Activation stats layer \" + str(layer_number), image, epoch)\n",
    "    \n",
    "    # Validation \n",
    "    model.eval()\n",
    "\n",
    "    vrunning_loss = [None] * 5\n",
    "    vrunning_loss_classif = [None] * 5\n",
    "    vrunning_loss_reg = [None] * 5\n",
    "    num_samples = [None] * 5\n",
    "    vepoch_loss_folds = [None] * 5\n",
    "    vepoch_loss_classif_folds = [None] * 5\n",
    "    vepoch_loss_reg_folds = [None] * 5\n",
    "    vepoch_accuracy_folds = [None] * 5\n",
    "    vepoch_utility_score_folds = [None] * 5\n",
    "    \n",
    "    for fold_indice in range(5):    \n",
    "        vrunning_loss[fold_indice] = 0.0\n",
    "        vrunning_loss_classif[fold_indice] = 0.0\n",
    "        vrunning_loss_reg[fold_indice] = 0.0\n",
    "        num_samples[fold_indice] = 0\n",
    "\n",
    "        for batch in test_loader[fold_indice]:\n",
    "            inputs, labels, labels_reg = batch[0].to('cuda'), batch[1].to('cuda'), batch[2].to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                #outputs_classif, outputs_reg = model(inputs)\n",
    "                outputs_classif = model(inputs)\n",
    "                loss_classif = loss_fn(outputs_classif, labels.unsqueeze(-1).double())\n",
    "\n",
    "                #loss_reg = loss_fn_reg(outputs_reg, torch.sigmoid(labels_reg.unsqueeze(-1).double()))\n",
    "                #loss_reg = loss_fn_reg(outputs_reg.unsqueeze(-1), labels_reg.unsqueeze(-1).double()) # Coefficient to balance reg loss which is much smaller\n",
    "                #loss_reg = MSE_SignedLoss(outputs_reg, labels_reg.unsqueeze(-1).double()) * torch.tensor(864.625) # Coefficient to balance reg loss which is much smaller\n",
    "                #loss_reg = MSE_SignedLoss(outputs_reg, labels_reg.unsqueeze(-1).double()) # Coefficient to balance reg loss which is much smaller\n",
    "                \n",
    "                #loss = loss_classif + loss_reg \n",
    "                loss = loss_classif \n",
    "                #loss = loss_reg \n",
    "                \n",
    "            vrunning_loss[fold_indice] += loss.item() * inputs.size(0)\n",
    "            num_samples[fold_indice] += labels.size(0)\n",
    "            \n",
    "            vrunning_loss_classif[fold_indice] += loss_classif.item() * inputs.size(0)\n",
    "            #running_loss_reg[fold_indice] += loss_reg.item() * inputs.size(0)\n",
    "            \n",
    "            vepoch_loss_folds[fold_indice] = vrunning_loss[fold_indice] / num_samples[fold_indice]\n",
    "            vepoch_loss_classif_folds[fold_indice] = vrunning_loss_classif[fold_indice] / num_samples[fold_indice]\n",
    "            #epoch_loss_reg_folds[fold_indice] = vrunning_loss_reg[fold_indice] / num_samples[fold_indice]\n",
    "\n",
    "        print('Epoch({}) - Fold {} - Validation Loss : {:.4f}'.format(epoch, fold_indice, vepoch_loss_folds[fold_indice]))\n",
    "        #print('Epoch({}) - Fold {} -> Validation Loss Classif : {:.4f}'.format(epoch, fold_indice, vepoch_loss_classif_folds[fold_indice]))\n",
    "        #print('Epoch({}) - Fold {} -> Validation Loss Reg : {:.4f}'.format(epoch, fold_indice, vepoch_loss_reg_folds[fold_indice]))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vepoch_accuracy_folds[fold_indice] = accuracy_score(ts_test_y[fold_indice].cpu().numpy(), (model(ts_test[fold_indice]).squeeze() > 0.5).cpu().numpy())  # model...[0] is the classification output of model\n",
    "            vepoch_utility_score_folds[fold_indice] = utility_function(df.loc[folds_list_test[fold_indice]], (model(ts_test[fold_indice]).squeeze() > 0.5).cpu().numpy())\n",
    "        print('Epoch({}) - Fold {} - Validation Accuracy : {:.4f}'.format(epoch, fold_indice, vepoch_accuracy_folds[fold_indice]))\n",
    "        print('Epoch({}) - Fold {} - Validation Utility score : {:.4f}'.format(epoch, fold_indice, vepoch_utility_score_folds[fold_indice]))\n",
    "        \n",
    "            \n",
    "    # update epoch loss\n",
    "    vepoch_loss = sum(vepoch_loss_folds) / len(vepoch_loss_folds)\n",
    "    vepoch_accuracy = sum(vepoch_accuracy_folds) / len(vepoch_accuracy_folds)\n",
    "    vepoch_utility_score = sum(vepoch_utility_score_folds) #/ len(vepoch_utility_score_folds)\n",
    "    print('Epoch({}) - GLOBAL - Validation Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "    print('Epoch({}) - GLOBAL - Validation Accuracy: {:.4f}'.format(epoch, vepoch_accuracy))\n",
    "    print('Epoch({}) - GLOBAL - Validation Utility score: {:.4f}'.format(epoch, vepoch_utility_score))\n",
    "\n",
    "    #print(f'Sum of model parameters ({epoch}):')\n",
    "    #[print(p.sum()) for p in model.parameters()]\n",
    "\n",
    "    writer.add_scalar(\"Global valid/Loss\", vepoch_loss, epoch)\n",
    "    writer.add_scalar(\"Global valid/Accuracy\", vepoch_accuracy, epoch)\n",
    "    writer.add_scalar(\"Global valid/Utility\", vepoch_utility_score, epoch)\n",
    "\n",
    "    for fold_indice in range(5):\n",
    "        writer.add_scalar(\"Fold valid Loss/Loss fold \"+str(fold_indice), vepoch_loss_folds[fold_indice], epoch)\n",
    "        writer.add_scalar(\"Fold valid Accuracy/Accuracy fold \"+str(fold_indice), vepoch_accuracy_folds[fold_indice], epoch)\n",
    "        writer.add_scalar(\"Fold valid Utility/Utility fold \"+str(fold_indice), vepoch_utility_score_folds[fold_indice], epoch)\n",
    "        \n",
    "    \n",
    "    writer.flush()\n",
    "\n",
    "    #if (epoch == 7):\n",
    "        #print('EPOCH 7 ATTAINED')\n",
    "        #break\n",
    "    \n",
    "    # Check if Early Stopping\n",
    "    #if vepoch_loss > the_last_loss:\n",
    "    #if (vepoch_utility_score < the_last_utility_score) and (vepoch_loss > the_last_loss) and (vepoch_accuracy < the_last_accuracy):\n",
    "    if (vepoch_loss > the_last_loss):\n",
    "        trigger_times += 1\n",
    "\n",
    "        print(f'Intermediate early stopping : vepoch_loss = {vepoch_loss:.4f}, the_last_loss={the_last_loss:.4f}')\n",
    "        #print(f'Intermediate early stopping : vepoch_accuracy = {vepoch_accuracy:.4f}, the_last_utility_score={the_last_accuracy:.4f}')\n",
    "        #print(f'Intermediate early stopping : vepoch_utility_score = {vepoch_utility_score:.4f}, the_last_utility_score={the_last_utility_score:.4f}')\n",
    "\n",
    "        if trigger_times >= patience:\n",
    "            print('Meet Early stopping!')\n",
    "            early_stopping_met = True\n",
    "            ##torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "            break\n",
    "    else:\n",
    "        trigger_times = 0\n",
    "        the_last_loss = vepoch_loss\n",
    "        the_last_utility_score = vepoch_utility_score\n",
    "        the_last_accuracy = vepoch_accuracy\n",
    "        \n",
    "        the_last_utility_score_folds = vepoch_utility_score_folds\n",
    "        the_last_accuracy_folds = vepoch_accuracy_folds\n",
    "        \n",
    "        the_best_epoch = epoch\n",
    "\n",
    "        # Save model for the best version so far\n",
    "        print(f'Saving model corresponding to last_utility_score == {the_last_utility_score}')\n",
    "        torch.save(model.state_dict(), f'model_NN_allfolds_V1.pt')\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "if (early_stopping_met == False):\n",
    "    print(\"Didn't meet early stopping : saving final model\")\n",
    "    # Save model if don't meet early stopping\n",
    "    torch.save(model.state_dict(), f'model_NN_allfolds_V1.pt')\n",
    "\n",
    "#utility_scores.append(the_last_utility_score)\n",
    "#accuracy_scores.append(the_last_accuracy)\n",
    "writer.add_text(f\"Global valid/Utility\", f\"Best utility: {the_last_utility_score}\", the_best_epoch)\n",
    "        \n",
    "scores_results = {'utility_score': the_last_utility_score, 'utility_scores': the_last_utility_score_folds, 'utility_score_std': np.std(the_last_utility_score_folds), 'accuracy_scores': the_last_accuracy_folds}\n",
    "\n",
    "writer.add_text('Final utility score', str(scores_results))\n",
    "writer.add_text('Batch size', str(BATCH_SIZE))\n",
    "writer.add_text('Patience', str(patience))\n",
    "writer.add_text('Number of epochs', str(NUM_EPOCHS))\n",
    "writer.add_text('Number of parameters per layer', str([p.numel() for p in model.parameters()]))\n",
    "writer.add_text('Model architecture', str(model).replace('\\n', '<BR>'))\n",
    "writer.add_text('Comment', MODEL_COMMENT)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print('Training summary:')\n",
    "print(scores_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vepoch_loss_reg_folds[fold_indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeakyReLU(negative_slope=0.01)\n",
      "LeakyReLU(negative_slope=0.01)\n",
      "LeakyReLU(negative_slope=0.01)\n",
      "LeakyReLU(negative_slope=0.01)\n",
      "Sigmoid()\n"
     ]
    }
   ],
   "source": [
    "for layer in model.modules():\n",
    "    if ('activation' in str(type(layer))):\n",
    "        print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mean': 0.06618570669301244,\n",
       "  'std': 0.12045511479785051,\n",
       "  'near_zero': 0.68656},\n",
       " {'mean': 0.06547741800634416,\n",
       "  'std': 0.11799794368985213,\n",
       "  'near_zero': 0.68742},\n",
       " {'mean': 0.06657497575316271,\n",
       "  'std': 0.12137901910311018,\n",
       "  'near_zero': 0.68508},\n",
       " {'mean': 0.06731119555120808,\n",
       "  'std': 0.12206110280806262,\n",
       "  'near_zero': 0.68542},\n",
       " {'mean': 0.06897175735430384,\n",
       "  'std': 0.1263957021922444,\n",
       "  'near_zero': 0.68514},\n",
       " {'mean': 0.07181969551699932,\n",
       "  'std': 0.13363219690358952,\n",
       "  'near_zero': 0.68352},\n",
       " {'mean': 0.07457552371267859,\n",
       "  'std': 0.13899772875287916,\n",
       "  'near_zero': 0.67952},\n",
       " {'mean': 0.07537183304904381,\n",
       "  'std': 0.13867310007568426,\n",
       "  'near_zero': 0.67746},\n",
       " {'mean': 0.07572386272832687,\n",
       "  'std': 0.13712492232879478,\n",
       "  'near_zero': 0.67382},\n",
       " {'mean': 0.07535796347673133,\n",
       "  'std': 0.13536153398535805,\n",
       "  'near_zero': 0.6764},\n",
       " {'mean': 0.07403449311129486,\n",
       "  'std': 0.13283270266522146,\n",
       "  'near_zero': 0.6762},\n",
       " {'mean': 0.07038238025389097,\n",
       "  'std': 0.12542212594464397,\n",
       "  'near_zero': 0.68132},\n",
       " {'mean': 0.06842003831582222,\n",
       "  'std': 0.12110104022169477,\n",
       "  'near_zero': 0.6846},\n",
       " {'mean': 0.06614628482434572,\n",
       "  'std': 0.11749203588915441,\n",
       "  'near_zero': 0.69038},\n",
       " {'mean': 0.06508378944074152,\n",
       "  'std': 0.11700506328678584,\n",
       "  'near_zero': 0.69352},\n",
       " {'mean': 0.06522883844218717,\n",
       "  'std': 0.11753761053204835,\n",
       "  'near_zero': 0.69238},\n",
       " {'mean': 0.06621375550575943,\n",
       "  'std': 0.12236035578372291,\n",
       "  'near_zero': 0.69772},\n",
       " {'mean': 0.06592501895361641,\n",
       "  'std': 0.12422117116624933,\n",
       "  'near_zero': 0.70282},\n",
       " {'mean': 0.06596984765947389,\n",
       "  'std': 0.12566432106073447,\n",
       "  'near_zero': 0.70392},\n",
       " {'mean': 0.0672623850095921,\n",
       "  'std': 0.12950202811264175,\n",
       "  'near_zero': 0.70472},\n",
       " {'mean': 0.06800083389208254,\n",
       "  'std': 0.13240061106856055,\n",
       "  'near_zero': 0.7058},\n",
       " {'mean': 0.068595276499267, 'std': 0.13255734088105944, 'near_zero': 0.7022},\n",
       " {'mean': 0.06871881227579438,\n",
       "  'std': 0.12997952511574665,\n",
       "  'near_zero': 0.6958},\n",
       " {'mean': 0.06904512526411573,\n",
       "  'std': 0.12888926472272463,\n",
       "  'near_zero': 0.69456},\n",
       " {'mean': 0.07038279272597282,\n",
       "  'std': 0.12861753131330145,\n",
       "  'near_zero': 0.68836},\n",
       " {'mean': 0.06995393792291006,\n",
       "  'std': 0.12732247711881964,\n",
       "  'near_zero': 0.68812},\n",
       " {'mean': 0.06912625964353838,\n",
       "  'std': 0.12381101017134208,\n",
       "  'near_zero': 0.68354},\n",
       " {'mean': 0.06877782168008324,\n",
       "  'std': 0.12241102578265159,\n",
       "  'near_zero': 0.68392},\n",
       " {'mean': 0.06913178280536489,\n",
       "  'std': 0.12366237944912373,\n",
       "  'near_zero': 0.6832},\n",
       " {'mean': 0.07187692395269189,\n",
       "  'std': 0.12829789585473114,\n",
       "  'near_zero': 0.67646},\n",
       " {'mean': 0.07143011562805607,\n",
       "  'std': 0.12763514024758052,\n",
       "  'near_zero': 0.6777},\n",
       " {'mean': 0.07107612005068087,\n",
       "  'std': 0.12881372002524943,\n",
       "  'near_zero': 0.68222},\n",
       " {'mean': 0.07087408607279978,\n",
       "  'std': 0.1292780550972065,\n",
       "  'near_zero': 0.68216},\n",
       " {'mean': 0.07232082398911796,\n",
       "  'std': 0.13414925335919006,\n",
       "  'near_zero': 0.6821211563248021},\n",
       " {'mean': 0.066889254535925, 'std': 0.13449782605187005, 'near_zero': 0.71258},\n",
       " {'mean': 0.07363005886658175, 'std': 0.1390349267136944, 'near_zero': 0.6811},\n",
       " {'mean': 0.0698876278324208,\n",
       "  'std': 0.12941668634572562,\n",
       "  'near_zero': 0.6897717872609606},\n",
       " {'mean': 0.07015129031236372,\n",
       "  'std': 0.13470945905900392,\n",
       "  'near_zero': 0.6947810803532196},\n",
       " {'mean': 0.07015129031236372,\n",
       "  'std': 0.13470945905900392,\n",
       "  'near_zero': 0.6947810803532196},\n",
       " {'mean': 0.06113501548310861,\n",
       "  'std': 0.12211462509409872,\n",
       "  'near_zero': 0.73204},\n",
       " {'mean': 0.05361353435048467,\n",
       "  'std': 0.11289093798782344,\n",
       "  'near_zero': 0.75836},\n",
       " {'mean': 0.055146366049258815,\n",
       "  'std': 0.10952798511810902,\n",
       "  'near_zero': 0.7410129564193169},\n",
       " {'mean': 0.05671036178256864,\n",
       "  'std': 0.11529562655391708,\n",
       "  'near_zero': 0.743952263952264},\n",
       " {'mean': 0.05671036178256864,\n",
       "  'std': 0.11529562655391708,\n",
       "  'near_zero': 0.743952263952264},\n",
       " {'mean': 0.06451496318548959,\n",
       "  'std': 0.11968226321128098,\n",
       "  'near_zero': 0.7012},\n",
       " {'mean': 0.05757022337917325,\n",
       "  'std': 0.11944204319991822,\n",
       "  'near_zero': 0.74102},\n",
       " {'mean': 0.05920700880324713,\n",
       "  'std': 0.12141074477113652,\n",
       "  'near_zero': 0.7312873759610962},\n",
       " {'mean': 0.06046727069697121,\n",
       "  'std': 0.12018129152330716,\n",
       "  'near_zero': 0.7242998674914694},\n",
       " {'mean': 0.06046727069697121,\n",
       "  'std': 0.12018129152330716,\n",
       "  'near_zero': 0.7242998674914694},\n",
       " {'mean': 0.05531634733259494,\n",
       "  'std': 0.10996973516288261,\n",
       "  'near_zero': 0.7385},\n",
       " {'mean': 0.05561322038453003,\n",
       "  'std': 0.11122571645354529,\n",
       "  'near_zero': 0.73878},\n",
       " {'mean': 0.05450429029410499,\n",
       "  'std': 0.10941481881554055,\n",
       "  'near_zero': 0.7369757069652686},\n",
       " {'mean': 0.055179970359818135,\n",
       "  'std': 0.1102497115480717,\n",
       "  'near_zero': 0.7381464910799708},\n",
       " {'mean': 0.055179970359818135,\n",
       "  'std': 0.1102497115480717,\n",
       "  'near_zero': 0.7381464910799708},\n",
       " {'mean': 0.05239198168243943,\n",
       "  'std': 0.10139829668114778,\n",
       "  'near_zero': 0.74226},\n",
       " {'mean': 0.05551416152077697,\n",
       "  'std': 0.1090992163631849,\n",
       "  'near_zero': 0.73458},\n",
       " {'mean': 0.05431102750020317,\n",
       "  'std': 0.10623013666867905,\n",
       "  'near_zero': 0.7396617436874702},\n",
       " {'mean': 0.05405891255254772,\n",
       "  'std': 0.10559684587130445,\n",
       "  'near_zero': 0.7387871531201577},\n",
       " {'mean': 0.05405891255254772,\n",
       "  'std': 0.10559684587130445,\n",
       "  'near_zero': 0.7387871531201577}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_output_activation_stats_layer.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>near_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.066186</td>\n",
       "      <td>0.120455</td>\n",
       "      <td>0.686560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.065477</td>\n",
       "      <td>0.117998</td>\n",
       "      <td>0.687420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066575</td>\n",
       "      <td>0.121379</td>\n",
       "      <td>0.685080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.067311</td>\n",
       "      <td>0.122061</td>\n",
       "      <td>0.685420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.068972</td>\n",
       "      <td>0.126396</td>\n",
       "      <td>0.685140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.071820</td>\n",
       "      <td>0.133632</td>\n",
       "      <td>0.683520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.074576</td>\n",
       "      <td>0.138998</td>\n",
       "      <td>0.679520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.075372</td>\n",
       "      <td>0.138673</td>\n",
       "      <td>0.677460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.075724</td>\n",
       "      <td>0.137125</td>\n",
       "      <td>0.673820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.075358</td>\n",
       "      <td>0.135362</td>\n",
       "      <td>0.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.074034</td>\n",
       "      <td>0.132833</td>\n",
       "      <td>0.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.070382</td>\n",
       "      <td>0.125422</td>\n",
       "      <td>0.681320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.068420</td>\n",
       "      <td>0.121101</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.066146</td>\n",
       "      <td>0.117492</td>\n",
       "      <td>0.690380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.065084</td>\n",
       "      <td>0.117005</td>\n",
       "      <td>0.693520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.065229</td>\n",
       "      <td>0.117538</td>\n",
       "      <td>0.692380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.066214</td>\n",
       "      <td>0.122360</td>\n",
       "      <td>0.697720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.065925</td>\n",
       "      <td>0.124221</td>\n",
       "      <td>0.702820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.065970</td>\n",
       "      <td>0.125664</td>\n",
       "      <td>0.703920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.067262</td>\n",
       "      <td>0.129502</td>\n",
       "      <td>0.704720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.068001</td>\n",
       "      <td>0.132401</td>\n",
       "      <td>0.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.068595</td>\n",
       "      <td>0.132557</td>\n",
       "      <td>0.702200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.068719</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.069045</td>\n",
       "      <td>0.128889</td>\n",
       "      <td>0.694560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.070383</td>\n",
       "      <td>0.128618</td>\n",
       "      <td>0.688360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.069954</td>\n",
       "      <td>0.127322</td>\n",
       "      <td>0.688120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.069126</td>\n",
       "      <td>0.123811</td>\n",
       "      <td>0.683540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.068778</td>\n",
       "      <td>0.122411</td>\n",
       "      <td>0.683920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.069132</td>\n",
       "      <td>0.123662</td>\n",
       "      <td>0.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.071877</td>\n",
       "      <td>0.128298</td>\n",
       "      <td>0.676460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.071430</td>\n",
       "      <td>0.127635</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.071076</td>\n",
       "      <td>0.128814</td>\n",
       "      <td>0.682220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.070874</td>\n",
       "      <td>0.129278</td>\n",
       "      <td>0.682160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.072321</td>\n",
       "      <td>0.134149</td>\n",
       "      <td>0.682121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean       std  near_zero\n",
       "0   0.066186  0.120455   0.686560\n",
       "1   0.065477  0.117998   0.687420\n",
       "2   0.066575  0.121379   0.685080\n",
       "3   0.067311  0.122061   0.685420\n",
       "4   0.068972  0.126396   0.685140\n",
       "5   0.071820  0.133632   0.683520\n",
       "6   0.074576  0.138998   0.679520\n",
       "7   0.075372  0.138673   0.677460\n",
       "8   0.075724  0.137125   0.673820\n",
       "9   0.075358  0.135362   0.676400\n",
       "10  0.074034  0.132833   0.676200\n",
       "11  0.070382  0.125422   0.681320\n",
       "12  0.068420  0.121101   0.684600\n",
       "13  0.066146  0.117492   0.690380\n",
       "14  0.065084  0.117005   0.693520\n",
       "15  0.065229  0.117538   0.692380\n",
       "16  0.066214  0.122360   0.697720\n",
       "17  0.065925  0.124221   0.702820\n",
       "18  0.065970  0.125664   0.703920\n",
       "19  0.067262  0.129502   0.704720\n",
       "20  0.068001  0.132401   0.705800\n",
       "21  0.068595  0.132557   0.702200\n",
       "22  0.068719  0.129980   0.695800\n",
       "23  0.069045  0.128889   0.694560\n",
       "24  0.070383  0.128618   0.688360\n",
       "25  0.069954  0.127322   0.688120\n",
       "26  0.069126  0.123811   0.683540\n",
       "27  0.068778  0.122411   0.683920\n",
       "28  0.069132  0.123662   0.683200\n",
       "29  0.071877  0.128298   0.676460\n",
       "30  0.071430  0.127635   0.677700\n",
       "31  0.071076  0.128814   0.682220\n",
       "32  0.070874  0.129278   0.682160\n",
       "33  0.072321  0.134149   0.682121"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_stats_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -2.1321,  ...,  0.5874,  4.3042],\n",
       "        [-1.0000,  0.6492,  ...,  1.5986,  0.5584],\n",
       "        ...,\n",
       "        [-1.0000,  0.2672,  ..., -1.7266,  1.3048],\n",
       "        [-1.0000,  0.4237,  ..., -1.2025, -1.4359]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_test[fold_indice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_load = nn.Sequential(\n",
    "        #nn.Dropout(0.2),\n",
    "        nn.Linear(len(FEATURES_LIST_TOTRAIN), 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 60),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(60, 30),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "       \n",
    "        nn.Linear(30, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).double().to('cuda')\n",
    "    \n",
    "model_load.load_state_dict(torch.load(f'model_NN_allfolds_V1.pt',map_location=torch.device('cuda')))\n",
    "'''\n",
    "\n",
    "#model_load.eval()\n",
    "#print(accuracy_score(ts_test_y.cpu().numpy(), (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n",
    "#\n",
    "#model_load.eval()\n",
    "#print(utility_function(df.loc[test_index], (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, fill NA with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, normalize with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        print(type(module_out))\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 =  nn.Sequential(\n",
    "        nn.Linear(4, 2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(2, 2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "\n",
    "    \n",
    "        nn.Linear(2, 1),\n",
    "        nn.Sigmoid(),\n",
    "\n",
    ")\n",
    "\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param_object in enumerate(model2.parameters()):\n",
    "    print(f'Object {i}')\n",
    "    print(param_object.shape)\n",
    "    print(param_object.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'activation' in str(type(model2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(model2[1], torch.nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output = SaveOutput()\n",
    "\n",
    "hook_handles = []\n",
    "\n",
    "for layer in model2.modules():\n",
    "    if ('activation' in str(type(layer))):\n",
    "        handle = layer.register_forward_hook(save_output)\n",
    "        hook_handles.append(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "print(model2(torch.tensor([1, 1, 1, 2], dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation.append([0.11, 01.11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour récupérer les gradients : https://discuss.pytorch.org/t/how-to-print-the-computed-gradient-values-for-a-network/34179\n",
    "print(model2[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
