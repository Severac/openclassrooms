{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch classifier notebook\n",
    "\n",
    "V1 : only 1 split. First implementation  \n",
    "All folds V1 : with all folds  \n",
    "All folds V2 : add activation stats plot  \n",
    "All folds V2.1 : back to  best MLP found so far, and backport fix of activation layers stats. Add weight decay and scheduler (fit one cycle) code\n",
    "\n",
    "All folds autoencoder MLP V1  \n",
    "All folds autoencoder MLP V2 : with weights and biases  \n",
    "V4 : Xgb resp n-1 predictor and fold predictor as input and Pytorch NN-Resnet as meta model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torch.optim as optim\n",
    "import torch_optimizer as optim  # Custom optimizers (not officially pytorch) : to use RAdam https://pypi.org/project/torch-optimizer/#radam\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import PIL.Image\n",
    "\n",
    "import datetime\n",
    "\n",
    "import faiss\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET_INPUT_FILE = 'train.csv'\n",
    "\n",
    "#FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)] + ['cross_41_42_43', 'cross_1_2']\n",
    "FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)]\n",
    "\n",
    "# For custom non-overlaped folds generation\n",
    "TRAIN_PERCENT = 0.70  \n",
    "TEST_PERCENT = 0.30\n",
    "\n",
    "# If subsplit of training set : percentage of second training set  \n",
    "TRAIN1_PERCENT = 0.20  \n",
    "\n",
    "ACT_N = False  # Add N previous predictions to input of MLP <= Does not work, logic is not right\n",
    "ACT_N_SIZE = 5\n",
    "\n",
    "CLUSTERING = False\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Behavior\n",
    "seed = 42\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Torch RNG\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)\n",
    "# CuDA Determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_SWEEP = True\n",
    "DO_SINGLE_TRAIN = False\n",
    "#BATCH_SIZE = 50000\n",
    "#BATCH_SIZE = 4096 # Gave once better results than 50000\n",
    "#BATCH_SIZE = 2048\n",
    "\n",
    "#BATCH_SIZE = 300000\n",
    "\n",
    "#BATCH_SIZE = 4096\n",
    "#BATCH_SIZE = 8192\n",
    "#BATCH_SIZE = 32768\n",
    "BATCH_SIZE = 195837\n",
    "WEIGHT_DECAY = 0.00012 # Remettre à 1e-5\n",
    "LEARNING_RATE = 0.0008602\n",
    "DROPOUT = 0.4019\n",
    "USE_AUTOENC = 'None'\n",
    "\n",
    "EARLY_STOPPING = True\n",
    "\n",
    "NUM_EPOCHS = 1000\n",
    "#NUM_EPOCHS = 36\n",
    "\n",
    "MODEL_FILE = f'model_NN_allfolds_V1.pt'\n",
    "\n",
    "BATCH_SIZE_AE = 40960\n",
    "NUM_EPOCHS_AE = 1000\n",
    "LEARNING_RATE_AE = 1e-3\n",
    "WEIGHT_DECAY_AE = 1e-4\n",
    "MODEL_FILE_AE = f'model_NN_AE_allfolds_V1.pt'\n",
    "\n",
    "RETRAIN_MODEl_AE = False\n",
    "\n",
    "MODEL_COMMENT_AE = f'All folds MLP autoenc, 2 layers 64 32, good model reloaded, batch size {BATCH_SIZE_AE}, lr={LEARNING_RATE_AE}, patience 5, standard scale, weight decay {WEIGHT_DECAY_AE}, dropout 0.5, with cross features, no scheduler, no std scale'\n",
    "MODEL_COMMENT = f'All folds MLP withOUT autoenc  5 layers 130, 200, 100, 75, 50 (instead of 130, 200 and 100) , good model reloaded, batch size {BATCH_SIZE}, lr={LEARNING_RATE}, patience 20, standard scale, weight decay {WEIGHT_DECAY}, 0.7 dropout, without cross features, no scheduler, no std scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'Best utility',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [524288, 262144, 131072, 65536, 32768, 16384, 8192, 4096, 2048, 1024, 512]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            #'values': [1e-2, 1e-3, 1e-4, 3e-4, 1e-5]\n",
    "            #'values': [1e-2, 1e-3, 1e-4]\n",
    "            'values': [1e-2, 1e-3]\n",
    "        },\n",
    "\n",
    "        'weight_decay': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        },\n",
    "    \n",
    "        'use_autoenc': {\n",
    "            'values': ['encoder-decoder', 'encoder', 'encoder-only', 'None']\n",
    "            #'values': ['encoder-decoder', 'None']\n",
    "        },\n",
    "        \n",
    "        'activation_function': {\n",
    "            'values': ['relu', 'leakyrelu']\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'Best utility',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'min': 4096,\n",
    "            'max': 300000,\n",
    "            \n",
    "            \n",
    "            #'min': 300000,\n",
    "            #'max': 300001,\n",
    "            'distribution': 'int_uniform',\n",
    "        },\n",
    "        'dropout': {\n",
    "            'min': 0.3,\n",
    "            'max': 0.5,\n",
    "            'distribution': 'uniform',\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 0.0005,\n",
    "            'max': 0.002,\n",
    "            'distribution': 'uniform',\n",
    "        },\n",
    "\n",
    "        'weight_decay': {\n",
    "            'min': 0.00001,\n",
    "            'max': 0.0002,\n",
    "            'distribution': 'uniform',\n",
    "\n",
    "        },\n",
    "    \n",
    "        'use_autoenc': {\n",
    "            #'values': ['encoder-only']\n",
    "            #'values': ['encoder', 'encoder-only']\n",
    "            #'values': ['encoder-decoder', 'None']\n",
    "            'values': ['None']\n",
    "        },\n",
    "        \n",
    "        'activation_function': {\n",
    "            'values': ['leakyrelu']\n",
    "        },\n",
    "\n",
    "        \n",
    "        'hidden_size': {\n",
    "            'min': 128,\n",
    "            'max': 512,\n",
    "            #'max': 256,\n",
    "            'distribution': 'int_uniform',\n",
    "        },\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfboyer\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    memory = torch.cuda.memory_allocated('cuda')\n",
    "    print(\"{:.3f} GB\".format(memory / 1024 ** 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyStandardScale(tensor, mean, std):\n",
    "    return((tensor - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# this is code slightly modified from the sklearn docs here:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cv_indices_custom(cv_custom, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv_custom):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accounts for variable instance counts in each split by dividing utility_pi by number of instances (but this has been removed)\n",
    "# It also does some copy of dataframe to prevent memory overwrite\n",
    "def utility_function(df_test, df_test_predictions):\n",
    "    df_test_copy = df_test.copy(deep=True)\n",
    "    df_test_copy.loc[:, 'utility_pj'] = df_test_copy['weight'] * df_test_copy['resp'] * df_test_predictions\n",
    "    #df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum() / df_test_copy.groupby('date')['utility_pj'].count()\n",
    "    df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum()\n",
    "\n",
    "    nb_unique_dates = df_test_utility_pi.shape[0]\n",
    "\n",
    "    if (np.sqrt(df_test_utility_pi.pow(2).sum()) == 0):\n",
    "        t = 0\n",
    "\n",
    "    else:\n",
    "        t = (df_test_utility_pi.sum() / np.sqrt(df_test_utility_pi.pow(2).sum())) * (np.sqrt(250 / np.abs(nb_unique_dates)))\n",
    "\n",
    "    u = min(max(t, 0), 6) * df_test_utility_pi.sum()\n",
    "    del df_test_copy\n",
    "    \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "# The aim of this function is to return closest date from an index\n",
    "# So that split indices correspond to start or end of a new day\n",
    "# myList contains list of instances that correspond to start of a new da\n",
    "\n",
    "def take_closest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutputActivationStats:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        #self.outputs.append(module_out)\n",
    "        #print('Save output callback :')\n",
    "        #print(module)\n",
    "        #print({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        self.outputs.append({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#\n",
    "#plot_cv_indices(cv, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.device"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 3090'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "    \n",
    "df = pd.read_csv(DATASET_INPUT_FILE)\n",
    "df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_41_42_43'] = df['feature_41'] + df['feature_42'] + df['feature_43']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_1_2'] = df['feature_1'] / (df['feature_2'] + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non overlap fold generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_indexes_list = df.groupby('date')['ts_id'].first().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_split_size = int((df.shape[0] // 5) * TRAIN_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_split_size = int((df.shape[0] // 5) * TEST_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 477711, 958233, 1435933, 1913985]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_start_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have 5 folds of 3 subsets each (2 training sets and 1 test set per fold)\n",
    "# (1st training set of each fold will be used for 1st model, ie auto encoder)\n",
    "\n",
    "NB_FOLDS = 5\n",
    "last_index = df.shape[0] - 1\n",
    "\n",
    "cv_table = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_train_start_index = train_split_start_indexes[fold_indice]\n",
    "    \n",
    "    if (fold_indice == NB_FOLDS - 1):    \n",
    "        nextfold_train_start_index = last_index\n",
    "        \n",
    "    else:\n",
    "        nextfold_train_start_index = train_split_start_indexes[fold_indice + 1]\n",
    "    \n",
    "    fold_test_start_index = take_closest(date_indexes_list, int(TRAIN_PERCENT * (nextfold_train_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    fold_train2_start_index = take_closest(date_indexes_list, int(TRAIN1_PERCENT * (fold_test_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    \n",
    "    cv_table.append(fold_train_start_index)\n",
    "    cv_table.append(fold_train2_start_index)\n",
    "    cv_table.append(fold_test_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_table.append(last_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples = []\n",
    "\n",
    "for i in range(0, NB_FOLDS*3, 3):\n",
    "    cv_tuples.append([df.loc[cv_table[i]:cv_table[i+1]-1, :].index.to_list(), df.loc[cv_table[i+1]:cv_table[i+2]-1, :].index.to_list(),\n",
    "                      df.loc[cv_table[i+2]:cv_table[i+3]-1, :].index.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141102"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_tuples[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#plot_cv_indices_custom(cv_tuples_generator, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20); \n",
    "\n",
    "#cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of training set :\n",
    "#train_sets_table =  [cv_tuples[i][0] for i in range(5)]\n",
    "#sum([len(train_set_table) for train_set_table in train_sets_table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our old time series split (with overlap : required 1 neural network trained per split)\n",
    "# But in this script it's not needed because we're training 1 unique network, with a different fold strategy (non overlaped)\n",
    "#cv = PurgedGroupTimeSeriesSplit(\n",
    "#    n_splits=5,\n",
    "#    max_train_group_size=180,\n",
    "#    group_gap=20,\n",
    "#    max_test_group_size=60\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index, test_index = next(cv.split(df, (df['resp'] > 0)*1, df['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.loc[train_index, 'resp'] > 0).astype(np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(f_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Sum of model parameters:')\n",
    "#[print(p.sum()) for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter()\n",
    "\n",
    "#writer.add_text('test', 'test:'  + str(model).replace('\\n', '<BR>'))\n",
    "\n",
    "#writer.flush()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list = []\n",
    "\n",
    "for fold, (train1_index, train2_index, test_index) in enumerate(cv_tuples_generator):\n",
    "    folds_list.append((train1_index, train2_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_train1 = [folds_list[i][0] for i in range(5)]\n",
    "folds_list_train1_flat = [folds_list_train1_item for sublist in folds_list_train1 for folds_list_train1_item in sublist]\n",
    "folds_list_train1_unique = list(set(folds_list_train1_flat))\n",
    "\n",
    "folds_list_train2 = [folds_list[i][1] for i in range(5)]\n",
    "folds_list_train2_flat = [folds_list_train2_item for sublist in folds_list_train2 for folds_list_train2_item in sublist]\n",
    "folds_list_train2_unique = list(set(folds_list_train2_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train2_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train1_item) for folds_list_train1_item in folds_list_train1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train2_item) for folds_list_train2_item in folds_list_train2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_test = [folds_list[i][2] for i in range(5)]\n",
    "folds_list_test_flat = [folds_list_test_item for sublist in folds_list_test for folds_list_test_item in sublist]\n",
    "folds_list_test_unique = set(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_test_item) for folds_list_test_item in folds_list_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat) + len(folds_list_train2_flat) + len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141980, 130)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[folds_list_test[4], FEATURES_LIST_TOTRAIN].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00880718,  0.39574469,  0.33059838,  0.00919269,  0.00341737,\n",
       "       -0.00498373, -0.01455459,  0.05534631,  0.02511896,  0.2646538 ,\n",
       "        0.16705702,  0.09489698,  0.04450428,  0.15251293,  0.07996651,\n",
       "        0.22166532,  0.12827658,  0.12181565,  0.10958852,  0.29772963,\n",
       "        0.26463247,  0.1881408 ,  0.17251055,  0.25474009,  0.23267903,\n",
       "        0.29794049,  0.2685417 ,  0.13985131,  0.16285107,  0.33060734,\n",
       "        0.34385913,  0.22684687,  0.25190658,  0.31637359,  0.3359838 ,\n",
       "        0.35284181,  0.36773315,  0.02650339,  0.0186391 ,  0.04320553,\n",
       "        0.05298663,  0.45417433,  0.37762691,  0.41617323,  0.43927675,\n",
       "        0.48651095,  0.49207956,  0.36839975,  0.50144387,  0.54379067,\n",
       "        0.53074971,  0.45673965,  0.05646874,  0.38900233,  0.37690587,\n",
       "        0.77549302,  0.92466193,  0.78590429,  0.80847667,  0.89895923,\n",
       "        0.55335406,  0.55554392,  0.55922873,  0.56139559,  0.44231975,\n",
       "        0.61884351,  0.61715568,  0.59770334,  0.59814018,  0.37738388,\n",
       "        0.23893403,  0.30802914,  0.00410365, -0.03220141, -0.00163732,\n",
       "       -0.01991575, -0.03158872, -0.0931838 , -0.00806526, -0.03578937,\n",
       "       -0.00251814, -0.01489434, -0.03498338, -0.10154564,  0.39337805,\n",
       "        0.54162178,  0.39241949,  0.42814332,  0.49755557,  0.39935045,\n",
       "        0.43319566,  0.52353302,  0.42238168,  0.42206715,  0.43484953,\n",
       "        0.4547188 ,  0.39837193,  0.5421566 ,  0.39730999,  0.42589424,\n",
       "        0.48653787,  0.41054099,  0.43399339,  0.48391166,  0.41683186,\n",
       "        0.41979739,  0.46144612,  0.455339  ,  0.39499643,  0.38242161,\n",
       "        0.39000896,  0.391784  ,  0.38065447,  0.40349802,  0.43203717,\n",
       "        0.37668634,  0.42790068,  0.42074866,  0.39520648,  0.44651147,\n",
       "        0.36161378,  0.29991827,  0.37089359,  0.30351037,  0.36007002,\n",
       "        0.27782367,  0.3742626 ,  0.25712366,  0.37390404,  0.268233  ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(folds_list_train1_unique + folds_list_train2_unique), FEATURES_LIST_TOTRAIN].to_numpy().mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677155"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(folds_list_train1_unique + folds_list_train2_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0088,  0.3957,  0.3306,  0.0092,  0.0034, -0.0050, -0.0146,  0.0553,\n",
       "         0.0251,  0.2647,  0.1671,  0.0949,  0.0445,  0.1525,  0.0800,  0.2217,\n",
       "         0.1283,  0.1218,  0.1096,  0.2977,  0.2646,  0.1881,  0.1725,  0.2547,\n",
       "         0.2327,  0.2979,  0.2685,  0.1399,  0.1629,  0.3306,  0.3439,  0.2268,\n",
       "         0.2519,  0.3164,  0.3360,  0.3528,  0.3677,  0.0265,  0.0186,  0.0432,\n",
       "         0.0530,  0.4542,  0.3776,  0.4162,  0.4393,  0.4865,  0.4921,  0.3684,\n",
       "         0.5014,  0.5438,  0.5307,  0.4567,  0.0565,  0.3890,  0.3769,  0.7755,\n",
       "         0.9247,  0.7859,  0.8085,  0.8990,  0.5534,  0.5555,  0.5592,  0.5614,\n",
       "         0.4423,  0.6188,  0.6172,  0.5977,  0.5981,  0.3774,  0.2389,  0.3080,\n",
       "         0.0041, -0.0322, -0.0016, -0.0199, -0.0316, -0.0932, -0.0081, -0.0358,\n",
       "        -0.0025, -0.0149, -0.0350, -0.1015,  0.3934,  0.5416,  0.3924,  0.4281,\n",
       "         0.4976,  0.3994,  0.4332,  0.5235,  0.4224,  0.4221,  0.4348,  0.4547,\n",
       "         0.3984,  0.5422,  0.3973,  0.4259,  0.4865,  0.4105,  0.4340,  0.4839,\n",
       "         0.4168,  0.4198,  0.4614,  0.4553,  0.3950,  0.3824,  0.3900,  0.3918,\n",
       "         0.3807,  0.4035,  0.4320,  0.3767,  0.4279,  0.4207,  0.3952,  0.4465,\n",
       "         0.3616,  0.2999,  0.3709,  0.3035,  0.3601,  0.2778,  0.3743,  0.2571,\n",
       "         0.3739,  0.2682], dtype=torch.float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor(df.loc[(folds_list_train1_unique + folds_list_train2_unique), FEATURES_LIST_TOTRAIN].to_numpy(), device='cpu'), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.8386e-03,  3.8558e-01,  3.5769e-01,  8.9192e-03,  4.1501e-03,\n",
       "        -3.7146e-03, -1.2589e-02,  5.1777e-02,  2.6828e-02,  2.4881e-01,\n",
       "         1.8235e-01,  8.9122e-02,  4.9486e-02,  1.4311e-01,  8.9027e-02,\n",
       "         2.1168e-01,  1.4630e-01,  1.2122e-01,  1.1358e-01,  2.9381e-01,\n",
       "         2.6877e-01,  1.8691e-01,  1.7698e-01,  2.5244e-01,  2.3856e-01,\n",
       "         2.9407e-01,  2.7318e-01,  1.3548e-01,  1.6088e-01,  3.2189e-01,\n",
       "         3.4253e-01,  2.2056e-01,  2.5013e-01,  3.0822e-01,  3.3535e-01,\n",
       "         3.4145e-01,  3.6583e-01,  2.9320e-02,  2.2892e-02,  4.0022e-02,\n",
       "         5.0750e-02,  4.4505e-01,  3.6018e-01,  3.4603e-01,  4.1153e-01,\n",
       "         4.3803e-01,  4.7612e-01,  3.4787e-01,  4.9963e-01,  5.6400e-01,\n",
       "         5.1226e-01,  4.5739e-01,  4.5744e-02,  3.6270e-01,  3.5887e-01,\n",
       "         6.5260e-01,  8.0495e-01,  6.6135e-01,  6.7981e-01,  7.6259e-01,\n",
       "         5.5640e-01,  5.5817e-01,  5.4554e-01,  5.4678e-01,  4.3506e-01,\n",
       "         6.0757e-01,  6.0850e-01,  5.9519e-01,  5.9594e-01,  3.6954e-01,\n",
       "         2.4337e-01,  3.3227e-01,  5.3933e-03, -3.2868e-02, -2.0445e-04,\n",
       "        -1.9092e-02, -3.1898e-02, -7.6800e-02, -6.0595e-03, -3.5435e-02,\n",
       "        -2.0995e-03, -1.4418e-02, -3.4615e-02, -8.0085e-02,  3.9822e-01,\n",
       "         5.5782e-01,  4.0240e-01,  4.4445e-01,  5.1409e-01,  4.0052e-01,\n",
       "         4.1025e-01,  5.2051e-01,  4.0508e-01,  4.0883e-01,  4.2889e-01,\n",
       "         4.1763e-01,  4.0227e-01,  5.5909e-01,  4.0711e-01,  4.3686e-01,\n",
       "         5.0012e-01,  4.0856e-01,  4.0506e-01,  4.8141e-01,  4.0166e-01,\n",
       "         4.0706e-01,  4.5305e-01,  4.1501e-01,  3.9999e-01,  4.1654e-01,\n",
       "         4.0074e-01,  4.0688e-01,  4.1228e-01,  4.0264e-01,  4.0711e-01,\n",
       "         3.7342e-01,  4.0443e-01,  4.0103e-01,  3.8582e-01,  4.1560e-01,\n",
       "         3.3513e-01,  2.6878e-01,  3.4355e-01,  2.8000e-01,  3.3515e-01,\n",
       "         2.4488e-01,  3.3918e-01,  2.3238e-01,  3.4256e-01,  2.4562e-01],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(f_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training KMeans clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissKMeans:\n",
    "    def __init__(self, n_clusters=8, n_init=10, max_iter=300):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.max_iter = max_iter\n",
    "        self.kmeans = None\n",
    "        self.cluster_centers_ = None\n",
    "        self.inertia_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.kmeans = faiss.Kmeans(d=X.shape[1],\n",
    "                                   k=self.n_clusters,\n",
    "                                   niter=self.max_iter,\n",
    "                                   nredo=self.n_init)\n",
    "        self.kmeans.train(X.astype(np.float32))\n",
    "        self.cluster_centers_ = self.kmeans.centroids\n",
    "        self.inertia_ = self.kmeans.obj[-1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.kmeans.index.search(X.astype(np.float32), 1)[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forclustering = df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN].astype({'feature_0': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forclustering = np.copy(df_forclustering[FEATURES_LIST_TOTRAIN].to_numpy(), order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLUSTERS = 5\n",
    "\n",
    "clusterer = FaissKMeans(n_clusters=NB_CLUSTERS, n_init=10, max_iter=3000)\n",
    "clusterer.fit(df_forclustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df.loc[:, FEATURES_LIST_TOTRAIN].astype({'feature_0': np.float32})\n",
    "df_full = np.copy(df_full[FEATURES_LIST_TOTRAIN].to_numpy(), order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clusters = clusterer.predict(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = y_clusters + 1  # +1 to avoid cluster indice 0 (less practical for gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (CLUSTERING == True):\n",
    "    FEATURES_LIST_TOTRAIN = FEATURES_LIST_TOTRAIN + ['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['red', 'blue', 'orange', 'brown', 'black']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "#ax = axes.ravel()\n",
    "#fig.tight_layout(pad=10.0)\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=18)\n",
    "ax.set_ylabel('Number of values in cluster', fontsize=18)\n",
    "ax.set_title('Number of values by cluster', fontsize=18)\n",
    "\n",
    "df.loc[folds_list_train2_unique, :].groupby(by='cluster')['resp'].count().plot.bar(figsize=(15,5), ax=ax, color=colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA40AAAFVCAYAAABLpgpSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABFaklEQVR4nO3deVxVdf7H8fdBZHMBF1AMlfAqqbmkqOgUmoo0jqKUmY4GikuijdbUNFZTmpOlzkzWDJVDKYqWzlQqjaPmkluNGy5TaSoaqCAqLrjnAuf3Rz/viHDxYBcv6uv5ePB4eM73+z3f95Er9eGc8z2GaZqmAAAAAAAohpurAwAAAAAAyi+KRgAAAACAQxSNAAAAAACHKBoBAAAAAA5RNAIAAAAAHKJoBAAAAAA4RNEIAICkQYMGyTAMV8coolOnTgoODnZ1jLuSYRgaNGiQq2MAgMtRNAIAAI0fP14LFy50dQxLZs6cqbffftvVMW7apk2bNHr0aP3iF79Q5cqVZRiGZs6c6epYAOAQRSMAANBrr71G0XidCxcu6IMPPnD6cRcvXqx3331XeXl5atGihdOPDwDORtEIAOVAfn6+zp8/7+oYQJk4c+bMbTm/l5eXKlas6OQ0UkJCgk6fPq0dO3bo2WefdfrxAcDZKBoB4BabOXOmDMPQihUr9Mc//lENGjSQl5eX/vnPf0qSTNPU+++/r9atW8vHx0dVqlTRww8/rFWrVhU5VkpKitq2bSs/Pz9VqlRJISEhGjBggHJzc+19rj4T98MPP6hXr17y9fVV1apVFRMTox9++MFS5vHjx8swDO3cuVPPPPOMAgMDValSJXXp0kW7d++WJM2fP1+tWrWSt7e3goODlZSUVOyxVqxYoW7dusnPz09eXl5q3ry5pk2bVqTfsmXL9MQTTygkJETe3t7y8/NTt27dtGbNmiJ9r57joUOH1L9/f1WrVk2VKlVSVFSU9uzZY+kcr8rNzVVsbKxq1KhhP8dt27bZ248cOSIPDw8NHDiw2PEjR46Um5ub9u/ff8O59u7dq8GDBysoKEgeHh6qU6eOevXqpS1btpQ4Ljg4WJ06dSqyf/Xq1UVudfzxxx81fvx4hYaGysfHR35+fmrWrJl+97vfSZIyMzPtz3LOmjVLhmHYv65l9ft2Ndu2bdsUFRUlX19fNW/evMTzuXz5snbt2qUDBw6U2O/q8desWaP9+/cXyrp69WpJhT/vffr0UfXq1VW1alVJUkFBgSZOnKiIiAjVrl1bHh4eqlevnhISEnT8+PEicxX3TOPVfevXr1fHjh1VqVIl1axZU0OHDtXZs2dvmF+SatWqpUqVKlnqCwDlgburAwDA3er555/X5cuXNWzYMFWtWlWhoaGSpCeffFJz585Vnz59NHjwYF28eFEfffSRIiMjNX/+fEVHR0uS5syZo7i4OD300EOaMGGCvL29deDAAS1ZskRHjx6Vv7+/fa5z587p4YcfVtu2bfXmm28qPT1d7733njZs2KBt27apdu3aljLHxcWpcuXKeumll5Sbm6u//OUvioqK0h//+Ee98MILSkhIUHx8vKZPn66nnnpKTZo00YMPPmgfn5SUpBEjRig8PFwvv/yyKlWqpOXLlyshIUH79u3Tn/70J3vfmTNn6sSJE4qNjVVQUJCys7P14YcfqkuXLlq1apUeeuihQtnOnTuniIgIhYeH64033lBGRobeeecd9erVS999950qVKhg6RwfeeQRVa9eXePHj9fhw4eVmJioiIgIrV+/Xvfff79q1aql6OhoffbZZ0pMTJSfn5997I8//qi5c+eqa9euql+/fonzpKWlqUuXLrp8+bKGDBmi+++/XydOnNCaNWv0n//8R61bt7aU90ZGjRqlGTNmKDY2Vs8++6zy8/OVnp6uL7/8UpLk7++v2bNn68knn9RDDz2k4cOHFzlGab5vknTgwAF17txZjz/+uB577LEbFlPZ2dlq3LixOnbsaC/+HHn77bf14osv6tixY5o6dap9f+PGje1/Pnv2rDp27Khf/OIXmjhxoo4ePSpJunTpkv70pz/pscceU69evVSpUiVt3rxZ06dP11dffaUtW7bIw8OjxPklafv27erRo4cGDx6sX//611q9erWmT58uNzc3h78sAYDbmgkAuKWSk5NNSWajRo3Mc+fOFWqbP3++Kcn8+9//Xmj/5cuXzdatW5vBwcFmQUGBaZqmGRMTY1apUsW8fPlyifN17NjRlGSOGTOm2LmeeuqpG2YeN26cKcns0aOHfX7TNM133nnHlGRWrlzZ3L9/v33/0aNHTU9PT7Nfv372fYcOHTI9PT3N/v37Fzn+6NGjTTc3N3Pv3r32fWfPni3S7/Dhw2aNGjXMX/7yl8We4+TJkwvtnzJliinJXLp06Q3PMS4uzpRkxsTEFDrHtLQ00zAMMyoqyr7viy++MCWZ7777bqFjzJkzx5Rk/uMf/yhxroKCArNp06amp6en+d///rdIe35+fqFzq1+/fqH2+vXrmx07diwybtWqVaYkMzk52b6vWrVqRf6+iiPJjIuLK7K/tN+3+vXrm5LMDz744IZzXpWRkWFKKvacilPc38m1bZLMl19+uUhbQUGBef78+SL7P/zww2K/b8X9nUgyDcMw169fX2h/9+7dTXd3d/PMmTOWzuGqTz75pMj3DADKG25PBQAXSUhIkI+PT6F9c+bMUZUqVdS7d28dO3bM/pWXl6eePXsqMzNT6enpkiRfX1+dP39e//73v2Wa5g3nGzt2bKHtmJgYhYaGlmrxk9GjRxe6bfHq1b5evXqpXr169v3+/v4KDQ21Z5WkTz/9VBcvXtSQIUMKnduxY8fUs2dPFRQUaOXKlfb+196+d/bsWR0/flwVKlRQu3bttHHjxiLZ3NzcNHr06EL7OnfuLEmFctzICy+8UOgcW7durcjISK1YscJ+xSwyMlL33nuvpk+fXmjs9OnTVaNGDfXu3bvEObZv364dO3Zo8ODBxd666ebmvP88+/r6aseOHfruu+9uanxpv2+SVL16dQ0ePNjyHMHBwTJN84ZXGUvj+eefL7LPMAx5e3tL+uk54ry8PB07dsz+OSnuc1Wc9u3bKzw8vNC+zp0768qVK8rMzPx5wQGgHKJoBAAXadSoUZF933//vc6cOaNatWrJ39+/0Nf48eMl/fRMnSS99NJLql+/vnr37i1/f3899thj+vDDD4td9MPPz6/YW1AbN26sI0eO6Ny5c5Yyh4SEFNquVq2aJOnee+8t0rdatWqFnhP7/vvvJUldu3Ytcm6RkZGFzk2S9u3bp379+qlatWqqUqWKatasKX9/fy1evFgnT54sMl+dOnXk5eVVaF+NGjUkqdjn1Ry59jbHq5o0aaL8/Hz7c4qGYWjo0KHaunWrtm/fLkn64YcftHr1aj355JM3vMXxahH7wAMPWM51s95++22dPHlSzZo1U4MGDTR06FClpqaqoKDA0vjSft8kqUGDBpZvBy4L/v7+hW4bvtY///lPtWvXTt7e3qpWrZr8/f3tn+viPlfFuf7fgXRznzUAuF3wTCMAuMj1VxmlnxbB8ff318cff+xw3P333y9JatiwoXbu3KmVK1dq5cqVWrNmjYYNG6Zx48Zp7dq1atCggX2Mo5fWW7lCeS1HhYCj/dce/+qfU1JSFBgYWGz/q/8zfvbsWUVEROjcuXN65pln1KxZM1WpUkVubm5688037c/jWclwfY6bUdz4+Ph4jRs3TtOnT9ff/vY3zZgxQ6ZpaujQoZaP5+j7ciOOxl25cqXIvl69eikzM1OLFy/WmjVrtGLFCk2fPl0PPfSQVqxYccMCtzTft6uK+2zfSo7mnz9/vp544gm1bdtW77zzjurWrSsvLy/l5+frkUcesVxIl+VnDQDKI4pGAChHGjZsqD179ig8PFyVK1e+YX9PT091795d3bt3l/TT+99+9atf6a233tK7775r73fy5EkdPny4yNXGXbt2KSAg4Jas5NiwYUNJUs2aNdW1a9cS+65cuVKHDh3SjBkzitzm+Ic//KHMMko/XVm7/tbD77//XhUqVCi0uE3t2rXVs2dPffTRR5o0aZJmzZqldu3aqWnTpjec4+qiR9euyloa1atX14kTJ4rsd7QabvXq1TVw4EANHDhQpmlq7NixmjJlilJTU/X444+XOFdpvm+3ys0W27Nnz5aXl5dWrVpVqLDctWuXs6IBwB2J21MBoByJjY1VQUGBXnzxxWLbr70N8NixY0XaW7VqJUnFFhSTJk0qtL1gwQLt3r37hs/fOUvfvn3l6empcePG6cKFC0XaT506pYsXL0r635Wc66/aLFu2zPJzZzdrypQphebdunWrVqxYoS5duhQp5IcNG6aTJ09qxIgRysrKsnSVUZJatGihpk2basaMGdqxY0eR9htdrWrUqJF27dql7Oxs+76LFy8W+kWB9L/n9q5lGIb9tthrPyeVK1cu9nNTmu/bzSrNKzeuZj158uRNXSk3DKPQFUXTNPX666+X6jgAcLfhSiMAlCNXX7ORmJiorVu3qkePHqpZs6aysrK0fv167d271341qVu3bvL19VVERITq1q2rvLw8+zsgn3zyyULHrVmzpubPn69Dhw6pU6dO9ldu1KpVy/6sZFkLCgrS+++/r6FDh6px48Z68sknVb9+feXm5urbb7/VwoULtXPnTgUHB+vBBx9U7dq19dxzzykzM1NBQUHavn27Zs+erWbNmunbb78ts5z79+9XVFSUoqOjlZOTo8TERHl7exd5rYQkRUVFqX79+pozZ44qVaqkfv36WZrDMAwlJyerS5cuatu2rf2VG3l5eVqzZo0eeeQR/eY3v3E4/umnn9a8efPUtWtXjRgxQpcuXdLs2bOL3JZ55swZBQYGKjo6Wg888IACAgKUkZGh999/X9WqVVPPnj3tfcPDw7VixQpNnjxZ9erVk2EY6tevX6m+bzerNK/cuJp10aJFevrpp9WhQwdVqFBBnTt3VkBAQInj+vTpo88++0ydO3dWbGysLl++rIULF+r8+fM3nf1m7N+/X7Nnz5Yk+y8N/vWvfykrK0uS7H/HAFBu3OrlWgHgbnf1lRurVq1y2CclJcV88MEHzSpVqpienp5m/fr1zZiYGHPevHn2PklJSWbXrl3NWrVqmRUrVjRr165t/vKXvzS//PLLQse6+nqCffv2mdHR0WaVKlXMypUrm9HR0WZ6erqlzFdfuZGRkVFo/9VXJYwbN67IGEevRfjqq6/M3r17m/7+/mbFihXNwMBAs1OnTuaf//xn88KFC/Z+//3vf82oqCjTz8/PrFy5stmxY0dz7dq19ldjWJmrpHzXu3rco0ePmgMHDjSrV69uent7mw8//LCZlpbmcNyECRNMSWZ8fPwN57jerl27zAEDBti/h4GBgWavXr3MLVu23PDcZs6caTZq1MisWLGiGRwcbE6ePNlcuXJlodc3XLx40Rw7dqzZpk0bs3r16qaHh4dZv359c/DgweaePXsKHW/Pnj1mZGSkWaVKFVNSkb9jq983R68DKUlpX7lx9uxZMz4+3gwICDDd3NwK/Xsq6XUcpvnTv5vGjRubnp6eZu3atc1hw4aZx48fd/h6DSv7TNPav+urrr4axdGXlWMAwK1kmCZPbAPAnaxTp07KzMzkVQBlZMqUKfr973+v//znP2rfvr2r4wAA4HQ80wgAwE26cuWK/v73v6tZs2YUjACAOxbPNAIAUEoZGRlav369UlNT9cMPP2ju3LmujgQAQJmhaAQAoJTWrFmjwYMHq2bNmnr11VctL4ADAMDtiGcaAQAAAAAO8UwjAAAAAMAhbk/VT+8v+znvlwIAAACA21lmZqaOHTtWbBtFo6Tg4GClpaW5OgYAAAAAuERYWJjDNm5PBQAAAAA4RNEIAAAAAHCIohEAAAAA4BBFIwAAAADAIUtF44ULF5SSkqKNGzeWdR4AAAAAQDliqWj09PTUsGHDtG3btrLOAwAAAAAoRywVjW5ubqpbt65Onz5d1nkAAAAAAOWI5Wca4+LiNHv2bF28eLEs8wAAAAAAyhF3qx07dOig+fPnq2XLlho5cqQaNmwoHx+fIv0iIiKcGhAAAAAA4DqWi8bIyEj7n8eMGSPDMAq1m6YpwzCUn5/vvHQAAAAAAJeyXDQmJyeXZQ4AAAAAQDlkuWiMi4sryxwAbmPX3XiAn8k0XZ0AAADgfywvhAMAAAAAuPuUqmg8ePCg4uPjFRQUJA8PD3355ZeSpNzcXMXHx2vz5s1lEhIAAAAA4BqWi8aMjAyFhYXps88+U9OmTQsteOPv76+0tDR9+OGHZRISAAAAAOAalp9pfPnll+Xm5qbvvvtO3t7eCggIKNTevXt3/etf/3J6QAAAAACA61i+0rhixQqNHDlSdevWLfK6DUmqX7++srKynBoOAAAAAOBalovG06dPKzAw0GH7pUuXdOXKFaeEAgAAAACUD5aLxrp162rHjh0O2zds2CCbzeaUUAAAAACA8sFy0fjoo49qxowZ+u677+z7rt6m+tlnn+mTTz5R3759nZ8QAAAAAOAyhmlae4306dOn1b59e2VmZioiIkLLli1T165ddfr0aW3atEktW7bU119/LS8vr7LO7HRhYWFKS0tzdQzgtlXMY874Gaz9VAYAAHCekmoiy1caq1atqvXr12vo0KFKS0uTaZpavny5du/erZEjR2rVqlW3ZcEIAAAAAHDM8pXG6+Xm5so0Tfn7+xe7murthCuNwM9zm/8IKHe40ggAAG41p1xpnDBhQqHnGf39/RUQEGAvGHfs2KEJEyb8zKgAAAAAgPLEctE4fvx4ffPNNw7bv/vuO7322mtOCQUAAAAAKB8sF4038uOPP8rd3b1UY5YuXarQ0FDZbDZNmjSpSLtpmho9erRsNpuaN2+urVu32tvi4+MVEBCg+++/v9CY3/3ud7rvvvvUvHlzxcTEKC8v76bOBwAAAABwg6Lx9OnTOnDggA4cOCBJOn78uH372q/t27fro48+Ut26dS1PnJ+fr1GjRmnJkiXauXOn5s6dq507dxbqs2TJEqWnpys9PV1JSUlKSEiwtw0aNEhLly4tctzIyEh99913+uabb9SoUSO9+eabljMBAAAAAAorsWicOnWq7r33Xt17770yDEPPPPOMffvar9atW2vFihUaMWKE5Yk3bdokm82mkJAQeXh4qF+/fkpNTS3UJzU1VbGxsTIMQ+Hh4crLy1NOTo4kKSIiQtWrVy9y3G7dutmveIaHhysrK8tyJgAAAABAYSXeT9qpUydJP90mOmHCBMXExKh58+aF+hiGocqVKys8PFwdOnSwPHF2dnahK5NBQUHauHHjDftkZ2crMDDQ0hwzZszQE088UWxbUlKSkpKSJP20EiwAAAAAoKgSi8aOHTuqY8eOkqT9+/drxIgRateunVMmLu5NH9e/usNKH0cmTpwod3d3DRgwoNj24cOHa/jw4ZJ+Wl4WAAAAAFCU5ZVrkpOTnTpxUFCQDh48aN/OyspSnTp1St2nOLNmzdKiRYu0cuXK2/4dkgAAAADgSpZXT920aZM++OCDQvtSU1PVrFkz3XPPPXrppZdKNXGbNm2Unp6ujIwMXbp0SfPmzVN0dHShPtHR0UpJSZFpmtqwYYN8fX1veGvq0qVLNXnyZH3++efy8fEpVSYAAAAAQGGWi8bXXntNn3/+uX37wIED6t+/vw4fPixfX19Nnjy5VFcj3d3dlZiYqKioKDVu3Fh9+/ZV06ZNNW3aNE2bNk2S1L17d4WEhMhms2nYsGF677337OP79++v9u3ba/fu3QoKCtL06dMlSU8//bTOnDmjyMhItWzZslSL8wAAAAAACjPM4h4cLEZQUJCefvppjR07VpI0ZcoUjRs3Tnv37tU999yjX/7yl8rLy9P69evLNHBZCAsLU1pamqtjALct7gJ3Lms/lQEAAJynpJrI8pXG48ePq3bt2vbtL774QhEREbrnnnsk/XQraXp6+s+MCgAAAAAoTywXjX5+fjpy5Igk6eLFi9qwYYMiIiLs7YZh6MKFC85PCAAAAABwGcurp7Zs2VIffvihunbtqgULFujHH39UVFSUvT0jI0O1atUqk5AAAAAAANewXDS+8sor6tatm9q2bSvTNBUZGVno/YaLFi1y2jscAQAAAADlg+WisUOHDtq6dau++OIL+fr6ql+/fva248ePq1u3boqJiSmTkAAAAAAA17BcNEpSo0aN1KhRoyL7a9SooalTpzotFAAAAACgfLC8EA4AAAAA4O5j+UpjSEjIDfsYhqF9+/b9rEAAAAAAgPLDctFYr149Gde9wfvKlSvKyMjQoUOHZLPZ7O9sBAAAAADcGSwXjatXr3bYNnfuXD333HOaNm2aMzIBAAAAAMoJpzzT2L9/f/Xu3VvPPfecMw4HAAAAACgnnLYQTsuWLbV27VpnHQ4AAAAAUA44rWjcvn273NxYjBUAAAAA7iSWn2l0dBXxxIkTWrFihT744AM9+uijTgsGAAAAAHA9y0Vjp06diqyeKkmmaUqSunbtqr/97W/OSwYAAAAAcDnLRWNycnKRfYZhqHr16mrUqJEaNWrk1GAAAAAAANezXDTGxcWVZQ4AAAAAQDnEyjUAAAAAAIccXmlMSUm5qQPGxsbedBgAAAAAQPnisGgcNGiQDMOwL3RjhWEYFI0AAAAAcAdxWDSuWrXqVuYAAAAAAJRDDovGjh073socAAAAAIByiIVwAAAAAAAOWS4ax40bp/vvv99he7NmzfT66687JRQAAAAAoHywXDQuWLBAkZGRDtu7deumTz/91CmhAAAAAADlg+WiMSMjQ/fdd5/D9tDQUGVkZDglFAAAAACgfCjVM415eXkO206ePKn8/PyfmwcAAAAAUI5YLhqbNm2q1NTUYttM09Tnn39e4pVIAAAAAMDtx3LROGTIEG3YsEGDBg1Sbm6ufX9ubq7i4+O1YcMGDRkypExCAgAAAABcw+F7Gq83bNgwrVmzRikpKZo9e7YCAwNlGIYOHTok0zT1xBNPKCEhoSyzAgAAAABusVI90zhnzhzNmzdPPXr0kK+vr6pUqaLo6Gj985//1Ny5c8sqIwAAAADARUpVNEpS3759lZqaqh07dmjnzp1asGCB+vTpc1OTL126VKGhobLZbJo0aVKRdtM0NXr0aNlsNjVv3lxbt261t8XHxysgIKDIuyNPnDihyMhINWzYUJGRkTp58uRNZQMAAAAA3ETR6Cz5+fkaNWqUlixZop07d2ru3LnauXNnoT5LlixRenq60tPTlZSUVOj210GDBmnp0qVFjjtp0iR16dJF6enp6tKlS7HFKAAAAADAGpcVjZs2bZLNZlNISIg8PDzUr1+/IquzpqamKjY2VoZhKDw8XHl5ecrJyZEkRUREqHr16kWOm5qaqri4OElSXFycFi5cWObnAgAAAAB3KpcVjdnZ2apbt659OygoSNnZ2aXuc70jR44oMDBQkhQYGKijR48W2y8pKUlhYWEKCwsrtBosAAAAAOB/XFY0mqZZZJ9hGKXuc7OGDx+utLQ0paWlyd/f3ynHBAAAAIA7jcuKxqCgIB08eNC+nZWVpTp16pS6z/Vq1aplv4U1JydHAQEBTkwNAAAAAHcXlxWNbdq0UXp6ujIyMnTp0iXNmzdP0dHRhfpER0crJSVFpmlqw4YN8vX1td966kh0dLRmzZolSZo1a5Z69epVZucAAAAAAHc6lxWN7u7uSkxMVFRUlBo3bqy+ffuqadOmmjZtmqZNmyZJ6t69u0JCQmSz2TRs2DC999579vH9+/dX+/bttXv3bgUFBWn69OmSpLFjx2r58uVq2LChli9frrFjx7rk/AAAAADgTmCYxT046MD69euVmJio9PR0HT9+vMgzh4ZhaN++fU4PWdbCwsKUlpbm6hjAbctJjxrj/1n/qQwAAOAcJdVE7lYPkpKSosGDB6tixYpq1KiR6tWr57SAAAAAAIDyyXLROHHiRIWGhmrFihU3XIwGAAAAAHBnsPxM4/79+5WQkEDBCAAAAAB3EctFY1BQkC5evFiWWQAAAAAA5YzlonHEiBH66KOPlJ+fX5Z5AAAAAADliOVnGlu3bq3PPvtMbdu21ahRo3TvvfeqQoUKRfpFREQ4NSAAAAAAwHUsF41dunSx/3no0KEyrltj3zRNGYbBlUgAAAAAuINYLhqTk5PLMgcAAAAAoByyXDTGxcWVZQ4AAAAAQDlkeSEcAAAAAMDdx+GVxrVr10r638I2V7dvhIVwAAAAAODO4bBo7NSpkwzD0IULF+Th4WHfdoSFcAAAAADgzuOwaJwxY4YMw1DFihUlsRAOAAAAANyNHBaNgwYNKrTNQjgAAAAAcPdhIRwAAAAAgEMUjQAAAAAAhygaAQAAAAAOUTQCAAAAAByiaAQAAAAAOETRCAAAAABw6KaLxgsXLujChQvOzAIAAAAAKGdKVTQePXpUI0eOVJ06dVS5cmVVrlxZgYGBGjlypI4cOVJWGQEAAAAALuJutWNGRoYefPBB5eTkKDQ0VOHh4TJNU7t27dK0adOUmpqqdevWKSQkpCzzAgAAAABuIctF43PPPafjx49r/vz56t27d6G2BQsWqH///nr++ec1f/58Z2cEAAAAALiI5dtTV65cqVGjRhUpGCUpJiZGCQkJWrlypTOzAQAAAABczHLRaBiGGjZs6LC9UaNGMgzDKaEAAAAAAOWD5aKxY8eOWrVqlcP21atXq1OnTs7IBAAAAAAoJywXjW+//bY2btyo5557TkePHrXvP3r0qH77299q48aNevvtt8siIwAAAADARQzTNE0rHUNCQnTu3DkdO3ZMkuTn5yfDMHTy5ElJUs2aNVWpUqXCBzcM7du3z8mRnS8sLExpaWmujgHctrgz3bms/VQGAABwnpJqIsurp9arV49nFgEAAADgLmO5aFy9enUZxgAAAAAAlEeWn2ksC0uXLlVoaKhsNpsmTZpUpN00TY0ePVo2m03NmzfX1q1bbzh2+/btCg8PV8uWLRUWFqZNmzbdknMBAAAAgDuR5aLx+PHj+v777wvty8jI0G9+8xsNGDBAX3zxRakmzs/P16hRo7RkyRLt3LlTc+fO1c6dOwv1WbJkidLT05Wenq6kpCQlJCTccOwLL7ygcePGafv27ZowYYJeeOGFUuUCAAAAAPyP5dtTx4wZoz179tiv3J09e1YPPfSQDh06JEn6xz/+oS+//FIRERGWjrdp0ybZbDaFhIRIkvr166fU1FQ1adLE3ic1NVWxsbEyDEPh4eHKy8tTTk6OMjMzHY41DEOnT5+WJJ06dUp16tSxeooAAAAAgOtYvtK4fv16/fKXv7Rv/+Mf/9ChQ4e0ePFiHTp0SI0bN9aUKVMsT5ydna26devat4OCgpSdnW2pT0lj3377bf3ud79T3bp19fzzz+vNN98sdv6kpCSFhYUpLCxMubm5lnMDAAAAwN3EctF45MgR1atXz769ZMkShYWF6ZFHHlHt2rU1aNAgbdu2zfLExb3p4/rVWR31KWns+++/r6lTp+rgwYOaOnWqhgwZUuz8w4cPV1pamtLS0uTv7285NwAAAADcTSwXjRUrVtSFCxfs22vWrFHHjh3t235+fjp+/LjliYOCgnTw4EH7dlZWVpFbSR31KWnsrFmz9Oijj0qSHn/8cRbCAQAAAICfwXLR2KhRI3322WcyTVOff/65Tpw4oS5dutjbDx48qOrVq1ueuE2bNkpPT1dGRoYuXbqkefPmKTo6ulCf6OhopaSkyDRNbdiwQb6+vgoMDCxxbJ06dbRmzRpJ0pdffqmGDRtazgQAAAAAKMzyQjijRo3SoEGDVK1aNZ0/f14hISGFisa1a9eqWbNm1id2d1diYqKioqKUn5+v+Ph4NW3aVNOmTZMkjRgxQt27d9fixYtls9nk4+Oj5OTkEsdK0gcffKAxY8boypUr8vLyUlJSkuVMAAAAAIDCDLO4BwQdmDNnjhYsWCBfX1+99NJLstlskn56HUe3bt00cuRIh88QlmdhYWFKS0tzdQzgtnXd48j4maz/VAYAAHCOkmqiUhWNdyqKRuDnoWh0Ln4qAwCAW62kmsjyM43X2rt3r77++mudOnXqZwUDAAAAAJRvpSoaFy1apAYNGig0NFQRERHasmWLJOno0aOy2Wz69NNPyyQkAAAAAMA1LBeNq1evVkxMjKpXr65x48YVeldiQECAGjRooHnz5pVJSAAAAACAa1guGidMmKAWLVpo48aNGjVqVJH29u3ba+vWrU4NBwAAAABwLctFY1pamgYMGCA3t+KHBAUF6fDhw04LBgAAAABwPctFY35+vjw9PR22Hzt2TB4eHk4JBQAAAAAoHywXjY0bN9a6descti9atEgtWrRwSigAAAAAQPlguWgcMmSIPv30U02fPl0FBQWSJMMwdP78eY0ePVrr16/X8OHDyywoAAAAAODWM0zT+mukBw4cqI8//lhVq1bVmTNn5O/vr+PHjys/P1+DBw/W9OnTyzJrmSnpRZYAbswwXJ3gzmL9pzIAAIBzlFQTuZfmQHPmzNFjjz2mOXPmaNeuXTJNU+3atVNsbKwee+wxp4QFAAAAAJQflorGCxcu6JNPPlFoaKhiYmIUExNT1rkAAAAAAOWApWcaPT09NWzYMG3btq2s8wAAAAAAyhFLRaObm5vq1q2r06dPl3UeAAAAAEA5Ynn11Li4OM2ePVsXL14syzwAAAAAgHLE8kI4HTp00Pz589WyZUuNHDlSDRs2lI+PT5F+ERERTg0IAAAAAHAdy0VjZGSk/c9jxoyRcd0a+6ZpyjAM5efnOy8dAAAAAMClLBeNycnJZZkDAAAAAFAOWS4a4+LiyjIHAAAAAKAcsrwQDgAAAADg7kPRCAAAAABwiKIRAAAAAOAQRSMAAAAAwCGKRgAAAACAQw6Lxvj4eG3cuNG+vXbtWuXm5t6SUAAAAACA8sFh0Thz5kzt27fPvv3www9r+fLltyQUAAAAAKB8cFg01qxZU0eOHLFvm6Z5SwIBAAAAAMoPd0cNHTp00Ouvv64DBw6oWrVqkqT58+dr7969Dg9mGIZeeeUV56cEAAAAALiEYTq4hJiZmam4uDh99dVXMk1ThmHc8GqjYRjKz88vk6BlKSwsTGlpaa6OAdy2DMPVCe4s3NgBAAButZJqIodXGoODg7VmzRpdunRJhw8fVnBwsN5++2316tWrzIICAAAAAMoXh0XjVR4eHqpXr57i4uLUrl071a9f/1bkAgAAAACUA5bf05icnKx27do5dfKlS5cqNDRUNptNkyZNKtJumqZGjx4tm82m5s2ba+vWrZbG/u1vf1NoaKiaNm2qF154wamZAQAAAOBuYrlolKRz585p3Lhxat68uSpXrqzKlSurefPmGj9+vM6dO1eqifPz8zVq1CgtWbJEO3fu1Ny5c7Vz585CfZYsWaL09HSlp6crKSlJCQkJNxy7atUqpaam6ptvvtGOHTv0/PPPlyoXAAAAAOB/LBeNJ06cUNu2bfXHP/5Rhw8f1gMPPKAHHnhAR44c0YQJE9S2bVudOHHC8sSbNm2SzWZTSEiIPDw81K9fP6Wmphbqk5qaqtjYWBmGofDwcOXl5SknJ6fEse+//77Gjh0rT09PSVJAQIDlTAAAAACAwiwXja+++qp27dqlxMRE5eTkaN26dVq3bp0OHTqkd999V7t379b48eMtT5ydna26devat4OCgpSdnW2pT0lj9+zZo3Xr1qldu3bq2LGjNm/ebDkTAAAAAKAwy0Xj559/rqFDh2rkyJGqUKGCfX+FChWUkJCg+Ph4LVy40PLExb2+w7hu3X5HfUoae+XKFZ08eVIbNmzQn/70J/Xt27fY/klJSQoLC1NYWJhyc3Mt5wYAAACAu4nlovHIkSN64IEHHLa3atVKR44csTxxUFCQDh48aN/OyspSnTp1LPUpaWxQUJAeffRRGYahtm3bys3NTceOHSsy//Dhw5WWlqa0tDT5+/tbzg0AAAAAdxPLRWOtWrW0bds2h+3btm1TrVq1LE/cpk0bpaenKyMjQ5cuXdK8efMUHR1dqE90dLRSUlJkmqY2bNggX19fBQYGlji2d+/e+vLLLyX9dKvqpUuXVLNmTcu5AAAAAAD/c8P3NF7Vs2dP/f3vf1erVq00bNgwubn9VG8WFBToww8/1IwZM/TUU09Zn9jdXYmJiYqKilJ+fr7i4+PVtGlTTZs2TZI0YsQIde/eXYsXL5bNZpOPj4+Sk5NLHCtJ8fHxio+P1/333y8PDw/NmjWryG2vAAAAAABrDLO4B/6Kcfz4cbVv31779u2Tv7+/QkNDJUm7d+9Wbm6ubDab/vOf/6hGjRplGrgshIWFKS0tzdUxgNsWv5dxLms/lQEAAJynpJrI8u2pNWrUUFpamsaOHasaNWpo8+bN2rx5s2rWrKkXX3xRmzdvvi0LRgAAAACAY5avNN7JuNII/DxcaXQufioDAIBbzSlXGgEAAAAAdx+KRgAAAACAQxSNAAAAAACHKBoBAAAAAA5RNAIAAAAAHKJoBAAAAAA4dFNF4969e/X111/r1KlTzs4DAAAAAChHSlU0Llq0SA0aNFBoaKgiIiK0ZcsWSdLRo0dls9n06aeflklIAAAAAIBruFvtuHr1asXExKhly5aKi4vT+PHj7W0BAQFq0KCB5s2bpz59+pRFTgAAbs7HhqsT3Dl+bbo6AQDABSxfaZwwYYJatGihjRs3atSoUUXa27dvr61btzo1HAAAAADAtSwXjWlpaRowYIDc3IofEhQUpMOHDzstGAAAAADA9SwXjfn5+fL09HTYfuzYMXl4eDglFAAAAACgfLBcNDZu3Fjr1q1z2L5o0SK1aNHCKaEAAAAAAOWD5aJxyJAh+vTTTzV9+nQVFBRIkgzD0Pnz5zV69GitX79ew4cPL7OgAAAAAIBbz/LqqQkJCfr66681bNgwPffcczIMQ/3799fx48eVn5+vwYMHa8CAAWWZFQAAAABwi1kuGiVpzpw5euyxxzRnzhzt2rVLpmmqXbt2io2N1WOPPVZWGQEAAAAALlKqolGSYmJiFBMTUxZZAAAAAADljOVnGgEAAAAAdx/LVxonTJhwwz6GYeiVV175WYEAAAAAAOWH5aJx/PjxDtsMw5BpmhSNAAAAAHCHsVw0ZmRkFNl35coV7du3T1OnTtWpU6c0a9Ysp4YDAAAAALiW5aKxfv36xe5v0KCBIiMjFRERoeTkZL3xxhtOCwcAAAAAcC2nLIRjGIb69OmjlJQUZxwOAAAAAFBOOG311EuXLun48ePOOhwAAAAAoBxwStGYlpamd955R40bN3bG4QAAAAAA5YTlZxpDQkKK3X/ixAmdOXNG7u7u+vDDD50WDAAAAADgepaLxnr16skwjEL7DMNQq1at1KhRIw0fPlzBwcHOzgcAAAAAcCHLRePq1avLMAYAAAAAoDxy2kI4AAAAAIA7D0UjAAAAAMAhh0Wjm5ubKlSoUKovd3fLd7tKkpYuXarQ0FDZbDZNmjSpSLtpmho9erRsNpuaN2+urVu3Wh775z//WYZh6NixY6XKBAAAAAD4H4dVXmxsbJGFb5wpPz9fo0aN0vLlyxUUFKQ2bdooOjpaTZo0sfdZsmSJ0tPTlZ6ero0bNyohIUEbN2684diDBw9q+fLlqlevXpnlBwAAAIC7gcOicebMmWU68aZNm2Sz2eyv8ujXr59SU1MLFY2pqan24jU8PFx5eXnKyclRZmZmiWOfffZZTZkyRb169SrTcwAAAACAO53LnmnMzs5W3bp17dtBQUHKzs621KeksZ9//rnuuecetWjRosT5k5KSFBYWprCwMOXm5jrjlAAAAADgjlO6hxCdyDTNIvuuvx3WUR9H+8+fP6+JEydq2bJlN5x/+PDhGj58uCQpLCzMamwAAAAAuKuU6krj119/rR49esjf31/u7u4/ayGcoKAgHTx40L6dlZWlOnXqWOrjaP++ffuUkZGhFi1aKDg4WFlZWWrVqpUOHz5cmtMEAAAAAPw/y0Xj2rVr9fDDD2vjxo1q166dCgoK9PDDD6tNmzYyTVP333+/nnzyScsTt2nTRunp6crIyNClS5c0b948RUdHF+oTHR2tlJQUmaapDRs2yNfXV4GBgQ7HNmvWTEePHlVmZqYyMzMVFBSkrVu3qnbt2tb/RgAAAAAAdpYvDU6cOFGBgYFKS0uTYRgKCAjQSy+9pM6dO2vZsmXq06eP3nvvPesTu7srMTFRUVFRys/PV3x8vJo2bapp06ZJkkaMGKHu3btr8eLFstls8vHxUXJycoljAQAAAADOZZjFPSBYjGrVqum3v/2tXnnlFZ04cUI1a9bUsmXL1LVrV0nSqFGj9P333+vLL78s08BlISwsTGlpaa6OAdy2yvDtPHclaz+VYdnHfECd5td8OAHgTlVSTWT59tSLFy/qnnvukSR5enpKks6cOWNvb9mypbZs2fJzcgIAAAAAyhnLRWNgYKCysrIkSZUqVZKfn5++++47e3tWVlapFsIBAAAAAJR/lqu8Nm3a6Ouvv7Zvd+vWTVOnTlX9+vVVUFCgxMREtWvXrkxCAgAAAABcw/KVxiFDhqhmzZq6cOGCJOmNN96Qt7e3Bg0apPj4eHl6emrKlCllFhQAAAAAcOtZvtIYGRmpyMhI+3ZISIj27NmjlStXqkKFCnrwwQfl6+tbJiEBAAAAAK7xsx5CrFSpUpF3KwIAAAAA7hyWb09t1aqV/vrXvyo3N7cs8wAAAAAAyhHLRePRo0f1zDPPKCgoSL1799aCBQt0+fLlsswGAAAAAHAxy0XjwYMH9cUXX6hv375auXKl+vTpo8DAQD399NPavHlzWWYEAAAAALiI5aLRMAxFRkZq9uzZOnz4sGbMmKEWLVpo2rRpCg8PV+PGjTVp0qSyzAoAAAAAuMUsF43XqlSpkuLi4rRy5Urt379fr7/+unJycvSHP/zB2fkAAAAAAC70s1ZP/eGHH5SSkqI5c+bo9OnTqlixorNyAQAAAADKgVJfaTx16pSSkpL04IMPqmHDhpowYYIqV66sv/zlLzp48GBZZAQAAAAAuIjlK42LFi1SSkqKFi1apB9//FEBAQEaM2aM4uLi1KJFi7LMCAAAAABwEctFY3R0tDw9PdWzZ0/FxcXpkUceUYUKFcoyGwAAAADAxSwXje+995769esnPz+/MowDAAAAAChPLBeNI0aMKMscAAAAAIBy6KZeuQEAAAAAuDtQNAIAAAAAHKJoBAAAAAA4RNEIAAAAAHCIohEAAAAA4BBFIwAAAADAIcuv3JCkc+fO6eOPP1Z6erqOHz8u0zQLtRuGoenTpzs1IAAAAADAdSwXjZs2bdKvfvUrHT9+3GEfikYAAAAAuLNYvj31t7/9rS5fvqx//vOfOnbsmAoKCop85efnl2VWAAAAAMAtZvlK45YtW/TSSy+pT58+ZZkHAAAAAFCOWL7SWLVqVdWoUaMsswAAAAAAyhnLReOjjz6qL774oiyzAAAAAADKGctF4+TJk3X06FH95je/0b59+4qsnAoAAAAAuPNYfqbRz89PhmFo06ZNeu+994rtYxiGrly54rRwAAAAAADXslw0xsbGyjAMp06+dOlSjRkzRvn5+Ro6dKjGjh1bqN00TY0ZM0aLFy+Wj4+PZs6cqVatWpU49ne/+53+9a9/ycPDQw0aNFBycrL8/PycmhsAAAAA7haWi8aZM2c6deL8/HyNGjVKy5cvV1BQkNq0aaPo6Gg1adLE3mfJkiVKT09Xenq6Nm7cqISEBG3cuLHEsZGRkXrzzTfl7u6u3//+93rzzTc1efJkp2YHAAAAgLuF5WcanW3Tpk2y2WwKCQmRh4eH+vXrp9TU1EJ9UlNT7Vc4w8PDlZeXp5ycnBLHduvWTe7uP9XC4eHhysrKuuXnBgAAAAB3CstXGq919uxZ5eXlqaCgoEhbvXr1LB0jOztbdevWtW8HBQVp48aNN+yTnZ1taawkzZgxQ0888USx8yclJSkpKUmSlJubaymzyzn59uC7Hos5AQAAADdUqqJx3rx5ev311/X999877JOfn2/pWMWtvnr9M5OO+lgZO3HiRLm7u2vAgAHFzj98+HANHz5ckhQWFmYpMwAAAADcbSzfnrpw4UL9+te/1pUrV/TUU0/JNE31799fjz/+uCpWrKhWrVrp1VdftTxxUFCQDh48aN/OyspSnTp1LPW50dhZs2Zp0aJF+uijj5y+eA8AAAAA3E0sF41//vOf1bhxY23fvl0TJkyQJMXHx2vevHlKS0vTnj171LJlS8sTt2nTRunp6crIyNClS5c0b948RUdHF+oTHR2tlJQUmaapDRs2yNfXV4GBgSWOXbp0qSZPnqzPP/9cPj4+lvMAAAAAAIqyXDR+8803iouLk5eXl9zcfhp29VbU+++/X8OHD9ebb75peWJ3d3clJiYqKipKjRs3Vt++fdW0aVNNmzZN06ZNkyR1795dISEhstlsGjZsmP39kI7GStLTTz+tM2fOKDIyUi1bttSIESMsZwIAAAAAFGb5mcb8/HzVqFFDkuTt7S1JOnXqlL09NDRU77//fqkm7969u7p3715o37VFnmEYevfddy2PlaS9e/eWKgMAAAAAwDHLVxqDgoK0f/9+ST8VjQEBAUpLS7O37969W5UqVXJ+QgAAAACAy1i+0tihQwetWLHC/jxjdHS03nnnHfn4+KigoEDvvvuuevbsWWZBAQAAAAC3nuWiceTIkVqwYIEuXLggb29vTZw4UZs2bdL48eMlSU2bNtWf//znssoJAAAAAHABy0VjmzZt1KZNG/u2v7+/tm/frm+++UYVKlRQ48aN7QvkAAAAAADuDJaLRkeaN2/ujBwAAAAAgHKo1JcG165dqz/84Q8aNmyYdu3aJUk6e/as1q5dq7y8PGfnAwAAAAC4kOWiMT8/X0888YQefvhhvfHGG5oxY4YOHTok6af3Jvbu3dv+HkUAAAAAwJ3BctE4efJkffbZZ3rrrbf0/fffyzRNe5uXl5diYmK0ePHiMgkJAAAAAHANy0VjSkqKYmNjNWbMGNWsWbNIe+PGjbVv3z6nhgMAAAAAuJblojEzM1Pt27d32O7n56eTJ086JRQAAAAAoHywXDRWqVJFJ06ccNi+d+9e+fv7OyUUAAAAAKB8sFw0Pvjgg5ozZ06hZxmvOnnypGbMmKGHH37YqeEAAAAAAK5luWh8+eWXlZ6ers6dO2vRokWSpP/+97/6+9//rlatWuncuXMaO3ZsmQUFAAAAANx67lY7hoWFaf78+RoyZIgGDx4sSXr++edlmqYCAgK0YMECNWnSpMyCAgAAAABuPctFoyR1795dmZmZWr58uf21Gw0bNlRUVJR8fHzKKiMAAAAAwEVKVTRKkqenp3r06KEePXqURR4AAAAAQDli+ZlGAAAAAMDdp8QrjZ07dy7VwQzD0MqVK39WIAAAAABA+VFi0bh69WpVrFhRHh4elg5mGIZTQgEAAAAAyocSi0Z3d3eZpqmuXbtq8ODB6tGjh9zcuKMVAAAAAO4WJVaA2dnZevPNN7V3717FxMTonnvu0e9//3vt3r37VuUDAAAAALhQiUWjv7+/nnvuOX377bdav369evXqpaSkJDVp0kTt27fXhx9+qDNnztyqrAAAAACAW8zyvaZt27bVtGnTlJOTo5SUFFWqVElPPfWU6tSpozlz5pRlRgAAAACAi5T6PY1eXl4aMGCAgoOD5ebmphUrVuiHH34oi2wAAAAAABcrVdF46NAhpaSkaObMmUpPT1edOnX04osvavDgwWWVDwAAAADgQjcsGi9fvqzU1FQlJydr2bJlqlChgqKjozV16lRFRUWxmioAAAAA3MFKLBpHjx6tjz/+WCdPnlTz5s31l7/8RQMHDlT16tVvVT4AAAAAtxDvXncu0zRdHeFnK7FoTExMlLe3t/r3769WrVrpypUrmjlzpsP+hmHo2WefdXZGAACAO87HTZu6OsId5dc7drg6AnDHuuHtqRcuXNDHH3+sjz/++IYHo2gEAAAAgDtLiUXjqlWrblUOAAAAAEA5VGLR2LFjx1uVAwAAAABQDrl06dOlS5cqNDRUNptNkyZNKtJumqZGjx4tm82m5s2ba+vWrTcce+LECUVGRqphw4aKjIzUyZMnb8m5AAAAAMCdyGVFY35+vkaNGqUlS5Zo586dmjt3rnbu3Fmoz5IlS5Senq709HQlJSUpISHhhmMnTZqkLl26KD09XV26dCm2GAUAAAAAWOOyonHTpk2y2WwKCQmRh4eH+vXrp9TU1EJ9UlNTFRsbK8MwFB4erry8POXk5JQ4NjU1VXFxcZKkuLg4LVy48FafGgAAAADcMW64empZyc7OVt26de3bQUFB2rhx4w37ZGdnlzj2yJEjCgwMlCQFBgbq6NGjxc6flJSkpKQkSdKuXbsUFhbmnBMrS61buzqBJbm5ufL393d1jBu7Hb7nt4nb5KN523w2+Wg6W/n/gN4un029xYfTqby9XZ3Aktvl8/kWPzydpvVt8h/22+WzeVvUGZIyMzMdtrmsaCzuJZfXv0jUUR8rY29k+PDhGj58eKnGwJqwsDClpaW5OgZQBJ9NlFd8NlGe8flEecVn89Zx2e2pQUFBOnjwoH07KytLderUsdSnpLG1atVSTk6OJCknJ0cBAQFleRoAAAAAcEdzWdHYpk0bpaenKyMjQ5cuXdK8efMUHR1dqE90dLRSUlJkmqY2bNggX19fBQYGljg2Ojpas2bNkiTNmjVLvXr1uuXnBgAAAAB3Cpfdnuru7q7ExERFRUUpPz9f8fHxatq0qaZNmyZJGjFihLp3767FixfLZrPJx8dHycnJJY6VpLFjx6pv376aPn266tWrp08++cRVp3jX4rZflFd8NlFe8dlEecbnE+UVn81bxzCLe0AQAAAAAAC58PZUAAAAAED5R9EIAAAAAHCIohEAAAAA4BBFI4A71q5du7Ry5UqdPXu20P6lS5e6KBHwk02bNmnz5s2SpJ07d+qtt97S4sWLXZwKKCo2NtbVEYBiffXVV3rrrbe0bNkyV0e5K7AQDspMcnKyBg8e7OoYuEv99a9/1bvvvqvGjRtr+/bteuedd+yv4GnVqpW2bt3q4oS4W7322mtasmSJrly5osjISG3cuFGdOnXSihUrFBUVpZdfftnVEXGXuv7VZ6ZpatWqVercubMk6fPPP3dFLECS1LZtW23atEmS9MEHH+jdd99VTEyMli1bpp49e2rs2LEuTnhno2hEmalXr54OHDjg6hi4SzVr1kzr169X5cqVlZmZqT59+ujJJ5/UmDFj9MADD2jbtm2ujoi7VLNmzbR9+3ZdvHhRtWvXVlZWlqpWraoLFy6oXbt2+uabb1wdEXepVq1aqUmTJho6dKgMw5Bpmurfv7/mzZsnSerYsaOLE+Judu1/u9u0aaPFixfL399f586dU3h4uL799lsXJ7yzuew9jbgzNG/evNj9pmnqyJEjtzgN8D/5+fmqXLmyJCk4OFirV69Wnz59tH//fvG7MriSu7u7KlSoIB8fHzVo0EBVq1aVJHl7e8vNjadG4DppaWl65513NHHiRP3pT39Sy5Yt5e3tTbGIcqGgoEAnT55UQUGBTNOUv7+/JKlSpUpyd6ekKWv8DeNnOXLkiL744gtVq1at0H7TNNWhQwcXpQKk2rVra/v27WrZsqUkqXLlylq0aJHi4+P5bSRcysPDQ+fPn5ePj4+2bNli33/q1CmKRriUm5ubnn32WT3++ON69tlnVatWLV25csXVsQBJP/2MbN26tUzTlGEYOnz4sGrXrq2zZ8/yy+BbgKIRP0uPHj109uxZ+/+YX6tTp063PA9wVUpKSpHfPLq7uyslJUVPPfWUi1IB0tq1a+Xp6SlJhYrEy5cva9asWa6KBdgFBQXpk08+0b///W/7lXDA1TIzM4vd7+bmpgULFtzaMHchnmkEAAAAADjEfTAAAAAAAIcoGgEAAAAADlE0AgBwE2bOnCnDMLR69WpXRwEAoExRNAIAUI4tXLhQ48ePd3UMAMBdjKIRAIBybOHChXrttddcHQMAcBejaAQA4C51+fJl/fjjj66OAQAo5ygaAQC4zqVLlzRlyhS1bNlSPj4+8vX1VVhYmBITE0scN378eBmGUez7xIKDg4u8v/bf//63OnbsqJo1a8rb21v16tXTo48+qj179kj66X23V9/daBiG/WvmzJn2Y+Tk5CghIUH16tWTh4eH6tSpo+HDh+vo0aPFZtuxY4d++9vfKigoSF5eXtqwYUPp/4IAAHcV9xt3AQDg7nHp0iVFRUVp9erV6tatmwYOHCgvLy99++23mj9/vp5++mmnzLNmzRpFR0erWbNmevHFF+Xn56dDhw5pxYoV2rt3rxo1aqSXX35ZBQUFWrdunWbPnm0f26FDB0nSgQMH1L59e126dElDhgxRgwYNtHfvXr3//vtatWqV0tLS5OvrW2jeAQMGyNvbW88995wMw1BgYKBTzgcAcOeiaAQA4Bpvv/22Vq9erRdffFFvvPFGobaCggKnzZOamqqCggItW7ZMAQEB9v2vvPKK/c+RkZH66KOPtG7dOg0cOLDIMX7zm9/o8uXL2rZtm4KCguz7H3/8cYWHh2vq1KlFFtHx8/PTihUr5O7O/wIAAKzh9lQAAK7x0UcfqVq1anr11VeLtLm5Oe8/m1evAH722We6cuVKqcefOnVKixYtUnR0tLy8vHTs2DH7V3BwsGw2m5YtW1Zk3DPPPEPBCAAoFYpGAACukZ6ervvuu09eXl5lOs/TTz+tBx54QCNHjlT16tXVvXt3/fWvf1Vubq6l8bt371ZBQYGmT58uf3//Il+7d+/WkSNHioxr1KiRs08FAHCH41eNAABcxzAMp4+7/mpijRo1tHnzZq1bt07Lly/X2rVr9eyzz2rcuHFavHix2rdvX+JcpmlKkgYOHKi4uLhi+3h7exfZ5+Pjc6PTAACgEIpGAACu0ahRI33//fe6ePGiPD09SzW2evXqkqQTJ04oODjYvv/HH39UTk6ObDZbof4VKlRQp06d7KuqfvPNN2rdurVef/11/fvf/5bkuBC12WwyDEOXLl1S165dS5UTAIDS4PZUAACuMWDAAJ08eVKvv/56kbarV/ccuXrr54oVKwrtnzp1apFFdI4dO1Zk/H333Sdvb2+dOHHCvq9y5cqSVGif9NOVyu7du2v+/PnFvjbDNE3Lt7oCAFASrjQCAHCNMWPG6F//+pdef/11bd68Wd26dZOXl5d27Nih3bt3FykIr9W1a1fdd999evXVV3X8+HHde++9+uqrr7RhwwbVrFmzUN9hw4YpKytL3bp1U/369XXhwgX94x//0JkzZxQbG2vvFx4ersTERI0cOVK/+tWvVLFiRbVr10733nuv3n//fT344IOKiIhQbGysHnjgARUUFOiHH35QamqqYmNji6yeCgBAaVE0AgBwDQ8PDy1btkx/+ctf9PHHH+ull16Sl5eXGjZsqMGDB5c4tkKFCkpNTdXo0aP1t7/9TR4eHurWrZvWrFmjX/ziF4X6Pvnkk5o5c6ZmzZql3NxcVa1aVU2aNNGnn36qxx57zN6vf//+2rZtm+bNm6dPPvlEBQUFSk5O1r333qu6detqy5Ytmjx5slJTUzVnzhx5eXmpbt266tmzp/r27Vsmf0cAgLuLYd7oXhsAAAAAwF2LZxoBAAAAAA5RNAIAAAAAHKJoBAAAAAA4RNEIAAAAAHCIohEAAAAA4BBFIwAAAADAIYpGAAAAAIBDFI0AAAAAAIcoGgEAAAAADlE0AgAAAAAc+j9nd8FYhhhCTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "#ax = axes.ravel()\n",
    "#fig.tight_layout(pad=10.0)\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=18)\n",
    "ax.set_ylabel('Mean value of resp in cluster', fontsize=18)\n",
    "ax.set_title('resp mean by cluster : train 1', fontsize=18)\n",
    "\n",
    "df.loc[folds_list_train1_unique, :].groupby(by='cluster')['resp'].mean().plot.bar(figsize=(15,5), ax=ax, color=colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "#ax = axes.ravel()\n",
    "#fig.tight_layout(pad=10.0)\n",
    "\n",
    "ax.set_xlabel('Cluster', fontsize=18)\n",
    "ax.set_ylabel('Mean value of resp in cluster', fontsize=18)\n",
    "ax.set_title('resp mean by cluster : train 2', fontsize=18)\n",
    "\n",
    "df.loc[folds_list_train2_unique, :].groupby(by='cluster')['resp'].mean().plot.bar(figsize=(15,5), ax=ax, color=colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AE loaded\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)  \n",
    "\n",
    "ENCODER_SIZE = 32\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            #nn.Dropout(0.5),  # Noise layer\n",
    "            nn.Linear(len(FEATURES_LIST_TOTRAIN), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, ENCODER_SIZE),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.Linear(ENCODER_SIZE, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, len(FEATURES_LIST_TOTRAIN)),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.training:\n",
    "            x = x + 0.1 * torch.randn(x.shape[0], x.shape[1]).double(). to('cuda')  # 0.1 = noise variance\n",
    "            \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x        \n",
    "\n",
    "model = AutoEncoder().double().to('cuda')\n",
    "    \n",
    "#print('Number of model parameters :')\n",
    "#numel_list = [p.numel() for p in model.parameters()]\n",
    "#sum(numel_list), numel_list\n",
    "    \n",
    "if (RETRAIN_MODEl_AE == False):\n",
    "    model_AE = model\n",
    "    model_AE.load_state_dict(torch.load(MODEL_FILE_AE,map_location=torch.device('cuda')))\n",
    "    print('Model AE loaded')\n",
    "\n",
    "else:    \n",
    "    print('Training started')\n",
    "    patience=5\n",
    "\n",
    "    utility_scores = [None] * 5\n",
    "    accuracy_scores = [None] * 5\n",
    "\n",
    "    today = datetime.datetime.now()\n",
    "    now_str = today.strftime(\"%b%d_%H-%M-%S\")\n",
    "    tensorboard_dir_AE = 'runs_AE/' + now_str\n",
    "    writer = SummaryWriter(log_dir=tensorboard_dir_AE)\n",
    "\n",
    "    ts_train = torch.tensor(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    #ts_train_y = torch.tensor((df.loc[folds_list_train_unique, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "    # Normalize data\n",
    "    ts_train_mean = torch.mean(ts_train, axis=0)\n",
    "    ts_train_std = torch.std(ts_train, axis=0)\n",
    "    #ts_train_mean = torch.tensor(f_mean)\n",
    "    # If you want to use Standard scale : calculate mean from f_mean and std scale from whole dataset\n",
    "    #ts_train = pyStandardScale(ts_train, ts_train_mean, ts_train_std)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(ts_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE_AE, shuffle=True)\n",
    "\n",
    "    ts_test = [None] * 5\n",
    "    #ts_test_y = [None] * 5    \n",
    "    test_dataset = [None] * 5\n",
    "    test_loader = [None] * 5\n",
    "\n",
    "    for fold_indice in range(5):\n",
    "        ts_test[fold_indice] = torch.tensor(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "        #ts_test_y[fold_indice] = torch.tensor((df.loc[folds_list_test[fold_indice], 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "        # Normalize\n",
    "        #ts_test[fold_indice] = pyStandardScale(ts_test[fold_indice], ts_train_mean, ts_train_std)\n",
    "\n",
    "        test_dataset[fold_indice] = torch.utils.data.TensorDataset(ts_test[fold_indice])\n",
    "        test_loader[fold_indice] = torch.utils.data.DataLoader(test_dataset[fold_indice], batch_size=BATCH_SIZE_AE)\n",
    "\n",
    "    loss_fn = nn.MSELoss().to('cuda')\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE_AE, weight_decay=WEIGHT_DECAY_AE) \n",
    "\n",
    "    scheduler = None\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
    "    #                                                         max_lr=1e-4, epochs=NUM_EPOCHS, steps_per_epoch=len(train_loader))\n",
    "\n",
    "    #model.eval()\n",
    "    #start_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #start_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #print('Start Validation Accuracy: {:.4f}'.format(start_accuracy))\n",
    "    #print('Start Validation Utility: {:.4f}'.format(start_utility_score))\n",
    "\n",
    "    Val_Loss = 0\n",
    "    N_Samples = 0\n",
    "\n",
    "    the_last_loss = 10000\n",
    "    the_last_utility_score = 0\n",
    "    the_last_accuracy = 0\n",
    "    trigger_times=0\n",
    "    early_stopping_met = False\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS_AE): \n",
    "        running_loss = 0.0        \n",
    "\n",
    "        ### Call back to save activation stats (mean, std dev and near 0 values after activation functions)\n",
    "        # Setting hook for activation layers stats\n",
    "\n",
    "        hook_handles = []\n",
    "        save_output_activation_stats = []\n",
    "\n",
    "        for layer in model.modules():\n",
    "            if ('activation' in str(type(layer))):\n",
    "                save_output_activation_stats_1layer = SaveOutputActivationStats()\n",
    "                handle = layer.register_forward_hook(save_output_activation_stats_1layer)\n",
    "                save_output_activation_stats.append(save_output_activation_stats_1layer)\n",
    "                hook_handles.append(handle)    \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs = batch[0].to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, inputs.double())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "        # update local train loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # update global train loss\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print('Epoch({}) - Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "        writer.add_scalar(f\"Global train/loss\", epoch_loss, epoch)\n",
    "\n",
    "        # Write activation stats graphs\n",
    "        for layer_number,save_output_activation_stats_layer in enumerate(save_output_activation_stats):\n",
    "            df_stats_layer = pd.DataFrame(save_output_activation_stats_layer.outputs)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(25, 4))\n",
    "\n",
    "            ax[0].set_title(f'Layer {layer_number} : Mean activation value', fontsize=16)\n",
    "            ax[0].set_xlabel('Batch instances')\n",
    "            ax[0].set_ylabel('Mean')\n",
    "            ax[0].plot(range(df_stats_layer.shape[0]), df_stats_layer['mean'])\n",
    "\n",
    "            ax[1].set_title(f'Layer {layer_number} : Std deviation activation value', fontsize=16)\n",
    "            ax[1].set_xlabel('Batch instances')\n",
    "            ax[1].set_ylabel('Standard deviation')\n",
    "            ax[1].plot(range(df_stats_layer.shape[0]), df_stats_layer['std'])\n",
    "\n",
    "            ax[2].set_title(f'Layer {layer_number} : Percentage of activation values near zero', fontsize=16)\n",
    "            ax[2].set_xlabel('Batch instances')\n",
    "            ax[2].set_ylabel('Percentage')\n",
    "            ax[2].plot(range(df_stats_layer.shape[0]), df_stats_layer['near_zero']);\n",
    "\n",
    "            plot_buf = io.BytesIO()\n",
    "            plt.savefig(plot_buf, format='jpeg')\n",
    "            plt.close()\n",
    "\n",
    "            plot_buf.seek(0)\n",
    "            image = PIL.Image.open(plot_buf)\n",
    "            image = transforms.ToTensor()(image)\n",
    "            writer.add_image(\"Train activation stats/Activation stats layer \" + str(layer_number), image, epoch)\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "\n",
    "        vrunning_loss = [None] * 5\n",
    "        num_samples = [None] * 5\n",
    "        vepoch_loss_folds = [None] * 5\n",
    "        vepoch_accuracy_folds = [None] * 5\n",
    "        vepoch_utility_score_folds = [None] * 5\n",
    "\n",
    "        for fold_indice in range(5):    \n",
    "            vrunning_loss[fold_indice] = 0.0\n",
    "            num_samples[fold_indice] = 0\n",
    "\n",
    "            for batch in test_loader[fold_indice]:\n",
    "                inputs = batch[0].to('cuda')\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, inputs.double())\n",
    "\n",
    "                vrunning_loss[fold_indice] += loss.item() * inputs.size(0)\n",
    "                num_samples[fold_indice] += inputs.size(0)\n",
    "\n",
    "                vepoch_loss_folds[fold_indice] = vrunning_loss[fold_indice] / num_samples[fold_indice]\n",
    "\n",
    "            print('Epoch({}) - Fold {} - Validation Loss : {:.4f}'.format(epoch, fold_indice, vepoch_loss_folds[fold_indice]))        \n",
    "\n",
    "        # update epoch loss\n",
    "        vepoch_loss = sum(vepoch_loss_folds) / len(vepoch_loss_folds)\n",
    "        print('Epoch({}) - GLOBAL - Validation Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "\n",
    "        #print(f'Sum of model parameters ({epoch}):')\n",
    "        #[print(p.sum()) for p in model.parameters()]\n",
    "\n",
    "        writer.add_scalar(\"Global valid/Loss\", vepoch_loss, epoch)\n",
    "\n",
    "        for fold_indice in range(5):\n",
    "            writer.add_scalar(\"Fold valid Loss/Loss fold \"+str(fold_indice), vepoch_loss_folds[fold_indice], epoch)        \n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        # Check if Early Stopping\n",
    "\n",
    "        if (vepoch_loss > the_last_loss):\n",
    "            if (EARLY_STOPPING == True):\n",
    "                trigger_times += 1\n",
    "\n",
    "                print(f'Intermediate early stopping : vepoch_loss = {vepoch_loss:.4f}, the_last_loss={the_last_loss:.4f}')\n",
    "\n",
    "                if trigger_times >= patience:\n",
    "                    print('Meet Early stopping!')\n",
    "                    early_stopping_met = True\n",
    "                    ##torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "                    break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "            the_last_loss = vepoch_loss\n",
    "\n",
    "            the_best_epoch = epoch\n",
    "\n",
    "            # Save model for the best version so far\n",
    "            print(f'Saving model corresponding to last_loss == {the_last_loss}')\n",
    "            torch.save(model.state_dict(), MODEL_FILE_AE)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "    if (early_stopping_met == False):\n",
    "        print(\"Didn't meet early stopping : saving final model\")\n",
    "        # Save model if don't meet early stopping\n",
    "        torch.save(model.state_dict(), MODEL_FILE_AE)\n",
    "\n",
    "    writer.add_text(f\"Global valid/Loss\", f\"Best loss: {the_last_loss}\", the_best_epoch)\n",
    "\n",
    "    scores_results = {'Loss': the_last_loss, 'Loss folds': vepoch_loss_folds, 'Loss_std': np.std(vepoch_loss_folds)}\n",
    "\n",
    "    writer.add_text('Final score', str(scores_results))\n",
    "    writer.add_text('Batch size', str(BATCH_SIZE_AE))\n",
    "    writer.add_text('Patience', str(patience))\n",
    "    writer.add_text('Number of epochs', str(NUM_EPOCHS_AE))\n",
    "    writer.add_text('Best epoch', str(the_best_epoch))\n",
    "    writer.add_text('Number of parameters per layer', str([p.numel() for p in model.parameters()]))\n",
    "    writer.add_text('Model architecture', str(model).replace('\\n', '<BR>'))\n",
    "    writer.add_text('Comment', MODEL_COMMENT_AE)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    print('Training summary:')\n",
    "    print(scores_results)\n",
    "\n",
    "    model_AE = model\n",
    "    model_AE.eval()\n",
    "    print('Training ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With noise 0.01 and clustering :  \n",
    "{'Loss': 0.3849842190821132, 'Loss folds': [0.4477343427193263, 0.36315438263176464, 0.3901575283001523, 0.4046033569269988, 0.31993053528058724], 'Loss_std': 0.0425621624935266}  \n",
    "\n",
    "With noise 0.1 without clustering :\n",
    "{'Loss': 0.4013562969953418, 'Loss folds': [0.4683083206738427, 0.38134665501868015, 0.4085802369265447, 0.4217434697302297, 0.3315976695350142], 'Loss_std': 0.04519391402025046}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3105054402961592"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[FEATURES_LIST_TOTRAIN].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4781999277368514"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.08 / df[FEATURES_LIST_TOTRAIN].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(ts_train[0:5, :], model_AE(ts_train[0:5, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_AE(ts_train[0:5, :])[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_train[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(ts_train[50, 5], model_AE(ts_train[:, :])[50, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model that predicts resp n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifier_wrapper(BaseEstimator, ClassifierMixin):  \n",
    "    ''' Params passed as dictionnary to __init__, for example :\n",
    "        params_space = {\n",
    "       'features': FEATURES_LIST_TOTRAIN, \n",
    "        'random_state': 42,\n",
    "        'max_depth': 12,\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': 0.01,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.3,\n",
    "        'tree_method': 'gpu_hist'\n",
    "        }\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        self.fitted = False\n",
    "        \n",
    "        self.features = list(params['features'])\n",
    "        self.random_state = params['random_state']\n",
    "        self.max_depth = params['max_depth']\n",
    "        self.n_estimators = params['n_estimators']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.subsample = params['subsample']\n",
    "        self.colsample_bytree = params['colsample_bytree']\n",
    "        self.gamma = params['gamma']\n",
    "        self.tree_method = params['tree_method']  \n",
    "        \n",
    "        #print('Features assigned :')\n",
    "        #print(self.features)\n",
    "\n",
    "        self.model_internal = XGBClassifier(\n",
    "            random_state= self.random_state,\n",
    "            max_depth= self.max_depth,\n",
    "            n_estimators= self.n_estimators,\n",
    "            learning_rate= self.learning_rate,\n",
    "            subsample= self.subsample,\n",
    "            colsample_bytree= self.colsample_bytree,\n",
    "            tree_method= self.tree_method,\n",
    "            gamma = self.gamma,\n",
    "            #objective= 'binary:logistic',\n",
    "            #disable_default_eval_metric=True,\n",
    "            )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('Model used for fitting:')\n",
    "        print(self.model_internal)\n",
    "        self.model_internal.fit(X[self.features], y)\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        if (self.fitted == True):\n",
    "            print('predict called')\n",
    "            return(self.model_internal.predict(X[self.features]))\n",
    "        \n",
    "        else:\n",
    "            print('You must fit model first')\n",
    "            return(None)\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        if (self.fitted == True):\n",
    "            print('predict proba called')\n",
    "            return(self.model_internal.predict_proba(X[self.features]))\n",
    "        \n",
    "        else:\n",
    "            print('You must fit model first')\n",
    "            return(None)\n",
    "        \n",
    "\n",
    "    #def set_params(self, **parameters):\n",
    "    #    for parameter, value in parameters.items():\n",
    "    #        setattr(self, parameter, value)\n",
    "\n",
    "        \n",
    "    def score(self, X, y=None):        \n",
    "        print('Type of X:')\n",
    "        print(type(X))\n",
    "        \n",
    "        print('Shape of X:')\n",
    "        print(X.shape)\n",
    "        \n",
    "        print('Type of y:')\n",
    "        print(type(y))\n",
    "        \n",
    "        print('model fitted ?')\n",
    "        print(self.fitted) # Usually returns yes at this point when called by cross_val_score\n",
    "        \n",
    "        if y is None:\n",
    "            print('y is None')\n",
    "            y_preds = pd.Series(self.model_internal.predict(X.reset_index(drop=True)[self.features]))\n",
    "            \n",
    "        else: # cross_val_score goes there\n",
    "            print('y is not None')\n",
    "            y_preds = pd.Series(y)\n",
    "        \n",
    "        return(utility_function(X.reset_index(drop=True), y_preds)) \n",
    "    \n",
    "    def accuracy_score(self, X, y=None):\n",
    "        if y is None:\n",
    "            print('y is None in accuracy_score method : pass predictions as y to avoid launching predict')\n",
    "            y_preds = pd.Series(self.model_internal.predict(X.reset_index(drop=True)[self.features]))\n",
    "            \n",
    "        else: # cross_val_score goes there\n",
    "            #print('y is not None')\n",
    "            y_preds = pd.Series(y)\n",
    "            \n",
    "        return(accuracy_score(X['resp_positive'], y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:59:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.9,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate label of current step\n",
    "y_train1_resp_positive = (df.loc[folds_list_train1_unique, 'resp'] > 0).astype(np.byte)\n",
    "\n",
    "# Shift values of resp to get resp of step n-1\n",
    "y_train1_resp_n1_positive = y_train1_resp_positive.shift(1, fill_value=0)\n",
    "\n",
    "\n",
    "model_n1 = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 12,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.01,\n",
    "    subsample= 0.9,\n",
    "    colsample_bytree= 0.2,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    )\n",
    "\n",
    "model_n1.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], y_train1_resp_n1_positive, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model that predicts original fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_indexes = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_indexes.append([item for sublist in folds_list[fold_indice] for item in sublist])\n",
    "    \n",
    "for fold_number, fold_indexes_1fold in enumerate(fold_indexes):\n",
    "    df.loc[fold_indexes_1fold, 'fold_number'] = str(int(fold_number))\n",
    "    \n",
    "df.loc[df.shape[0] - 1, 'fold_number'] = str(int(NB_FOLDS - 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    480522\n",
       "3    478052\n",
       "0    477711\n",
       "2    477700\n",
       "4    476506\n",
       "Name: fold_number, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fold_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 140)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fold predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:03:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 10,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.02,\n",
    "    subsample= 0.5,\n",
    "    colsample_bytree= 0.6,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    #objective= 'binary:logistic',\n",
    "    #disable_default_eval_metric=True,\n",
    "    )\n",
    "\n",
    "#model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, ['fold_'+str(i) for i in range(NB_FOLDS)]])\n",
    "model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, 'fold_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    38357\n",
      "1    32039\n",
      "3    25586\n",
      "2    23961\n",
      "4    21159\n",
      "dtype: int64\n",
      "2    35328\n",
      "3    34457\n",
      "4    28933\n",
      "1    27646\n",
      "0    16086\n",
      "dtype: int64\n",
      "4    43336\n",
      "3    41516\n",
      "2    29827\n",
      "1    15522\n",
      "0    15450\n",
      "dtype: int64\n",
      "4    54848\n",
      "3    42308\n",
      "2    20733\n",
      "1    12215\n",
      "0    12048\n",
      "dtype: int64\n",
      "4    56691\n",
      "3    38288\n",
      "2    19587\n",
      "0    13913\n",
      "1    13501\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(pd.Series(model_xgb.predict(df.loc[folds_list_test[i], FEATURES_LIST_TOTRAIN])).value_counts()) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_scores': [0.27183881163980667, 0.19407511407511407, 0.2047840385579227, 0.2976250773819573, 0.39928863220171856]}\n",
      "{'precision_scores': [0.27183881163980667, 0.19407511407511407, 0.2047840385579227, 0.2976250773819573, 0.39928863220171856]}\n",
      "{'recall_scores': [0.27183881163980667, 0.19407511407511407, 0.2047840385579227, 0.2976250773819573, 0.39928863220171856]}\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS): \n",
    "    test_predictions = model_xgb.predict(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN])\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions))  \n",
    "    precision_scores.append(precision_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions, average='micro'))  \n",
    "    recall_scores.append(recall_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions, average='micro'))  \n",
    "\n",
    "    df_featimportance = pd.DataFrame(model_xgb.feature_importances_, index=df[FEATURES_LIST_TOTRAIN].columns, columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "    df_featimportance_cumulated = pd.concat([df_featimportance, pd.DataFrame({'% feat importance cumulé' : (df_featimportance['Importance'] / df_featimportance['Importance'].sum()).cumsum()})], axis=1)\n",
    "    #print(f'Feature importances for split {fold_indice}:')\n",
    "    #print(df_featimportance_cumulated)\n",
    "\n",
    "print({'accuracy_scores': accuracy_scores})\n",
    "print({'precision_scores': precision_scores})\n",
    "print({'recall_scores': recall_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_n1 = model_n1.predict_proba(df.loc[:, FEATURES_LIST_TOTRAIN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_fold = model_xgb.predict_proba(df.loc[:, FEATURES_LIST_TOTRAIN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['respn1_predict'] = preds_n1[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fold_predict'] = preds_fold[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_LIST_TOTRAIN = FEATURES_LIST_TOTRAIN + ['respn1_predict', 'fold_predict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 11zvcrka\n",
      "Sweep URL: https://wandb.ai/fboyer/janestreet-mlp/sweeps/11zvcrka\n"
     ]
    }
   ],
   "source": [
    "if (DO_SWEEP == True):\n",
    "    sweep_id = wandb.sweep(sweep_config, entity='fboyer', project=\"janestreet-mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_MLP = dict(\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay = WEIGHT_DECAY,\n",
    "        dropout = DROPOUT,\n",
    "        use_autoenc = USE_AUTOENC,\n",
    "        activation_function = 'leakyrelu',\n",
    "        hidden_size = 256,\n",
    "        )\n",
    "\n",
    "    if (DO_SWEEP == True):\n",
    "        #run = wandb.init()\n",
    "        #config_MLP = run.config\n",
    "        wandb.init(config=config_MLP) # this config_MLP is ignored when sweep\n",
    "        config_MLP = wandb.config\n",
    "\n",
    "    else:\n",
    "        wandb.init(project='janestreet-mlp', config=config_MLP)\n",
    "        config_MLP = wandb.config\n",
    "\n",
    "    print('Training started')\n",
    "    patience=20\n",
    "\n",
    "    print('Run config :')\n",
    "    print(\"config:\", dict(config_MLP))\n",
    "\n",
    "    utility_scores = [None] * 5\n",
    "    accuracy_scores = [None] * 5\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    ts_train = torch.tensor(df.loc[folds_list_train2_unique, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    ts_train_y = torch.tensor((df.loc[folds_list_train2_unique, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "    # Normalize data\n",
    "    ts_train_mean = torch.mean(ts_train, axis=0)\n",
    "    ts_train_std = torch.std(ts_train, axis=0)\n",
    "    #ts_train = pyStandardScale(ts_train, ts_train_mean, ts_train_std)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(ts_train, ts_train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config_MLP.batch_size, shuffle=True) # pin_memory : VOIR RESULTAT\n",
    "\n",
    "    ts_test = [None] * 5\n",
    "    ts_test_y = [None] * 5    \n",
    "    test_dataset = [None] * 5\n",
    "    test_loader = [None] * 5\n",
    "\n",
    "    for fold_indice in range(5):\n",
    "        ts_test[fold_indice] = torch.tensor(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "        ts_test_y[fold_indice] = torch.tensor((df.loc[folds_list_test[fold_indice], 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "        # Normalize\n",
    "        #ts_test[fold_indice] = pyStandardScale(ts_test[fold_indice], ts_train_mean, ts_train_std)\n",
    "\n",
    "        test_dataset[fold_indice] = torch.utils.data.TensorDataset(ts_test[fold_indice], ts_test_y[fold_indice])\n",
    "        test_loader[fold_indice] = torch.utils.data.DataLoader(test_dataset[fold_indice], batch_size=config_MLP.batch_size)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, AEncoder):\n",
    "        #def __init__(self):\n",
    "            super(MLP,self).__init__()\n",
    "\n",
    "            #self.act_n = torch.zeros((config_MLP.batch_size, ACT_N_SIZE))\n",
    "            if (ACT_N == True):\n",
    "                self.act_n = torch.zeros(ACT_N_SIZE, device='cuda').unsqueeze(0)\n",
    "                act_n_size = ACT_N_SIZE\n",
    "\n",
    "            else:\n",
    "                act_n_size = 0\n",
    "                \n",
    "            self.AEncoder = AEncoder\n",
    "\n",
    "            if (config_MLP.use_autoenc == 'encoder-decoder'):\n",
    "                #self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) + self.AEncoder.decoder[0].in_features, 200)\n",
    "                self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) * 2 + act_n_size, 200) # <= % near 0 élevé\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder'):\n",
    "                self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) + ENCODER_SIZE + act_n_size, 200) # <= % near 0 élevé\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder-only'):\n",
    "                self.layer1 = nn.Linear(ENCODER_SIZE + act_n_size, 200) # <= % near 0 élevé\n",
    "                \n",
    "            else:\n",
    "                self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) + act_n_size, 200) # <= % near 0 élevé\n",
    "\n",
    "            if (config_MLP.activation_function == 'relu'):\n",
    "                self.act1 = nn.ReLU()\n",
    "                \n",
    "            elif (config_MLP.activation_function == 'leakyrelu'):\n",
    "                self.act1 = nn.LeakyReLU()\n",
    "                \n",
    "            self.dropout1 = nn.Dropout(config_MLP.dropout)\n",
    "\n",
    "            self.layer2 = nn.Linear(200, 100)\n",
    "\n",
    "            if (config_MLP.activation_function == 'relu'):\n",
    "                self.act2 = nn.ReLU()\n",
    "                \n",
    "            elif (config_MLP.activation_function == 'leakyrelu'):\n",
    "                self.act2 = nn.LeakyReLU()\n",
    "\n",
    "            self.dropout2 = nn.Dropout(config_MLP.dropout)\n",
    "\n",
    "            self.layer3 = nn.Linear(100, 75)\n",
    "\n",
    "            if (config_MLP.activation_function == 'relu'):\n",
    "                self.act3 = nn.ReLU()\n",
    "                \n",
    "            elif (config_MLP.activation_function == 'leakyrelu'):\n",
    "                self.act3 = nn.LeakyReLU()\n",
    "\n",
    "            self.dropout3 = nn.Dropout(config_MLP.dropout)\n",
    "            \n",
    "            self.layer4 = nn.Linear(75, 50)\n",
    "\n",
    "            if (config_MLP.activation_function == 'relu'):\n",
    "                self.act4 = nn.ReLU()\n",
    "                \n",
    "            elif (config_MLP.activation_function == 'leakyrelu'):\n",
    "                self.act4 = nn.LeakyReLU()\n",
    "\n",
    "            self.dropout4 = nn.Dropout(config_MLP.dropout)\n",
    "\n",
    "            self.layer5 = nn.Linear(50, 1)\n",
    "            self.act5 = nn.Sigmoid()\n",
    "\n",
    "        def encoder(self, x):\n",
    "            self.AEncoder.eval()\n",
    "\n",
    "            encoded = self.AEncoder.encoder(x)\n",
    "\n",
    "            return encoded\n",
    "\n",
    "        def encoder_decoder(self, x):\n",
    "            self.AEncoder.eval()\n",
    "\n",
    "            encoded_decoded = self.AEncoder(x)\n",
    "\n",
    "            return encoded_decoded\n",
    "\n",
    "        def forward(self,x):\n",
    "            #x_encoded = self.encoder(x)\n",
    "\n",
    "            #x = torch.cat((x, x_encoded), dim=1)\n",
    "            if (config_MLP.use_autoenc == 'encoder-decoder'):\n",
    "                x_decoded = self.encoder_decoder(x)\n",
    "                x = torch.cat((x, x_decoded), dim=1)\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder'):\n",
    "                x_decoded = self.encoder(x)\n",
    "                x = torch.cat((x, x_decoded), dim=1)\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder-only'):\n",
    "                x_decoded = self.encoder(x)\n",
    "                x = x_decoded\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if (ACT_N == True):\n",
    "                x = torch.cat((x, self.act_n.expand(x.shape[0], ACT_N_SIZE)), dim=1)    \n",
    "\n",
    "            x = self.dropout1(self.act1(self.layer1(x)))\n",
    "            x = self.dropout2(self.act2(self.layer2(x)))\n",
    "            \n",
    "            x = self.dropout3(self.act3(self.layer3(x)))\n",
    "            x = self.dropout4(self.act4(self.layer4(x)))\n",
    "\n",
    "            x = self.act5(self.layer5(x))            \n",
    "\n",
    "            # Remove oldest previously saved output (located at the end of tensor => index -1 for not selecting it) of NN and replace by new one (which is x, that we assign at start of tensor)\n",
    "            # expand() because self.act_n when first assigned has only 1 line, but here it expands to number of lines in x (batch size)\n",
    "            if (ACT_N == True):\n",
    "                self.act_n = torch.cat((x, self.act_n[:, :-1].expand(x.shape[0], ACT_N_SIZE-1)), dim=1)\n",
    "\n",
    "\n",
    "            #print('self.act_n:')\n",
    "            #print(self.act_n)\n",
    "\n",
    "            return x        \n",
    "\n",
    "    class Model_Resnet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model_Resnet, self).__init__()\n",
    "            self.batch_norm0 = nn.BatchNorm1d(len(FEATURES_LIST_TOTRAIN))\n",
    "            self.dropout0 = nn.Dropout(0.2)\n",
    "\n",
    "            dropout_rate = config_MLP.dropout\n",
    "            hidden_size = config_MLP.hidden_size\n",
    "            self.dense1 = nn.Linear(len(FEATURES_LIST_TOTRAIN), hidden_size)\n",
    "            self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "            self.dropout1 = nn.Dropout(dropout_rate)\n",
    "            \n",
    "            self.cat1 = lambda a,b : torch.cat([a, b], 1)\n",
    "\n",
    "            self.dense2 = nn.Linear(hidden_size+len(FEATURES_LIST_TOTRAIN), hidden_size)\n",
    "            self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "            self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "            self.cat2 = lambda a,b : torch.cat([a, b], 1)\n",
    "            \n",
    "            self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "            self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "            self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "            self.cat3 = lambda a,b : torch.cat([a, b], 1)            \n",
    "            \n",
    "            self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "            self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n",
    "            self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "            self.cat4 = lambda a,b : torch.cat([a, b], 1)            \n",
    "            \n",
    "            self.dense5 = nn.Linear(hidden_size+hidden_size, 1)\n",
    "            self.act5 = nn.Sigmoid()\n",
    "\n",
    "            self.LeakyReLU1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "            self.LeakyReLU2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "            self.LeakyReLU3 = nn.LeakyReLU(negative_slope=0.01)\n",
    "            self.LeakyReLU4 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.dropout0(self.batch_norm0(x))\n",
    "\n",
    "            x1 = self.dropout1(self.LeakyReLU1(self.batch_norm1(self.dense1(x))))\n",
    "            x = self.cat1(x, x1)\n",
    "\n",
    "            x2 = self.dropout2(self.LeakyReLU2(self.batch_norm2(self.dense2(x))))\n",
    "            x = self.cat2(x1, x2)\n",
    "\n",
    "            x3 = self.dropout3(self.LeakyReLU3(self.batch_norm3(self.dense3(x))))\n",
    "            x = self.cat3(x2, x3)\n",
    "\n",
    "            x4 = self.dropout4(self.LeakyReLU4(self.batch_norm4(self.dense4(x))))\n",
    "\n",
    "            x = self.cat4(x3, x4)\n",
    "\n",
    "            x = self.act5(self.dense5(x))\n",
    "\n",
    "            return x        \n",
    "    \n",
    "    #model = MLP(model_AE).double().to('cuda')\n",
    "    \n",
    "    #for param in model.AEncoder.parameters():\n",
    "        #param.requires_grad = False\n",
    "\n",
    "    model = Model_Resnet().double().to('cuda')\n",
    "        \n",
    "    '''\n",
    "    model = nn.Sequential(\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(len(FEATURES_LIST_TOTRAIN), 200),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(200, 150),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(150, 100),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(100, 80),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(80, 60),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(60, 50),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(50, 40),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(40, 30),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(30, 20),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(config_MLP['dropout']),\n",
    "\n",
    "            nn.Linear(20, 10),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.45),\n",
    "\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid(),\n",
    "        ).double().to('cuda')\n",
    "    '''\n",
    "\n",
    "    #print('Number of model parameters :')\n",
    "    #numel_list = [p.numel() for p in model.parameters()]\n",
    "    #sum(numel_list), numel_list\n",
    "\n",
    "    loss_fn = nn.BCELoss().to('cuda')\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=config_MLP.learning_rate, weight_decay=config_MLP.weight_decay) \n",
    "    \n",
    "    # According to this, Adam can sometimes update parameters even if they are frozen with requires_grad : \n",
    "    # https://discuss.pytorch.org/t/why-is-it-when-i-call-require-grad-false-on-all-my-params-my-weights-in-the-network-would-still-update/22126/14\n",
    "    # So we exclude frozen parameters when calling the optimizer, with a filter\n",
    "    optimizer = optim.RAdam(filter(lambda p: p.requires_grad, model.parameters()), lr=config_MLP.learning_rate, weight_decay=config_MLP.weight_decay) \n",
    "\n",
    "    scheduler = None\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
    "    #                                                         max_lr=1e-4, epochs=config_MLP.epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "    #if (DO_SWEEP == False):\n",
    "    wandb.watch(model, loss_fn, log=\"all\", log_freq=1)\n",
    "\n",
    "    model.eval()\n",
    "    #start_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #start_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #print('Start Validation Accuracy: {:.4f}'.format(start_accuracy))\n",
    "    #print('Start Validation Utility: {:.4f}'.format(start_utility_score))\n",
    "\n",
    "\n",
    "    Val_Loss = 0\n",
    "    N_Samples = 0\n",
    "\n",
    "    the_last_loss = 100\n",
    "    the_last_utility_score = 0\n",
    "    the_last_accuracy = 0\n",
    "    trigger_times=0\n",
    "    early_stopping_met = False\n",
    "\n",
    "    for epoch in range(config_MLP.epochs): \n",
    "        running_loss = 0.0        \n",
    "\n",
    "        ### Call back to save activation stats (mean, std dev and near 0 values after activation functions)\n",
    "        # Setting hook for activation layers stats\n",
    "\n",
    "        hook_handles = []\n",
    "        save_output_activation_stats = []\n",
    "\n",
    "        for layer in model.modules():\n",
    "            if ('activation' in str(type(layer))):\n",
    "                save_output_activation_stats_1layer = SaveOutputActivationStats()\n",
    "                handle = layer.register_forward_hook(save_output_activation_stats_1layer)\n",
    "                save_output_activation_stats.append(save_output_activation_stats_1layer)\n",
    "                hook_handles.append(handle)    \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # with torch.set_grad_enabled(True):  # A enlever ? (réactive les gradients de l'auto encoder ?)\n",
    "            # Ce with couvrait jusqu'à scheduler.step()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "\n",
    "            #loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        # update local train loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # update global train loss\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print('Epoch({}) - Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "        writer.add_scalar(f\"Global train/loss\", epoch_loss, epoch)\n",
    "        wandb.log({'Global train/loss' : epoch_loss}, step=epoch)\n",
    "\n",
    "        # Write activation stats graphs\n",
    "        for layer_number,save_output_activation_stats_layer in enumerate(save_output_activation_stats):\n",
    "            df_stats_layer = pd.DataFrame(save_output_activation_stats_layer.outputs)\n",
    "\n",
    "            if ((df_stats_layer.shape[0] == 0) and (df_stats_layer.shape[1] == 0)):\n",
    "                print(f'Activation stats: No data returned for stats at layer {layer_number}')\n",
    "\n",
    "            else:\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(25, 4))\n",
    "\n",
    "                ax[0].set_title(f'Layer {layer_number} : Mean activation value', fontsize=16)\n",
    "                ax[0].set_xlabel('Batch instances')\n",
    "                ax[0].set_ylabel('Mean')\n",
    "                ax[0].plot(range(df_stats_layer.shape[0]), df_stats_layer['mean'])\n",
    "\n",
    "                ax[1].set_title(f'Layer {layer_number} : Std deviation activation value', fontsize=16)\n",
    "                ax[1].set_xlabel('Batch instances')\n",
    "                ax[1].set_ylabel('Standard deviation')\n",
    "                ax[1].plot(range(df_stats_layer.shape[0]), df_stats_layer['std'])\n",
    "\n",
    "                ax[2].set_title(f'Layer {layer_number} : Percentage of activation values near zero', fontsize=16)\n",
    "                ax[2].set_xlabel('Batch instances')\n",
    "                ax[2].set_ylabel('Percentage')\n",
    "                ax[2].plot(range(df_stats_layer.shape[0]), df_stats_layer['near_zero']);\n",
    "\n",
    "                plot_buf = io.BytesIO()\n",
    "                plt.savefig(plot_buf, format='jpeg')\n",
    "                plt.close()\n",
    "\n",
    "                plot_buf.seek(0)\n",
    "                image = PIL.Image.open(plot_buf)\n",
    "                image = transforms.ToTensor()(image)\n",
    "                writer.add_image(\"Train activation stats/Activation stats layer \" + str(layer_number), image, epoch)\n",
    "                wandb.log({'Train activation stats/Activation stats layer' + str(layer_number) : wandb.Image(image)}, step=epoch)\n",
    "                \n",
    "        # Validation \n",
    "        model.eval()\n",
    "\n",
    "        vrunning_loss = [None] * 5\n",
    "        num_samples = [None] * 5\n",
    "        vepoch_loss_folds = [None] * 5\n",
    "        vepoch_accuracy_folds = [None] * 5\n",
    "        vepoch_utility_score_folds = [None] * 5\n",
    "\n",
    "        for fold_indice in range(5):    \n",
    "            #print('Loop fold indice')\n",
    "            vrunning_loss[fold_indice] = 0.0\n",
    "            num_samples[fold_indice] = 0\n",
    "\n",
    "            for batch in test_loader[fold_indice]:\n",
    "                inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    #display_memory()\n",
    "                    outputs = model(inputs)\n",
    "                    #display_memory()\n",
    "                    loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "\n",
    "                vrunning_loss[fold_indice] += loss.item() * inputs.size(0)\n",
    "                num_samples[fold_indice] += labels.size(0)\n",
    "\n",
    "                vepoch_loss_folds[fold_indice] = vrunning_loss[fold_indice] / num_samples[fold_indice]\n",
    "\n",
    "            print('Epoch({}) - Fold {} - Validation Loss : {:.4f}'.format(epoch, fold_indice, vepoch_loss_folds[fold_indice]))\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                vepoch_accuracy_folds[fold_indice] = accuracy_score(ts_test_y[fold_indice].cpu().numpy(), (model(ts_test[fold_indice]).squeeze() > 0.5).cpu().numpy())\n",
    "                vepoch_utility_score_folds[fold_indice] = utility_function(df.loc[folds_list_test[fold_indice]], (model(ts_test[fold_indice]).squeeze() > 0.5).cpu().numpy())\n",
    "            print('Epoch({}) - Fold {} - Validation Accuracy : {:.4f}'.format(epoch, fold_indice, vepoch_accuracy_folds[fold_indice]))\n",
    "            print('Epoch({}) - Fold {} - Validation Utility score : {:.4f}'.format(epoch, fold_indice, vepoch_utility_score_folds[fold_indice]))\n",
    "\n",
    "\n",
    "        # update epoch loss\n",
    "        vepoch_loss = sum(vepoch_loss_folds) / len(vepoch_loss_folds)\n",
    "        vepoch_accuracy = sum(vepoch_accuracy_folds) / len(vepoch_accuracy_folds)\n",
    "        vepoch_utility_score = sum(vepoch_utility_score_folds) #/ len(vepoch_utility_score_folds)\n",
    "        print('Epoch({}) - GLOBAL - Validation Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "        print('Epoch({}) - GLOBAL - Validation Accuracy: {:.4f}'.format(epoch, vepoch_accuracy))\n",
    "        print('Epoch({}) - GLOBAL - Validation Utility score: {:.4f}'.format(epoch, vepoch_utility_score))\n",
    "\n",
    "        #print(f'Sum of model parameters ({epoch}):')\n",
    "        #[print(p.sum()) for p in model.parameters()]\n",
    "\n",
    "        writer.add_scalar(\"Global valid/Loss\", vepoch_loss, epoch)\n",
    "        writer.add_scalar(\"Global valid/Accuracy\", vepoch_accuracy, epoch)\n",
    "        writer.add_scalar(\"Global valid/Utility\", vepoch_utility_score, epoch)\n",
    "        wandb.log({'Global valid/Loss' : vepoch_loss}, step=epoch)\n",
    "        wandb.log({'Global valid/Accuracy' : vepoch_accuracy}, step=epoch)\n",
    "        wandb.log({'Global valid/Utility' : vepoch_utility_score}, step=epoch)\n",
    "\n",
    "        for fold_indice in range(5):\n",
    "            writer.add_scalar(\"Fold valid Loss/Loss fold \"+str(fold_indice), vepoch_loss_folds[fold_indice], epoch)\n",
    "            writer.add_scalar(\"Fold valid Accuracy/Accuracy fold \"+str(fold_indice), vepoch_accuracy_folds[fold_indice], epoch)\n",
    "            writer.add_scalar(\"Fold valid Utility/Utility fold \"+str(fold_indice), vepoch_utility_score_folds[fold_indice], epoch)\n",
    "\n",
    "            wandb.log({\"Fold valid Loss/Loss fold \"+str(fold_indice) : vepoch_loss_folds[fold_indice]}, step=epoch)\n",
    "            wandb.log({\"Fold valid Accuracy/Accuracy fold \"+str(fold_indice) : vepoch_accuracy_folds[fold_indice]}, step=epoch)\n",
    "            wandb.log({\"Fold valid Utility/Utility fold \"+str(fold_indice) : vepoch_utility_score_folds[fold_indice]}, step=epoch)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        # Check if Early Stopping\n",
    "        #if vepoch_loss > the_last_loss:\n",
    "        #if (vepoch_utility_score < the_last_utility_score) and (vepoch_loss > the_last_loss) and (vepoch_accuracy < the_last_accuracy):\n",
    "\n",
    "        if (vepoch_loss > the_last_loss):\n",
    "            if (EARLY_STOPPING == True):\n",
    "                trigger_times += 1\n",
    "\n",
    "                print(f'Intermediate early stopping : vepoch_loss = {vepoch_loss:.4f}, the_last_loss={the_last_loss:.4f}')\n",
    "                #print(f'Intermediate early stopping : vepoch_accuracy = {vepoch_accuracy:.4f}, the_last_utility_score={the_last_accuracy:.4f}')\n",
    "                #print(f'Intermediate early stopping : vepoch_utility_score = {vepoch_utility_score:.4f}, the_last_utility_score={the_last_utility_score:.4f}')\n",
    "\n",
    "                if trigger_times >= patience:\n",
    "                    print('Meet Early stopping!')\n",
    "                    early_stopping_met = True\n",
    "                    ##torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "                    break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "            the_last_loss = vepoch_loss\n",
    "            the_last_utility_score = vepoch_utility_score\n",
    "            the_last_accuracy = vepoch_accuracy\n",
    "\n",
    "            the_last_utility_score_folds = vepoch_utility_score_folds\n",
    "            the_last_accuracy_folds = vepoch_accuracy_folds\n",
    "\n",
    "            the_best_epoch = epoch\n",
    "\n",
    "            # Save model for the best version so far\n",
    "            print(f'Saving model corresponding to last_utility_score == {the_last_utility_score}')\n",
    "            torch.save(model.state_dict(), MODEL_FILE + '.' + wandb.run.name)\n",
    "            #wandb.save(MODE_FILE)\n",
    "            #torch.onnx.export(model, batch, MODEL_FILE_ONNX)\n",
    "            #wandb.save(MODEL_FILE_ONNX)\n",
    "            wandb.log({\"Best accuracy\" : the_last_accuracy}, step=epoch)\n",
    "            wandb.log({\"Best utility\" : the_last_utility_score}, step=epoch)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "    if (early_stopping_met == False):\n",
    "        print(\"Didn't meet early stopping : saving final model\")\n",
    "        # Save model if don't meet early stopping\n",
    "        torch.save(model.state_dict(), MODEL_FILE + '.' + wandb.run.name)\n",
    "        ###wandb.save(MODE_FILE)\n",
    "        #torch.onnx.export(model, test_loader[0], MODEL_FILE_ONNX)\n",
    "        #wandb.save(MODEL_FILE_ONNX)\n",
    "        wandb.log({\"Best accuracy\" : the_last_accuracy}, step=epoch)\n",
    "        wandb.log({\"Best utility\" : the_last_utility_score}, step=epoch)\n",
    "\n",
    "    #utility_scores.append(the_last_utility_score)\n",
    "    #accuracy_scores.append(the_last_accuracy)\n",
    "    writer.add_text(f\"Global valid/Utility\", f\"Best utility: {the_last_utility_score}\", the_best_epoch)\n",
    "\n",
    "    scores_results = {'utility_score': the_last_utility_score, 'utility_scores': the_last_utility_score_folds, 'utility_score_std': np.std(the_last_utility_score_folds), 'accuracy_scores': the_last_accuracy_folds}\n",
    "\n",
    "    writer.add_text('Final utility score', str(scores_results))\n",
    "    writer.add_text('Batch size', str(config_MLP.batch_size))\n",
    "    writer.add_text('Patience', str(patience))\n",
    "    writer.add_text('Number of epochs', str(NUM_EPOCHS))\n",
    "    writer.add_text('Best epoch', str(the_best_epoch))\n",
    "    writer.add_text('Number of parameters per layer', str([p.numel() for p in model.parameters()]))\n",
    "    writer.add_text('Model architecture', str(model).replace('\\n', '<BR>'))\n",
    "    writer.add_text('Comment', MODEL_COMMENT)\n",
    "\n",
    "\n",
    "    wandb.run.summary['Final utility score'] = str(scores_results)\n",
    "    wandb.run.summary['Batch size'] = str(config_MLP.batch_size)\n",
    "    wandb.run.summary['Patience'] = str(patience)\n",
    "    wandb.run.summary['Number of epochs'] = str(NUM_EPOCHS)\n",
    "    wandb.run.summary['Best epoch'] = str(the_best_epoch)\n",
    "    wandb.run.summary['Number of parameters per layer'] = str([p.numel() for p in model.parameters()])\n",
    "    wandb.run.summary['Model architecture'] = str(model).replace('\\n', '<BR>')\n",
    "    wandb.log({\"Best accuracy\" : the_last_accuracy}, step=epoch)\n",
    "    wandb.log({\"Best utility\" : the_last_utility_score}, step=epoch)\n",
    "\n",
    "    #wandb.log({\"Comment\" : MODEL_COMMENT})\n",
    "    wandb.run.summary['comment'] = MODEL_COMMENT\n",
    "\n",
    "    writer.close()\n",
    "    wandb.run.finish()\n",
    "    #run.finish()\n",
    "\n",
    "    print('Training summary:')\n",
    "    print(scores_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gcxq5iub with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 118270\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.48771444274846404\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0017845615782806487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_autoenc: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 2.352229140955047e-05\n",
      "2021-02-20 12:39:19.713661: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-02-20 12:39:19.713683: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.19<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">grateful-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/sweeps/11zvcrka\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/sweeps/11zvcrka</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/gcxq5iub\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/gcxq5iub</a><br/>\n",
       "                Run data is saved locally in <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_123918-gcxq5iub</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Run config :\n",
      "config: {'activation_function': 'leakyrelu', 'batch_size': 118270, 'dropout': 0.48771444274846404, 'hidden_size': 300, 'learning_rate': 0.0017845615782806487, 'use_autoenc': 'None', 'weight_decay': 2.352229140955047e-05, 'epochs': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-20 12:39:20.954855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(0) - Training Loss: 0.7266\n",
      "Epoch(0) - Fold 0 - Validation Loss : 0.6928\n",
      "Epoch(0) - Fold 0 - Validation Accuracy : 0.5106\n",
      "Epoch(0) - Fold 0 - Validation Utility score : 294.8773\n",
      "Epoch(0) - Fold 1 - Validation Loss : 0.6932\n",
      "Epoch(0) - Fold 1 - Validation Accuracy : 0.5065\n",
      "Epoch(0) - Fold 1 - Validation Utility score : 315.7475\n",
      "Epoch(0) - Fold 2 - Validation Loss : 0.6945\n",
      "Epoch(0) - Fold 2 - Validation Accuracy : 0.5067\n",
      "Epoch(0) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 3 - Validation Loss : 0.6940\n",
      "Epoch(0) - Fold 3 - Validation Accuracy : 0.5064\n",
      "Epoch(0) - Fold 3 - Validation Utility score : 34.8634\n",
      "Epoch(0) - Fold 4 - Validation Loss : 0.6931\n",
      "Epoch(0) - Fold 4 - Validation Accuracy : 0.5097\n",
      "Epoch(0) - Fold 4 - Validation Utility score : 530.8758\n",
      "Epoch(0) - GLOBAL - Validation Loss: 0.6935\n",
      "Epoch(0) - GLOBAL - Validation Accuracy: 0.5080\n",
      "Epoch(0) - GLOBAL - Validation Utility score: 1176.3640\n",
      "Saving model corresponding to last_utility_score == 1176.3640435486232\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.7122\n",
      "Epoch(1) - Fold 0 - Validation Loss : 0.6931\n",
      "Epoch(1) - Fold 0 - Validation Accuracy : 0.5118\n",
      "Epoch(1) - Fold 0 - Validation Utility score : 108.0350\n",
      "Epoch(1) - Fold 1 - Validation Loss : 0.6924\n",
      "Epoch(1) - Fold 1 - Validation Accuracy : 0.5125\n",
      "Epoch(1) - Fold 1 - Validation Utility score : 416.0142\n",
      "Epoch(1) - Fold 2 - Validation Loss : 0.6938\n",
      "Epoch(1) - Fold 2 - Validation Accuracy : 0.5066\n",
      "Epoch(1) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(1) - Fold 3 - Validation Loss : 0.6932\n",
      "Epoch(1) - Fold 3 - Validation Accuracy : 0.5122\n",
      "Epoch(1) - Fold 3 - Validation Utility score : 130.5595\n",
      "Epoch(1) - Fold 4 - Validation Loss : 0.6919\n",
      "Epoch(1) - Fold 4 - Validation Accuracy : 0.5135\n",
      "Epoch(1) - Fold 4 - Validation Utility score : 763.2132\n",
      "Epoch(1) - GLOBAL - Validation Loss: 0.6929\n",
      "Epoch(1) - GLOBAL - Validation Accuracy: 0.5113\n",
      "Epoch(1) - GLOBAL - Validation Utility score: 1417.8220\n",
      "Saving model corresponding to last_utility_score == 1417.821989428292\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.7058\n",
      "Epoch(2) - Fold 0 - Validation Loss : 0.6924\n",
      "Epoch(2) - Fold 0 - Validation Accuracy : 0.5124\n",
      "Epoch(2) - Fold 0 - Validation Utility score : 511.2260\n",
      "Epoch(2) - Fold 1 - Validation Loss : 0.6922\n",
      "Epoch(2) - Fold 1 - Validation Accuracy : 0.5148\n",
      "Epoch(2) - Fold 1 - Validation Utility score : 370.5251\n",
      "Epoch(2) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(2) - Fold 2 - Validation Accuracy : 0.5071\n",
      "Epoch(2) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(2) - Fold 3 - Validation Loss : 0.6931\n",
      "Epoch(2) - Fold 3 - Validation Accuracy : 0.5116\n",
      "Epoch(2) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(2) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(2) - Fold 4 - Validation Accuracy : 0.5161\n",
      "Epoch(2) - Fold 4 - Validation Utility score : 1171.0070\n",
      "Epoch(2) - GLOBAL - Validation Loss: 0.6926\n",
      "Epoch(2) - GLOBAL - Validation Accuracy: 0.5124\n",
      "Epoch(2) - GLOBAL - Validation Utility score: 2052.7581\n",
      "Saving model corresponding to last_utility_score == 2052.758110902571\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.7021\n",
      "Epoch(3) - Fold 0 - Validation Loss : 0.6920\n",
      "Epoch(3) - Fold 0 - Validation Accuracy : 0.5201\n",
      "Epoch(3) - Fold 0 - Validation Utility score : 505.8992\n",
      "Epoch(3) - Fold 1 - Validation Loss : 0.6916\n",
      "Epoch(3) - Fold 1 - Validation Accuracy : 0.5192\n",
      "Epoch(3) - Fold 1 - Validation Utility score : 542.5473\n",
      "Epoch(3) - Fold 2 - Validation Loss : 0.6935\n",
      "Epoch(3) - Fold 2 - Validation Accuracy : 0.5045\n",
      "Epoch(3) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(3) - Fold 3 - Validation Loss : 0.6930\n",
      "Epoch(3) - Fold 3 - Validation Accuracy : 0.5114\n",
      "Epoch(3) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(3) - Fold 4 - Validation Loss : 0.6912\n",
      "Epoch(3) - Fold 4 - Validation Accuracy : 0.5188\n",
      "Epoch(3) - Fold 4 - Validation Utility score : 1285.6135\n",
      "Epoch(3) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(3) - GLOBAL - Validation Accuracy: 0.5148\n",
      "Epoch(3) - GLOBAL - Validation Utility score: 2334.0600\n",
      "Saving model corresponding to last_utility_score == 2334.060036088827\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.6997\n",
      "Epoch(4) - Fold 0 - Validation Loss : 0.6921\n",
      "Epoch(4) - Fold 0 - Validation Accuracy : 0.5218\n",
      "Epoch(4) - Fold 0 - Validation Utility score : 477.9690\n",
      "Epoch(4) - Fold 1 - Validation Loss : 0.6915\n",
      "Epoch(4) - Fold 1 - Validation Accuracy : 0.5210\n",
      "Epoch(4) - Fold 1 - Validation Utility score : 628.2560\n",
      "Epoch(4) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(4) - Fold 2 - Validation Accuracy : 0.5048\n",
      "Epoch(4) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(4) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(4) - Fold 3 - Validation Accuracy : 0.5128\n",
      "Epoch(4) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(4) - Fold 4 - Validation Loss : 0.6913\n",
      "Epoch(4) - Fold 4 - Validation Accuracy : 0.5186\n",
      "Epoch(4) - Fold 4 - Validation Utility score : 1227.4153\n",
      "Epoch(4) - GLOBAL - Validation Loss: 0.6923\n",
      "Epoch(4) - GLOBAL - Validation Accuracy: 0.5158\n",
      "Epoch(4) - GLOBAL - Validation Utility score: 2333.6403\n",
      "Intermediate early stopping : vepoch_loss = 0.6923, the_last_loss=0.6922\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 0.6982\n",
      "Epoch(5) - Fold 0 - Validation Loss : 0.6920\n",
      "Epoch(5) - Fold 0 - Validation Accuracy : 0.5237\n",
      "Epoch(5) - Fold 0 - Validation Utility score : 510.0732\n",
      "Epoch(5) - Fold 1 - Validation Loss : 0.6915\n",
      "Epoch(5) - Fold 1 - Validation Accuracy : 0.5208\n",
      "Epoch(5) - Fold 1 - Validation Utility score : 584.3048\n",
      "Epoch(5) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(5) - Fold 2 - Validation Accuracy : 0.5057\n",
      "Epoch(5) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(5) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(5) - Fold 3 - Validation Accuracy : 0.5127\n",
      "Epoch(5) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(5) - Fold 4 - Validation Loss : 0.6912\n",
      "Epoch(5) - Fold 4 - Validation Accuracy : 0.5187\n",
      "Epoch(5) - Fold 4 - Validation Utility score : 1167.9351\n",
      "Epoch(5) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(5) - GLOBAL - Validation Accuracy: 0.5163\n",
      "Epoch(5) - GLOBAL - Validation Utility score: 2262.3131\n",
      "Saving model corresponding to last_utility_score == 2262.3130786539714\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 0.6970\n",
      "Epoch(6) - Fold 0 - Validation Loss : 0.6920\n",
      "Epoch(6) - Fold 0 - Validation Accuracy : 0.5245\n",
      "Epoch(6) - Fold 0 - Validation Utility score : 482.9698\n",
      "Epoch(6) - Fold 1 - Validation Loss : 0.6915\n",
      "Epoch(6) - Fold 1 - Validation Accuracy : 0.5203\n",
      "Epoch(6) - Fold 1 - Validation Utility score : 603.8685\n",
      "Epoch(6) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(6) - Fold 2 - Validation Accuracy : 0.5070\n",
      "Epoch(6) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(6) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(6) - Fold 3 - Validation Accuracy : 0.5136\n",
      "Epoch(6) - Fold 3 - Validation Utility score : 8.9474\n",
      "Epoch(6) - Fold 4 - Validation Loss : 0.6912\n",
      "Epoch(6) - Fold 4 - Validation Accuracy : 0.5187\n",
      "Epoch(6) - Fold 4 - Validation Utility score : 1134.5349\n",
      "Epoch(6) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(6) - GLOBAL - Validation Accuracy: 0.5168\n",
      "Epoch(6) - GLOBAL - Validation Utility score: 2230.3206\n",
      "Intermediate early stopping : vepoch_loss = 0.6922, the_last_loss=0.6922\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 0.6959\n",
      "Epoch(7) - Fold 0 - Validation Loss : 0.6918\n",
      "Epoch(7) - Fold 0 - Validation Accuracy : 0.5230\n",
      "Epoch(7) - Fold 0 - Validation Utility score : 493.9915\n",
      "Epoch(7) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(7) - Fold 1 - Validation Accuracy : 0.5189\n",
      "Epoch(7) - Fold 1 - Validation Utility score : 607.9497\n",
      "Epoch(7) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(7) - Fold 2 - Validation Accuracy : 0.5073\n",
      "Epoch(7) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(7) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(7) - Fold 3 - Validation Accuracy : 0.5135\n",
      "Epoch(7) - Fold 3 - Validation Utility score : 2.0162\n",
      "Epoch(7) - Fold 4 - Validation Loss : 0.6911\n",
      "Epoch(7) - Fold 4 - Validation Accuracy : 0.5213\n",
      "Epoch(7) - Fold 4 - Validation Utility score : 1214.4980\n",
      "Epoch(7) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(7) - GLOBAL - Validation Accuracy: 0.5168\n",
      "Epoch(7) - GLOBAL - Validation Utility score: 2318.4554\n",
      "Saving model corresponding to last_utility_score == 2318.4553634512295\n",
      "\n",
      "\n",
      "Epoch(8) - Training Loss: 0.6950\n",
      "Epoch(8) - Fold 0 - Validation Loss : 0.6917\n",
      "Epoch(8) - Fold 0 - Validation Accuracy : 0.5222\n",
      "Epoch(8) - Fold 0 - Validation Utility score : 294.4287\n",
      "Epoch(8) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(8) - Fold 1 - Validation Accuracy : 0.5220\n",
      "Epoch(8) - Fold 1 - Validation Utility score : 754.9044\n",
      "Epoch(8) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(8) - Fold 2 - Validation Accuracy : 0.5066\n",
      "Epoch(8) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(8) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(8) - Fold 3 - Validation Accuracy : 0.5128\n",
      "Epoch(8) - Fold 3 - Validation Utility score : 10.8554\n",
      "Epoch(8) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(8) - Fold 4 - Validation Accuracy : 0.5225\n",
      "Epoch(8) - Fold 4 - Validation Utility score : 1157.4956\n",
      "Epoch(8) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(8) - GLOBAL - Validation Accuracy: 0.5173\n",
      "Epoch(8) - GLOBAL - Validation Utility score: 2217.6840\n",
      "Saving model corresponding to last_utility_score == 2217.684007166481\n",
      "\n",
      "\n",
      "Epoch(9) - Training Loss: 0.6944\n",
      "Epoch(9) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(9) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(9) - Fold 0 - Validation Utility score : 372.1670\n",
      "Epoch(9) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(9) - Fold 1 - Validation Accuracy : 0.5214\n",
      "Epoch(9) - Fold 1 - Validation Utility score : 852.4470\n",
      "Epoch(9) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(9) - Fold 2 - Validation Accuracy : 0.5076\n",
      "Epoch(9) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(9) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(9) - Fold 3 - Validation Accuracy : 0.5126\n",
      "Epoch(9) - Fold 3 - Validation Utility score : 7.6827\n",
      "Epoch(9) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(9) - Fold 4 - Validation Accuracy : 0.5222\n",
      "Epoch(9) - Fold 4 - Validation Utility score : 1240.6805\n",
      "Epoch(9) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(9) - GLOBAL - Validation Accuracy: 0.5174\n",
      "Epoch(9) - GLOBAL - Validation Utility score: 2472.9772\n",
      "Saving model corresponding to last_utility_score == 2472.977202520319\n",
      "\n",
      "\n",
      "Epoch(10) - Training Loss: 0.6938\n",
      "Epoch(10) - Fold 0 - Validation Loss : 0.6919\n",
      "Epoch(10) - Fold 0 - Validation Accuracy : 0.5217\n",
      "Epoch(10) - Fold 0 - Validation Utility score : 412.1358\n",
      "Epoch(10) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(10) - Fold 1 - Validation Accuracy : 0.5203\n",
      "Epoch(10) - Fold 1 - Validation Utility score : 783.5943\n",
      "Epoch(10) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(10) - Fold 2 - Validation Accuracy : 0.5053\n",
      "Epoch(10) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(10) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(10) - Fold 3 - Validation Accuracy : 0.5129\n",
      "Epoch(10) - Fold 3 - Validation Utility score : 5.9464\n",
      "Epoch(10) - Fold 4 - Validation Loss : 0.6909\n",
      "Epoch(10) - Fold 4 - Validation Accuracy : 0.5237\n",
      "Epoch(10) - Fold 4 - Validation Utility score : 1162.1461\n",
      "Epoch(10) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(10) - GLOBAL - Validation Accuracy: 0.5168\n",
      "Epoch(10) - GLOBAL - Validation Utility score: 2363.8226\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6920\n",
      "\n",
      "\n",
      "Epoch(11) - Training Loss: 0.6934\n",
      "Epoch(11) - Fold 0 - Validation Loss : 0.6917\n",
      "Epoch(11) - Fold 0 - Validation Accuracy : 0.5231\n",
      "Epoch(11) - Fold 0 - Validation Utility score : 307.6302\n",
      "Epoch(11) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(11) - Fold 1 - Validation Accuracy : 0.5233\n",
      "Epoch(11) - Fold 1 - Validation Utility score : 809.0584\n",
      "Epoch(11) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(11) - Fold 2 - Validation Accuracy : 0.5070\n",
      "Epoch(11) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(11) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(11) - Fold 3 - Validation Accuracy : 0.5145\n",
      "Epoch(11) - Fold 3 - Validation Utility score : 0.0810\n",
      "Epoch(11) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(11) - Fold 4 - Validation Accuracy : 0.5244\n",
      "Epoch(11) - Fold 4 - Validation Utility score : 1331.2502\n",
      "Epoch(11) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(11) - GLOBAL - Validation Accuracy: 0.5184\n",
      "Epoch(11) - GLOBAL - Validation Utility score: 2448.0198\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6920\n",
      "\n",
      "\n",
      "Epoch(12) - Training Loss: 0.6931\n",
      "Epoch(12) - Fold 0 - Validation Loss : 0.6917\n",
      "Epoch(12) - Fold 0 - Validation Accuracy : 0.5220\n",
      "Epoch(12) - Fold 0 - Validation Utility score : 401.4679\n",
      "Epoch(12) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(12) - Fold 1 - Validation Accuracy : 0.5218\n",
      "Epoch(12) - Fold 1 - Validation Utility score : 836.2862\n",
      "Epoch(12) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(12) - Fold 2 - Validation Accuracy : 0.5051\n",
      "Epoch(12) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(12) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(12) - Fold 3 - Validation Accuracy : 0.5136\n",
      "Epoch(12) - Fold 3 - Validation Utility score : 21.7111\n",
      "Epoch(12) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(12) - Fold 4 - Validation Accuracy : 0.5244\n",
      "Epoch(12) - Fold 4 - Validation Utility score : 1194.4836\n",
      "Epoch(12) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(12) - GLOBAL - Validation Accuracy: 0.5174\n",
      "Epoch(12) - GLOBAL - Validation Utility score: 2453.9488\n",
      "Saving model corresponding to last_utility_score == 2453.9487690710193\n",
      "\n",
      "\n",
      "Epoch(13) - Training Loss: 0.6929\n",
      "Epoch(13) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(13) - Fold 0 - Validation Accuracy : 0.5231\n",
      "Epoch(13) - Fold 0 - Validation Utility score : 323.5873\n",
      "Epoch(13) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(13) - Fold 1 - Validation Accuracy : 0.5230\n",
      "Epoch(13) - Fold 1 - Validation Utility score : 871.1783\n",
      "Epoch(13) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(13) - Fold 2 - Validation Accuracy : 0.5072\n",
      "Epoch(13) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(13) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(13) - Fold 3 - Validation Accuracy : 0.5152\n",
      "Epoch(13) - Fold 3 - Validation Utility score : 7.9644\n",
      "Epoch(13) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(13) - Fold 4 - Validation Accuracy : 0.5261\n",
      "Epoch(13) - Fold 4 - Validation Utility score : 1352.5255\n",
      "Epoch(13) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(13) - GLOBAL - Validation Accuracy: 0.5189\n",
      "Epoch(13) - GLOBAL - Validation Utility score: 2555.2555\n",
      "Saving model corresponding to last_utility_score == 2555.2555320788697\n",
      "\n",
      "\n",
      "Epoch(14) - Training Loss: 0.6927\n",
      "Epoch(14) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(14) - Fold 0 - Validation Accuracy : 0.5216\n",
      "Epoch(14) - Fold 0 - Validation Utility score : 295.9267\n",
      "Epoch(14) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(14) - Fold 1 - Validation Accuracy : 0.5237\n",
      "Epoch(14) - Fold 1 - Validation Utility score : 1006.7106\n",
      "Epoch(14) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(14) - Fold 2 - Validation Accuracy : 0.5072\n",
      "Epoch(14) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(14) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(14) - Fold 3 - Validation Accuracy : 0.5145\n",
      "Epoch(14) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(14) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(14) - Fold 4 - Validation Accuracy : 0.5255\n",
      "Epoch(14) - Fold 4 - Validation Utility score : 1281.4465\n",
      "Epoch(14) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(14) - GLOBAL - Validation Accuracy: 0.5185\n",
      "Epoch(14) - GLOBAL - Validation Utility score: 2584.0837\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(15) - Training Loss: 0.6924\n",
      "Epoch(15) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(15) - Fold 0 - Validation Accuracy : 0.5219\n",
      "Epoch(15) - Fold 0 - Validation Utility score : 380.3316\n",
      "Epoch(15) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(15) - Fold 1 - Validation Accuracy : 0.5218\n",
      "Epoch(15) - Fold 1 - Validation Utility score : 863.9309\n",
      "Epoch(15) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(15) - Fold 2 - Validation Accuracy : 0.5075\n",
      "Epoch(15) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(15) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(15) - Fold 3 - Validation Accuracy : 0.5131\n",
      "Epoch(15) - Fold 3 - Validation Utility score : 0.4467\n",
      "Epoch(15) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(15) - Fold 4 - Validation Accuracy : 0.5248\n",
      "Epoch(15) - Fold 4 - Validation Utility score : 1248.2490\n",
      "Epoch(15) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(15) - GLOBAL - Validation Accuracy: 0.5178\n",
      "Epoch(15) - GLOBAL - Validation Utility score: 2492.9582\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(16) - Training Loss: 0.6921\n",
      "Epoch(16) - Fold 0 - Validation Loss : 0.6917\n",
      "Epoch(16) - Fold 0 - Validation Accuracy : 0.5217\n",
      "Epoch(16) - Fold 0 - Validation Utility score : 362.5870\n",
      "Epoch(16) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(16) - Fold 1 - Validation Accuracy : 0.5215\n",
      "Epoch(16) - Fold 1 - Validation Utility score : 874.8336\n",
      "Epoch(16) - Fold 2 - Validation Loss : 0.6938\n",
      "Epoch(16) - Fold 2 - Validation Accuracy : 0.5070\n",
      "Epoch(16) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(16) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(16) - Fold 3 - Validation Accuracy : 0.5127\n",
      "Epoch(16) - Fold 3 - Validation Utility score : 0.0000\n",
      "Epoch(16) - Fold 4 - Validation Loss : 0.6906\n",
      "Epoch(16) - Fold 4 - Validation Accuracy : 0.5246\n",
      "Epoch(16) - Fold 4 - Validation Utility score : 1312.7673\n",
      "Epoch(16) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(16) - GLOBAL - Validation Accuracy: 0.5175\n",
      "Epoch(16) - GLOBAL - Validation Utility score: 2550.1879\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(17) - Training Loss: 0.6920\n",
      "Epoch(17) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(17) - Fold 0 - Validation Accuracy : 0.5217\n",
      "Epoch(17) - Fold 0 - Validation Utility score : 347.2289\n",
      "Epoch(17) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(17) - Fold 1 - Validation Accuracy : 0.5220\n",
      "Epoch(17) - Fold 1 - Validation Utility score : 750.0954\n",
      "Epoch(17) - Fold 2 - Validation Loss : 0.6938\n",
      "Epoch(17) - Fold 2 - Validation Accuracy : 0.5061\n",
      "Epoch(17) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(17) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(17) - Fold 3 - Validation Accuracy : 0.5139\n",
      "Epoch(17) - Fold 3 - Validation Utility score : 0.8802\n",
      "Epoch(17) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(17) - Fold 4 - Validation Accuracy : 0.5252\n",
      "Epoch(17) - Fold 4 - Validation Utility score : 1256.8146\n",
      "Epoch(17) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(17) - GLOBAL - Validation Accuracy: 0.5178\n",
      "Epoch(17) - GLOBAL - Validation Utility score: 2355.0191\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(18) - Training Loss: 0.6918\n",
      "Epoch(18) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(18) - Fold 0 - Validation Accuracy : 0.5240\n",
      "Epoch(18) - Fold 0 - Validation Utility score : 347.9973\n",
      "Epoch(18) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(18) - Fold 1 - Validation Accuracy : 0.5241\n",
      "Epoch(18) - Fold 1 - Validation Utility score : 920.6754\n",
      "Epoch(18) - Fold 2 - Validation Loss : 0.6938\n",
      "Epoch(18) - Fold 2 - Validation Accuracy : 0.5072\n",
      "Epoch(18) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(18) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(18) - Fold 3 - Validation Accuracy : 0.5143\n",
      "Epoch(18) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(18) - Fold 4 - Validation Loss : 0.6906\n",
      "Epoch(18) - Fold 4 - Validation Accuracy : 0.5254\n",
      "Epoch(18) - Fold 4 - Validation Utility score : 1482.0980\n",
      "Epoch(18) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(18) - GLOBAL - Validation Accuracy: 0.5190\n",
      "Epoch(18) - GLOBAL - Validation Utility score: 2750.7707\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(19) - Training Loss: 0.6916\n",
      "Epoch(19) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(19) - Fold 0 - Validation Accuracy : 0.5217\n",
      "Epoch(19) - Fold 0 - Validation Utility score : 368.9147\n",
      "Epoch(19) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(19) - Fold 1 - Validation Accuracy : 0.5230\n",
      "Epoch(19) - Fold 1 - Validation Utility score : 1012.6644\n",
      "Epoch(19) - Fold 2 - Validation Loss : 0.6939\n",
      "Epoch(19) - Fold 2 - Validation Accuracy : 0.5060\n",
      "Epoch(19) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(19) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(19) - Fold 3 - Validation Accuracy : 0.5133\n",
      "Epoch(19) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(19) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(19) - Fold 4 - Validation Accuracy : 0.5244\n",
      "Epoch(19) - Fold 4 - Validation Utility score : 1294.4382\n",
      "Epoch(19) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(19) - GLOBAL - Validation Accuracy: 0.5177\n",
      "Epoch(19) - GLOBAL - Validation Utility score: 2676.0172\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(20) - Training Loss: 0.6916\n",
      "Epoch(20) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(20) - Fold 0 - Validation Accuracy : 0.5230\n",
      "Epoch(20) - Fold 0 - Validation Utility score : 347.8912\n",
      "Epoch(20) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(20) - Fold 1 - Validation Accuracy : 0.5218\n",
      "Epoch(20) - Fold 1 - Validation Utility score : 833.8485\n",
      "Epoch(20) - Fold 2 - Validation Loss : 0.6940\n",
      "Epoch(20) - Fold 2 - Validation Accuracy : 0.5053\n",
      "Epoch(20) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(20) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(20) - Fold 3 - Validation Accuracy : 0.5130\n",
      "Epoch(20) - Fold 3 - Validation Utility score : 0.7954\n",
      "Epoch(20) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(20) - Fold 4 - Validation Accuracy : 0.5252\n",
      "Epoch(20) - Fold 4 - Validation Utility score : 1267.3690\n",
      "Epoch(20) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(20) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(20) - GLOBAL - Validation Utility score: 2449.9040\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(21) - Training Loss: 0.6914\n",
      "Epoch(21) - Fold 0 - Validation Loss : 0.6917\n",
      "Epoch(21) - Fold 0 - Validation Accuracy : 0.5216\n",
      "Epoch(21) - Fold 0 - Validation Utility score : 333.3869\n",
      "Epoch(21) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(21) - Fold 1 - Validation Accuracy : 0.5224\n",
      "Epoch(21) - Fold 1 - Validation Utility score : 819.8648\n",
      "Epoch(21) - Fold 2 - Validation Loss : 0.6940\n",
      "Epoch(21) - Fold 2 - Validation Accuracy : 0.5065\n",
      "Epoch(21) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(21) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(21) - Fold 3 - Validation Accuracy : 0.5137\n",
      "Epoch(21) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(21) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(21) - Fold 4 - Validation Accuracy : 0.5268\n",
      "Epoch(21) - Fold 4 - Validation Utility score : 1360.6711\n",
      "Epoch(21) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(21) - GLOBAL - Validation Accuracy: 0.5182\n",
      "Epoch(21) - GLOBAL - Validation Utility score: 2513.9229\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(22) - Training Loss: 0.6913\n",
      "Epoch(22) - Fold 0 - Validation Loss : 0.6919\n",
      "Epoch(22) - Fold 0 - Validation Accuracy : 0.5226\n",
      "Epoch(22) - Fold 0 - Validation Utility score : 298.0604\n",
      "Epoch(22) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(22) - Fold 1 - Validation Accuracy : 0.5234\n",
      "Epoch(22) - Fold 1 - Validation Utility score : 841.1045\n",
      "Epoch(22) - Fold 2 - Validation Loss : 0.6941\n",
      "Epoch(22) - Fold 2 - Validation Accuracy : 0.5060\n",
      "Epoch(22) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(22) - Fold 3 - Validation Loss : 0.6930\n",
      "Epoch(22) - Fold 3 - Validation Accuracy : 0.5129\n",
      "Epoch(22) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(22) - Fold 4 - Validation Loss : 0.6906\n",
      "Epoch(22) - Fold 4 - Validation Accuracy : 0.5264\n",
      "Epoch(22) - Fold 4 - Validation Utility score : 1477.5430\n",
      "Epoch(22) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(22) - GLOBAL - Validation Accuracy: 0.5182\n",
      "Epoch(22) - GLOBAL - Validation Utility score: 2616.7079\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(23) - Training Loss: 0.6913\n",
      "Epoch(23) - Fold 0 - Validation Loss : 0.6917\n",
      "Epoch(23) - Fold 0 - Validation Accuracy : 0.5235\n",
      "Epoch(23) - Fold 0 - Validation Utility score : 376.5757\n",
      "Epoch(23) - Fold 1 - Validation Loss : 0.6909\n",
      "Epoch(23) - Fold 1 - Validation Accuracy : 0.5232\n",
      "Epoch(23) - Fold 1 - Validation Utility score : 996.8821\n",
      "Epoch(23) - Fold 2 - Validation Loss : 0.6941\n",
      "Epoch(23) - Fold 2 - Validation Accuracy : 0.5060\n",
      "Epoch(23) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(23) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(23) - Fold 3 - Validation Accuracy : 0.5131\n",
      "Epoch(23) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(23) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(23) - Fold 4 - Validation Accuracy : 0.5266\n",
      "Epoch(23) - Fold 4 - Validation Utility score : 1499.7695\n",
      "Epoch(23) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(23) - GLOBAL - Validation Accuracy: 0.5185\n",
      "Epoch(23) - GLOBAL - Validation Utility score: 2873.2274\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(24) - Training Loss: 0.6911\n",
      "Epoch(24) - Fold 0 - Validation Loss : 0.6919\n",
      "Epoch(24) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(24) - Fold 0 - Validation Utility score : 386.6681\n",
      "Epoch(24) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(24) - Fold 1 - Validation Accuracy : 0.5230\n",
      "Epoch(24) - Fold 1 - Validation Utility score : 892.0171\n",
      "Epoch(24) - Fold 2 - Validation Loss : 0.6941\n",
      "Epoch(24) - Fold 2 - Validation Accuracy : 0.5068\n",
      "Epoch(24) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(24) - Fold 3 - Validation Loss : 0.6931\n",
      "Epoch(24) - Fold 3 - Validation Accuracy : 0.5118\n",
      "Epoch(24) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(24) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(24) - Fold 4 - Validation Accuracy : 0.5271\n",
      "Epoch(24) - Fold 4 - Validation Utility score : 1446.0162\n",
      "Epoch(24) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(24) - GLOBAL - Validation Accuracy: 0.5184\n",
      "Epoch(24) - GLOBAL - Validation Utility score: 2724.7013\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(25) - Training Loss: 0.6910\n",
      "Epoch(25) - Fold 0 - Validation Loss : 0.6919\n",
      "Epoch(25) - Fold 0 - Validation Accuracy : 0.5226\n",
      "Epoch(25) - Fold 0 - Validation Utility score : 411.6018\n",
      "Epoch(25) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(25) - Fold 1 - Validation Accuracy : 0.5216\n",
      "Epoch(25) - Fold 1 - Validation Utility score : 936.3138\n",
      "Epoch(25) - Fold 2 - Validation Loss : 0.6941\n",
      "Epoch(25) - Fold 2 - Validation Accuracy : 0.5060\n",
      "Epoch(25) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(25) - Fold 3 - Validation Loss : 0.6931\n",
      "Epoch(25) - Fold 3 - Validation Accuracy : 0.5119\n",
      "Epoch(25) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(25) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(25) - Fold 4 - Validation Accuracy : 0.5260\n",
      "Epoch(25) - Fold 4 - Validation Utility score : 1335.7799\n",
      "Epoch(25) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(25) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(25) - GLOBAL - Validation Utility score: 2683.6955\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(26) - Training Loss: 0.6909\n",
      "Epoch(26) - Fold 0 - Validation Loss : 0.6918\n",
      "Epoch(26) - Fold 0 - Validation Accuracy : 0.5219\n",
      "Epoch(26) - Fold 0 - Validation Utility score : 335.7481\n",
      "Epoch(26) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(26) - Fold 1 - Validation Accuracy : 0.5229\n",
      "Epoch(26) - Fold 1 - Validation Utility score : 991.9965\n",
      "Epoch(26) - Fold 2 - Validation Loss : 0.6941\n",
      "Epoch(26) - Fold 2 - Validation Accuracy : 0.5070\n",
      "Epoch(26) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(26) - Fold 3 - Validation Loss : 0.6930\n",
      "Epoch(26) - Fold 3 - Validation Accuracy : 0.5124\n",
      "Epoch(26) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(26) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(26) - Fold 4 - Validation Accuracy : 0.5262\n",
      "Epoch(26) - Fold 4 - Validation Utility score : 1340.4990\n",
      "Epoch(26) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(26) - GLOBAL - Validation Accuracy: 0.5181\n",
      "Epoch(26) - GLOBAL - Validation Utility score: 2668.2436\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(27) - Training Loss: 0.6908\n",
      "Epoch(27) - Fold 0 - Validation Loss : 0.6918\n",
      "Epoch(27) - Fold 0 - Validation Accuracy : 0.5225\n",
      "Epoch(27) - Fold 0 - Validation Utility score : 384.4096\n",
      "Epoch(27) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(27) - Fold 1 - Validation Accuracy : 0.5212\n",
      "Epoch(27) - Fold 1 - Validation Utility score : 895.9128\n",
      "Epoch(27) - Fold 2 - Validation Loss : 0.6941\n",
      "Epoch(27) - Fold 2 - Validation Accuracy : 0.5067\n",
      "Epoch(27) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(27) - Fold 3 - Validation Loss : 0.6930\n",
      "Epoch(27) - Fold 3 - Validation Accuracy : 0.5117\n",
      "Epoch(27) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(27) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(27) - Fold 4 - Validation Accuracy : 0.5252\n",
      "Epoch(27) - Fold 4 - Validation Utility score : 1334.1612\n",
      "Epoch(27) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(27) - GLOBAL - Validation Accuracy: 0.5174\n",
      "Epoch(27) - GLOBAL - Validation Utility score: 2614.4836\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(28) - Training Loss: 0.6908\n",
      "Epoch(28) - Fold 0 - Validation Loss : 0.6918\n",
      "Epoch(28) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(28) - Fold 0 - Validation Utility score : 394.0887\n",
      "Epoch(28) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(28) - Fold 1 - Validation Accuracy : 0.5212\n",
      "Epoch(28) - Fold 1 - Validation Utility score : 891.1176\n",
      "Epoch(28) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(28) - Fold 2 - Validation Accuracy : 0.5056\n",
      "Epoch(28) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(28) - Fold 3 - Validation Loss : 0.6931\n",
      "Epoch(28) - Fold 3 - Validation Accuracy : 0.5105\n",
      "Epoch(28) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(28) - Fold 4 - Validation Loss : 0.6904\n",
      "Epoch(28) - Fold 4 - Validation Accuracy : 0.5250\n",
      "Epoch(28) - Fold 4 - Validation Utility score : 1279.3655\n",
      "Epoch(28) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(28) - GLOBAL - Validation Accuracy: 0.5171\n",
      "Epoch(28) - GLOBAL - Validation Utility score: 2564.5718\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(29) - Training Loss: 0.6907\n",
      "Epoch(29) - Fold 0 - Validation Loss : 0.6920\n",
      "Epoch(29) - Fold 0 - Validation Accuracy : 0.5220\n",
      "Epoch(29) - Fold 0 - Validation Utility score : 347.8283\n",
      "Epoch(29) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(29) - Fold 1 - Validation Accuracy : 0.5222\n",
      "Epoch(29) - Fold 1 - Validation Utility score : 912.4913\n",
      "Epoch(29) - Fold 2 - Validation Loss : 0.6942\n",
      "Epoch(29) - Fold 2 - Validation Accuracy : 0.5061\n",
      "Epoch(29) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(29) - Fold 3 - Validation Loss : 0.6931\n",
      "Epoch(29) - Fold 3 - Validation Accuracy : 0.5114\n",
      "Epoch(29) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(29) - Fold 4 - Validation Loss : 0.6906\n",
      "Epoch(29) - Fold 4 - Validation Accuracy : 0.5268\n",
      "Epoch(29) - Fold 4 - Validation Utility score : 1338.5995\n",
      "Epoch(29) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(29) - GLOBAL - Validation Accuracy: 0.5177\n",
      "Epoch(29) - GLOBAL - Validation Utility score: 2598.9191\n",
      "Intermediate early stopping : vepoch_loss = 0.6922, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(30) - Training Loss: 0.6906\n",
      "Epoch(30) - Fold 0 - Validation Loss : 0.6918\n",
      "Epoch(30) - Fold 0 - Validation Accuracy : 0.5231\n",
      "Epoch(30) - Fold 0 - Validation Utility score : 409.0984\n",
      "Epoch(30) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(30) - Fold 1 - Validation Accuracy : 0.5234\n",
      "Epoch(30) - Fold 1 - Validation Utility score : 954.2149\n",
      "Epoch(30) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(30) - Fold 2 - Validation Accuracy : 0.5063\n",
      "Epoch(30) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(30) - Fold 3 - Validation Loss : 0.6930\n",
      "Epoch(30) - Fold 3 - Validation Accuracy : 0.5111\n",
      "Epoch(30) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(30) - Fold 4 - Validation Loss : 0.6904\n",
      "Epoch(30) - Fold 4 - Validation Accuracy : 0.5263\n",
      "Epoch(30) - Fold 4 - Validation Utility score : 1387.8305\n",
      "Epoch(30) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(30) - GLOBAL - Validation Accuracy: 0.5180\n",
      "Epoch(30) - GLOBAL - Validation Utility score: 2751.1438\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(31) - Training Loss: 0.6905\n",
      "Epoch(31) - Fold 0 - Validation Loss : 0.6919\n",
      "Epoch(31) - Fold 0 - Validation Accuracy : 0.5237\n",
      "Epoch(31) - Fold 0 - Validation Utility score : 405.1581\n",
      "Epoch(31) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(31) - Fold 1 - Validation Accuracy : 0.5241\n",
      "Epoch(31) - Fold 1 - Validation Utility score : 994.7974\n",
      "Epoch(31) - Fold 2 - Validation Loss : 0.6943\n",
      "Epoch(31) - Fold 2 - Validation Accuracy : 0.5066\n",
      "Epoch(31) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(31) - Fold 3 - Validation Loss : 0.6932\n",
      "Epoch(31) - Fold 3 - Validation Accuracy : 0.5122\n",
      "Epoch(31) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(31) - Fold 4 - Validation Loss : 0.6903\n",
      "Epoch(31) - Fold 4 - Validation Accuracy : 0.5269\n",
      "Epoch(31) - Fold 4 - Validation Utility score : 1492.2186\n",
      "Epoch(31) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(31) - GLOBAL - Validation Accuracy: 0.5187\n",
      "Epoch(31) - GLOBAL - Validation Utility score: 2892.1741\n",
      "Intermediate early stopping : vepoch_loss = 0.6922, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(32) - Training Loss: 0.6904\n",
      "Epoch(32) - Fold 0 - Validation Loss : 0.6920\n",
      "Epoch(32) - Fold 0 - Validation Accuracy : 0.5216\n",
      "Epoch(32) - Fold 0 - Validation Utility score : 419.5343\n",
      "Epoch(32) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(32) - Fold 1 - Validation Accuracy : 0.5221\n",
      "Epoch(32) - Fold 1 - Validation Utility score : 967.5700\n",
      "Epoch(32) - Fold 2 - Validation Loss : 0.6944\n",
      "Epoch(32) - Fold 2 - Validation Accuracy : 0.5063\n",
      "Epoch(32) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(32) - Fold 3 - Validation Loss : 0.6930\n",
      "Epoch(32) - Fold 3 - Validation Accuracy : 0.5120\n",
      "Epoch(32) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(32) - Fold 4 - Validation Loss : 0.6904\n",
      "Epoch(32) - Fold 4 - Validation Accuracy : 0.5255\n",
      "Epoch(32) - Fold 4 - Validation Utility score : 1328.9212\n",
      "Epoch(32) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(32) - GLOBAL - Validation Accuracy: 0.5175\n",
      "Epoch(32) - GLOBAL - Validation Utility score: 2716.0255\n",
      "Intermediate early stopping : vepoch_loss = 0.6922, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(33) - Training Loss: 0.6904\n",
      "Epoch(33) - Fold 0 - Validation Loss : 0.6920\n",
      "Epoch(33) - Fold 0 - Validation Accuracy : 0.5229\n",
      "Epoch(33) - Fold 0 - Validation Utility score : 376.3782\n",
      "Epoch(33) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(33) - Fold 1 - Validation Accuracy : 0.5233\n",
      "Epoch(33) - Fold 1 - Validation Utility score : 933.8089\n",
      "Epoch(33) - Fold 2 - Validation Loss : 0.6944\n",
      "Epoch(33) - Fold 2 - Validation Accuracy : 0.5076\n",
      "Epoch(33) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(33) - Fold 3 - Validation Loss : 0.6933\n",
      "Epoch(33) - Fold 3 - Validation Accuracy : 0.5113\n",
      "Epoch(33) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(33) - Fold 4 - Validation Loss : 0.6905\n",
      "Epoch(33) - Fold 4 - Validation Accuracy : 0.5256\n",
      "Epoch(33) - Fold 4 - Validation Utility score : 1335.8702\n",
      "Epoch(33) - GLOBAL - Validation Loss: 0.6923\n",
      "Epoch(33) - GLOBAL - Validation Accuracy: 0.5181\n",
      "Epoch(33) - GLOBAL - Validation Utility score: 2646.0573\n",
      "Intermediate early stopping : vepoch_loss = 0.6923, the_last_loss=0.6919\n",
      "Meet Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15521<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_123918-gcxq5iub/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_123918-gcxq5iub/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Global train/loss</td><td>0.6904</td></tr><tr><td>Global valid/Loss</td><td>0.69225</td></tr><tr><td>Global valid/Accuracy</td><td>0.51813</td></tr><tr><td>Global valid/Utility</td><td>2646.05726</td></tr><tr><td>Fold valid Loss/Loss fold 0</td><td>0.69201</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 0</td><td>0.52294</td></tr><tr><td>Fold valid Utility/Utility fold 0</td><td>376.37815</td></tr><tr><td>Fold valid Loss/Loss fold 1</td><td>0.69115</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 1</td><td>0.52328</td></tr><tr><td>Fold valid Utility/Utility fold 1</td><td>933.80892</td></tr><tr><td>Fold valid Loss/Loss fold 2</td><td>0.69436</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 2</td><td>0.5076</td></tr><tr><td>Fold valid Utility/Utility fold 2</td><td>-0.0</td></tr><tr><td>Fold valid Loss/Loss fold 3</td><td>0.6933</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 3</td><td>0.51128</td></tr><tr><td>Fold valid Utility/Utility fold 3</td><td>-0.0</td></tr><tr><td>Fold valid Loss/Loss fold 4</td><td>0.69045</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 4</td><td>0.52557</td></tr><tr><td>Fold valid Utility/Utility fold 4</td><td>1335.87018</td></tr><tr><td>Best accuracy</td><td>0.5189</td></tr><tr><td>Best utility</td><td>2555.25553</td></tr><tr><td>_runtime</td><td>2079</td></tr><tr><td>_timestamp</td><td>1613823237</td></tr><tr><td>_step</td><td>33</td></tr><tr><td>Final utility score</td><td>{'utility_score': 25...</td></tr><tr><td>Batch size</td><td>118270</td></tr><tr><td>Patience</td><td>20</td></tr><tr><td>Number of epochs</td><td>1000</td></tr><tr><td>Best epoch</td><td>13</td></tr><tr><td>Number of parameters per layer</td><td>[132, 132, 39600, 30...</td></tr><tr><td>Model architecture</td><td>Model_Resnet(<BR>  (...</td></tr><tr><td>comment</td><td>All folds MLP withOU...</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Global train/loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Global valid/Loss</td><td>█▅▄▂▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Global valid/Accuracy</td><td>▁▃▄▅▆▆▇▇▇▇▇█▇██▇▇▇█▇▇████▇▇▇▇▇▇█▇▇</td></tr><tr><td>Global valid/Utility</td><td>▁▂▅▆▆▅▅▆▅▆▆▆▆▇▇▆▇▆▇▇▆▆▇█▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>Fold valid Loss/Loss fold 0</td><td>▇█▅▃▄▃▃▂▂▁▃▂▂▁▂▁▂▂▁▂▁▂▂▂▃▃▂▂▂▃▂▂▃▃</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 0</td><td>▁▂▂▆▇██▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>Fold valid Utility/Utility fold 0</td><td>▄▁██▇███▄▆▆▄▆▅▄▆▅▅▅▆▅▅▄▆▆▆▅▆▆▅▆▆▆▆</td></tr><tr><td>Fold valid Loss/Loss fold 1</td><td>█▅▅▃▃▃▃▃▂▂▃▂▂▁▂▂▂▁▁▁▁▁▁▁▁▂▁▂▂▂▁▁▂▂</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 1</td><td>▁▃▄▆▇▇▇▆▇▇▇█▇██▇▇▇██▇▇███▇█▇▇▇██▇█</td></tr><tr><td>Fold valid Utility/Utility fold 1</td><td>▁▂▂▃▄▄▄▄▅▆▆▆▆▇█▇▇▅▇█▆▆▆█▇▇█▇▇▇▇██▇</td></tr><tr><td>Fold valid Loss/Loss fold 2</td><td>█▃▂▁▃▂▂▂▂▂▂▂▃▃▂▂▃▃▃▄▅▅▅▅▅▅▅▆▇▆▇▇█▇</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 2</td><td>▆▆▇▁▂▄▇▇▆█▃▇▂▇▇█▇▅▇▄▃▆▄▅▆▄▇▆▃▅▅▆▅█</td></tr><tr><td>Fold valid Utility/Utility fold 2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold valid Loss/Loss fold 3</td><td>█▄▄▃▃▂▃▃▂▂▂▃▁▁▁▂▂▁▂▂▂▂▃▂▄▃▃▃▃▃▃▄▃▅</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 3</td><td>▁▆▅▅▆▆▇▇▆▆▆▇▇█▇▆▆▇▇▆▆▇▆▆▅▅▆▅▄▅▅▆▅▅</td></tr><tr><td>Fold valid Utility/Utility fold 3</td><td>▃█▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold valid Loss/Loss fold 4</td><td>█▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▂▁▁▁▁</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 4</td><td>▁▃▄▅▅▅▅▆▆▆▇▇▇█▇▇▇▇▇▇▇██████▇▇███▇▇</td></tr><tr><td>Fold valid Utility/Utility fold 4</td><td>▁▃▆▆▆▆▅▆▆▆▆▇▆▇▆▆▇▆█▇▆▇███▇▇▇▆▇▇█▇▇</td></tr><tr><td>Best accuracy</td><td>▁▃▄▅▆▇▇▇▇██</td></tr><tr><td>Best utility</td><td>▁▂▅▇▇▇▆█▇██</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 171 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">grateful-sweep-1</strong>: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/gcxq5iub\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/gcxq5iub</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training summary:\n",
      "{'utility_score': 2555.2555320788697, 'utility_scores': [323.5872929429306, 871.1782724656175, -0.0, 7.9644429221805755, 1352.5255237481408], 'utility_score_std': 526.7224841351699, 'accuracy_scores': [0.5230755056625703, 0.5229835029835029, 0.5071781175549773, 0.5151879678091057, 0.5260600084518946]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mcyrv4ea with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 145907\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4441073658602744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 163\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000730378913475265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_autoenc: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 5.334962359070206e-05\n",
      "2021-02-20 13:14:04.208010: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-02-20 13:14:04.208032: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.19<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pretty-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/sweeps/11zvcrka\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/sweeps/11zvcrka</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/mcyrv4ea\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/mcyrv4ea</a><br/>\n",
       "                Run data is saved locally in <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_131403-mcyrv4ea</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Run config :\n",
      "config: {'activation_function': 'leakyrelu', 'batch_size': 145907, 'dropout': 0.4441073658602744, 'hidden_size': 163, 'learning_rate': 0.000730378913475265, 'use_autoenc': 'None', 'weight_decay': 5.334962359070206e-05, 'epochs': 1000}\n",
      "Epoch(0) - Training Loss: 0.7197\n",
      "Epoch(0) - Fold 0 - Validation Loss : 0.6943\n",
      "Epoch(0) - Fold 0 - Validation Accuracy : 0.4992\n",
      "Epoch(0) - Fold 0 - Validation Utility score : 9.6311\n",
      "Epoch(0) - Fold 1 - Validation Loss : 0.6939\n",
      "Epoch(0) - Fold 1 - Validation Accuracy : 0.5007\n",
      "Epoch(0) - Fold 1 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(0) - Fold 2 - Validation Accuracy : 0.5061\n",
      "Epoch(0) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 3 - Validation Loss : 0.6942\n",
      "Epoch(0) - Fold 3 - Validation Accuracy : 0.4984\n",
      "Epoch(0) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 4 - Validation Loss : 0.6936\n",
      "Epoch(0) - Fold 4 - Validation Accuracy : 0.5027\n",
      "Epoch(0) - Fold 4 - Validation Utility score : 687.8038\n",
      "Epoch(0) - GLOBAL - Validation Loss: 0.6939\n",
      "Epoch(0) - GLOBAL - Validation Accuracy: 0.5014\n",
      "Epoch(0) - GLOBAL - Validation Utility score: 697.4349\n",
      "Saving model corresponding to last_utility_score == 697.4349331083175\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.7165\n",
      "Epoch(1) - Fold 0 - Validation Loss : 0.6933\n",
      "Epoch(1) - Fold 0 - Validation Accuracy : 0.5085\n",
      "Epoch(1) - Fold 0 - Validation Utility score : 51.1599\n",
      "Epoch(1) - Fold 1 - Validation Loss : 0.6930\n",
      "Epoch(1) - Fold 1 - Validation Accuracy : 0.5117\n",
      "Epoch(1) - Fold 1 - Validation Utility score : 29.3428\n",
      "Epoch(1) - Fold 2 - Validation Loss : 0.6933\n",
      "Epoch(1) - Fold 2 - Validation Accuracy : 0.5065\n",
      "Epoch(1) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(1) - Fold 3 - Validation Loss : 0.6940\n",
      "Epoch(1) - Fold 3 - Validation Accuracy : 0.5032\n",
      "Epoch(1) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(1) - Fold 4 - Validation Loss : 0.6928\n",
      "Epoch(1) - Fold 4 - Validation Accuracy : 0.5112\n",
      "Epoch(1) - Fold 4 - Validation Utility score : 880.0943\n",
      "Epoch(1) - GLOBAL - Validation Loss: 0.6933\n",
      "Epoch(1) - GLOBAL - Validation Accuracy: 0.5082\n",
      "Epoch(1) - GLOBAL - Validation Utility score: 960.5970\n",
      "Saving model corresponding to last_utility_score == 960.5969551428683\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.7127\n",
      "Epoch(2) - Fold 0 - Validation Loss : 0.6928\n",
      "Epoch(2) - Fold 0 - Validation Accuracy : 0.5135\n",
      "Epoch(2) - Fold 0 - Validation Utility score : 101.5089\n",
      "Epoch(2) - Fold 1 - Validation Loss : 0.6924\n",
      "Epoch(2) - Fold 1 - Validation Accuracy : 0.5163\n",
      "Epoch(2) - Fold 1 - Validation Utility score : 121.9143\n",
      "Epoch(2) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(2) - Fold 2 - Validation Accuracy : 0.5066\n",
      "Epoch(2) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(2) - Fold 3 - Validation Loss : 0.6934\n",
      "Epoch(2) - Fold 3 - Validation Accuracy : 0.5082\n",
      "Epoch(2) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(2) - Fold 4 - Validation Loss : 0.6922\n",
      "Epoch(2) - Fold 4 - Validation Accuracy : 0.5135\n",
      "Epoch(2) - Fold 4 - Validation Utility score : 809.1231\n",
      "Epoch(2) - GLOBAL - Validation Loss: 0.6928\n",
      "Epoch(2) - GLOBAL - Validation Accuracy: 0.5116\n",
      "Epoch(2) - GLOBAL - Validation Utility score: 1032.5462\n",
      "Saving model corresponding to last_utility_score == 1032.546210311014\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.7087\n",
      "Epoch(3) - Fold 0 - Validation Loss : 0.6927\n",
      "Epoch(3) - Fold 0 - Validation Accuracy : 0.5143\n",
      "Epoch(3) - Fold 0 - Validation Utility score : 119.8226\n",
      "Epoch(3) - Fold 1 - Validation Loss : 0.6922\n",
      "Epoch(3) - Fold 1 - Validation Accuracy : 0.5180\n",
      "Epoch(3) - Fold 1 - Validation Utility score : 204.8273\n",
      "Epoch(3) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(3) - Fold 2 - Validation Accuracy : 0.5077\n",
      "Epoch(3) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(3) - Fold 3 - Validation Loss : 0.6932\n",
      "Epoch(3) - Fold 3 - Validation Accuracy : 0.5080\n",
      "Epoch(3) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(3) - Fold 4 - Validation Loss : 0.6920\n",
      "Epoch(3) - Fold 4 - Validation Accuracy : 0.5154\n",
      "Epoch(3) - Fold 4 - Validation Utility score : 957.2404\n",
      "Epoch(3) - GLOBAL - Validation Loss: 0.6927\n",
      "Epoch(3) - GLOBAL - Validation Accuracy: 0.5127\n",
      "Epoch(3) - GLOBAL - Validation Utility score: 1281.8902\n",
      "Saving model corresponding to last_utility_score == 1281.8902485978592\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.7062\n",
      "Epoch(4) - Fold 0 - Validation Loss : 0.6924\n",
      "Epoch(4) - Fold 0 - Validation Accuracy : 0.5173\n",
      "Epoch(4) - Fold 0 - Validation Utility score : 285.9776\n",
      "Epoch(4) - Fold 1 - Validation Loss : 0.6921\n",
      "Epoch(4) - Fold 1 - Validation Accuracy : 0.5189\n",
      "Epoch(4) - Fold 1 - Validation Utility score : 342.9251\n",
      "Epoch(4) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(4) - Fold 2 - Validation Accuracy : 0.5071\n",
      "Epoch(4) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(4) - Fold 3 - Validation Loss : 0.6931\n",
      "Epoch(4) - Fold 3 - Validation Accuracy : 0.5093\n",
      "Epoch(4) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(4) - Fold 4 - Validation Loss : 0.6918\n",
      "Epoch(4) - Fold 4 - Validation Accuracy : 0.5178\n",
      "Epoch(4) - Fold 4 - Validation Utility score : 1018.2809\n",
      "Epoch(4) - GLOBAL - Validation Loss: 0.6925\n",
      "Epoch(4) - GLOBAL - Validation Accuracy: 0.5141\n",
      "Epoch(4) - GLOBAL - Validation Utility score: 1647.1836\n",
      "Saving model corresponding to last_utility_score == 1647.1836126864948\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 0.7040\n",
      "Epoch(5) - Fold 0 - Validation Loss : 0.6923\n",
      "Epoch(5) - Fold 0 - Validation Accuracy : 0.5184\n",
      "Epoch(5) - Fold 0 - Validation Utility score : 266.3917\n",
      "Epoch(5) - Fold 1 - Validation Loss : 0.6921\n",
      "Epoch(5) - Fold 1 - Validation Accuracy : 0.5171\n",
      "Epoch(5) - Fold 1 - Validation Utility score : 392.3556\n",
      "Epoch(5) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(5) - Fold 2 - Validation Accuracy : 0.5065\n",
      "Epoch(5) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(5) - Fold 3 - Validation Loss : 0.6930\n",
      "Epoch(5) - Fold 3 - Validation Accuracy : 0.5092\n",
      "Epoch(5) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(5) - Fold 4 - Validation Loss : 0.6917\n",
      "Epoch(5) - Fold 4 - Validation Accuracy : 0.5192\n",
      "Epoch(5) - Fold 4 - Validation Utility score : 1205.2098\n",
      "Epoch(5) - GLOBAL - Validation Loss: 0.6924\n",
      "Epoch(5) - GLOBAL - Validation Accuracy: 0.5141\n",
      "Epoch(5) - GLOBAL - Validation Utility score: 1863.9570\n",
      "Saving model corresponding to last_utility_score == 1863.9570493084652\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 0.7024\n",
      "Epoch(6) - Fold 0 - Validation Loss : 0.6921\n",
      "Epoch(6) - Fold 0 - Validation Accuracy : 0.5195\n",
      "Epoch(6) - Fold 0 - Validation Utility score : 275.1784\n",
      "Epoch(6) - Fold 1 - Validation Loss : 0.6920\n",
      "Epoch(6) - Fold 1 - Validation Accuracy : 0.5183\n",
      "Epoch(6) - Fold 1 - Validation Utility score : 451.9000\n",
      "Epoch(6) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(6) - Fold 2 - Validation Accuracy : 0.5067\n",
      "Epoch(6) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(6) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(6) - Fold 3 - Validation Accuracy : 0.5098\n",
      "Epoch(6) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(6) - Fold 4 - Validation Loss : 0.6915\n",
      "Epoch(6) - Fold 4 - Validation Accuracy : 0.5197\n",
      "Epoch(6) - Fold 4 - Validation Utility score : 1133.6784\n",
      "Epoch(6) - GLOBAL - Validation Loss: 0.6923\n",
      "Epoch(6) - GLOBAL - Validation Accuracy: 0.5148\n",
      "Epoch(6) - GLOBAL - Validation Utility score: 1860.7568\n",
      "Saving model corresponding to last_utility_score == 1860.7567849749587\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 0.7012\n",
      "Epoch(7) - Fold 0 - Validation Loss : 0.6920\n",
      "Epoch(7) - Fold 0 - Validation Accuracy : 0.5188\n",
      "Epoch(7) - Fold 0 - Validation Utility score : 249.6581\n",
      "Epoch(7) - Fold 1 - Validation Loss : 0.6920\n",
      "Epoch(7) - Fold 1 - Validation Accuracy : 0.5188\n",
      "Epoch(7) - Fold 1 - Validation Utility score : 569.4274\n",
      "Epoch(7) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(7) - Fold 2 - Validation Accuracy : 0.5076\n",
      "Epoch(7) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(7) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(7) - Fold 3 - Validation Accuracy : 0.5108\n",
      "Epoch(7) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(7) - Fold 4 - Validation Loss : 0.6914\n",
      "Epoch(7) - Fold 4 - Validation Accuracy : 0.5204\n",
      "Epoch(7) - Fold 4 - Validation Utility score : 1263.2182\n",
      "Epoch(7) - GLOBAL - Validation Loss: 0.6923\n",
      "Epoch(7) - GLOBAL - Validation Accuracy: 0.5153\n",
      "Epoch(7) - GLOBAL - Validation Utility score: 2082.3038\n",
      "Saving model corresponding to last_utility_score == 2082.3037618490007\n",
      "\n",
      "\n",
      "Epoch(8) - Training Loss: 0.7001\n",
      "Epoch(8) - Fold 0 - Validation Loss : 0.6919\n",
      "Epoch(8) - Fold 0 - Validation Accuracy : 0.5204\n",
      "Epoch(8) - Fold 0 - Validation Utility score : 286.2138\n",
      "Epoch(8) - Fold 1 - Validation Loss : 0.6920\n",
      "Epoch(8) - Fold 1 - Validation Accuracy : 0.5201\n",
      "Epoch(8) - Fold 1 - Validation Utility score : 636.5891\n",
      "Epoch(8) - Fold 2 - Validation Loss : 0.6931\n",
      "Epoch(8) - Fold 2 - Validation Accuracy : 0.5079\n",
      "Epoch(8) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(8) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(8) - Fold 3 - Validation Accuracy : 0.5109\n",
      "Epoch(8) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(8) - Fold 4 - Validation Loss : 0.6913\n",
      "Epoch(8) - Fold 4 - Validation Accuracy : 0.5205\n",
      "Epoch(8) - Fold 4 - Validation Utility score : 1213.0630\n",
      "Epoch(8) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(8) - GLOBAL - Validation Accuracy: 0.5160\n",
      "Epoch(8) - GLOBAL - Validation Utility score: 2135.8658\n",
      "Saving model corresponding to last_utility_score == 2135.8658097499992\n",
      "\n",
      "\n",
      "Epoch(9) - Training Loss: 0.6991\n",
      "Epoch(9) - Fold 0 - Validation Loss : 0.6917\n",
      "Epoch(9) - Fold 0 - Validation Accuracy : 0.5207\n",
      "Epoch(9) - Fold 0 - Validation Utility score : 325.7758\n",
      "Epoch(9) - Fold 1 - Validation Loss : 0.6919\n",
      "Epoch(9) - Fold 1 - Validation Accuracy : 0.5195\n",
      "Epoch(9) - Fold 1 - Validation Utility score : 613.8302\n",
      "Epoch(9) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(9) - Fold 2 - Validation Accuracy : 0.5074\n",
      "Epoch(9) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(9) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(9) - Fold 3 - Validation Accuracy : 0.5122\n",
      "Epoch(9) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(9) - Fold 4 - Validation Loss : 0.6912\n",
      "Epoch(9) - Fold 4 - Validation Accuracy : 0.5208\n",
      "Epoch(9) - Fold 4 - Validation Utility score : 1247.3358\n",
      "Epoch(9) - GLOBAL - Validation Loss: 0.6922\n",
      "Epoch(9) - GLOBAL - Validation Accuracy: 0.5161\n",
      "Epoch(9) - GLOBAL - Validation Utility score: 2186.9419\n",
      "Saving model corresponding to last_utility_score == 2186.9418937789615\n",
      "\n",
      "\n",
      "Epoch(10) - Training Loss: 0.6983\n",
      "Epoch(10) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(10) - Fold 0 - Validation Accuracy : 0.5214\n",
      "Epoch(10) - Fold 0 - Validation Utility score : 432.9150\n",
      "Epoch(10) - Fold 1 - Validation Loss : 0.6918\n",
      "Epoch(10) - Fold 1 - Validation Accuracy : 0.5205\n",
      "Epoch(10) - Fold 1 - Validation Utility score : 663.8725\n",
      "Epoch(10) - Fold 2 - Validation Loss : 0.6932\n",
      "Epoch(10) - Fold 2 - Validation Accuracy : 0.5087\n",
      "Epoch(10) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(10) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(10) - Fold 3 - Validation Accuracy : 0.5124\n",
      "Epoch(10) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(10) - Fold 4 - Validation Loss : 0.6912\n",
      "Epoch(10) - Fold 4 - Validation Accuracy : 0.5213\n",
      "Epoch(10) - Fold 4 - Validation Utility score : 1125.9288\n",
      "Epoch(10) - GLOBAL - Validation Loss: 0.6921\n",
      "Epoch(10) - GLOBAL - Validation Accuracy: 0.5169\n",
      "Epoch(10) - GLOBAL - Validation Utility score: 2222.7163\n",
      "Saving model corresponding to last_utility_score == 2222.7163462742074\n",
      "\n",
      "\n",
      "Epoch(11) - Training Loss: 0.6976\n",
      "Epoch(11) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(11) - Fold 0 - Validation Accuracy : 0.5217\n",
      "Epoch(11) - Fold 0 - Validation Utility score : 420.8338\n",
      "Epoch(11) - Fold 1 - Validation Loss : 0.6917\n",
      "Epoch(11) - Fold 1 - Validation Accuracy : 0.5210\n",
      "Epoch(11) - Fold 1 - Validation Utility score : 680.5310\n",
      "Epoch(11) - Fold 2 - Validation Loss : 0.6933\n",
      "Epoch(11) - Fold 2 - Validation Accuracy : 0.5091\n",
      "Epoch(11) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(11) - Fold 3 - Validation Loss : 0.6925\n",
      "Epoch(11) - Fold 3 - Validation Accuracy : 0.5135\n",
      "Epoch(11) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(11) - Fold 4 - Validation Loss : 0.6912\n",
      "Epoch(11) - Fold 4 - Validation Accuracy : 0.5214\n",
      "Epoch(11) - Fold 4 - Validation Utility score : 1156.1432\n",
      "Epoch(11) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(11) - GLOBAL - Validation Accuracy: 0.5173\n",
      "Epoch(11) - GLOBAL - Validation Utility score: 2257.5080\n",
      "Saving model corresponding to last_utility_score == 2257.5080175750722\n",
      "\n",
      "\n",
      "Epoch(12) - Training Loss: 0.6969\n",
      "Epoch(12) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(12) - Fold 0 - Validation Accuracy : 0.5223\n",
      "Epoch(12) - Fold 0 - Validation Utility score : 407.3385\n",
      "Epoch(12) - Fold 1 - Validation Loss : 0.6916\n",
      "Epoch(12) - Fold 1 - Validation Accuracy : 0.5214\n",
      "Epoch(12) - Fold 1 - Validation Utility score : 757.4889\n",
      "Epoch(12) - Fold 2 - Validation Loss : 0.6933\n",
      "Epoch(12) - Fold 2 - Validation Accuracy : 0.5082\n",
      "Epoch(12) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(12) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(12) - Fold 3 - Validation Accuracy : 0.5136\n",
      "Epoch(12) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(12) - Fold 4 - Validation Loss : 0.6912\n",
      "Epoch(12) - Fold 4 - Validation Accuracy : 0.5224\n",
      "Epoch(12) - Fold 4 - Validation Utility score : 1288.6392\n",
      "Epoch(12) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(12) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(12) - GLOBAL - Validation Utility score: 2453.4665\n",
      "Saving model corresponding to last_utility_score == 2453.466515400136\n",
      "\n",
      "\n",
      "Epoch(13) - Training Loss: 0.6962\n",
      "Epoch(13) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(13) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(13) - Fold 0 - Validation Utility score : 403.7497\n",
      "Epoch(13) - Fold 1 - Validation Loss : 0.6915\n",
      "Epoch(13) - Fold 1 - Validation Accuracy : 0.5220\n",
      "Epoch(13) - Fold 1 - Validation Utility score : 785.3166\n",
      "Epoch(13) - Fold 2 - Validation Loss : 0.6933\n",
      "Epoch(13) - Fold 2 - Validation Accuracy : 0.5083\n",
      "Epoch(13) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(13) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(13) - Fold 3 - Validation Accuracy : 0.5126\n",
      "Epoch(13) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(13) - Fold 4 - Validation Loss : 0.6911\n",
      "Epoch(13) - Fold 4 - Validation Accuracy : 0.5224\n",
      "Epoch(13) - Fold 4 - Validation Utility score : 1343.3803\n",
      "Epoch(13) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(13) - GLOBAL - Validation Accuracy: 0.5177\n",
      "Epoch(13) - GLOBAL - Validation Utility score: 2532.4465\n",
      "Saving model corresponding to last_utility_score == 2532.4465373935273\n",
      "\n",
      "\n",
      "Epoch(14) - Training Loss: 0.6958\n",
      "Epoch(14) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(14) - Fold 0 - Validation Accuracy : 0.5228\n",
      "Epoch(14) - Fold 0 - Validation Utility score : 390.3723\n",
      "Epoch(14) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(14) - Fold 1 - Validation Accuracy : 0.5228\n",
      "Epoch(14) - Fold 1 - Validation Utility score : 819.2250\n",
      "Epoch(14) - Fold 2 - Validation Loss : 0.6933\n",
      "Epoch(14) - Fold 2 - Validation Accuracy : 0.5079\n",
      "Epoch(14) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(14) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(14) - Fold 3 - Validation Accuracy : 0.5130\n",
      "Epoch(14) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(14) - Fold 4 - Validation Loss : 0.6911\n",
      "Epoch(14) - Fold 4 - Validation Accuracy : 0.5225\n",
      "Epoch(14) - Fold 4 - Validation Utility score : 1279.1768\n",
      "Epoch(14) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(14) - GLOBAL - Validation Accuracy: 0.5178\n",
      "Epoch(14) - GLOBAL - Validation Utility score: 2488.7741\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(15) - Training Loss: 0.6953\n",
      "Epoch(15) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(15) - Fold 0 - Validation Accuracy : 0.5224\n",
      "Epoch(15) - Fold 0 - Validation Utility score : 402.4493\n",
      "Epoch(15) - Fold 1 - Validation Loss : 0.6915\n",
      "Epoch(15) - Fold 1 - Validation Accuracy : 0.5210\n",
      "Epoch(15) - Fold 1 - Validation Utility score : 890.2652\n",
      "Epoch(15) - Fold 2 - Validation Loss : 0.6933\n",
      "Epoch(15) - Fold 2 - Validation Accuracy : 0.5084\n",
      "Epoch(15) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(15) - Fold 3 - Validation Loss : 0.6925\n",
      "Epoch(15) - Fold 3 - Validation Accuracy : 0.5132\n",
      "Epoch(15) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(15) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(15) - Fold 4 - Validation Accuracy : 0.5228\n",
      "Epoch(15) - Fold 4 - Validation Utility score : 1283.1629\n",
      "Epoch(15) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(15) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(15) - GLOBAL - Validation Utility score: 2575.8774\n",
      "Saving model corresponding to last_utility_score == 2575.877371402595\n",
      "\n",
      "\n",
      "Epoch(16) - Training Loss: 0.6950\n",
      "Epoch(16) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(16) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(16) - Fold 0 - Validation Utility score : 383.8526\n",
      "Epoch(16) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(16) - Fold 1 - Validation Accuracy : 0.5221\n",
      "Epoch(16) - Fold 1 - Validation Utility score : 846.1084\n",
      "Epoch(16) - Fold 2 - Validation Loss : 0.6933\n",
      "Epoch(16) - Fold 2 - Validation Accuracy : 0.5078\n",
      "Epoch(16) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(16) - Fold 3 - Validation Loss : 0.6925\n",
      "Epoch(16) - Fold 3 - Validation Accuracy : 0.5136\n",
      "Epoch(16) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(16) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(16) - Fold 4 - Validation Accuracy : 0.5221\n",
      "Epoch(16) - Fold 4 - Validation Utility score : 1290.4266\n",
      "Epoch(16) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(16) - GLOBAL - Validation Accuracy: 0.5178\n",
      "Epoch(16) - GLOBAL - Validation Utility score: 2520.3876\n",
      "Saving model corresponding to last_utility_score == 2520.3875706387234\n",
      "\n",
      "\n",
      "Epoch(17) - Training Loss: 0.6945\n",
      "Epoch(17) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(17) - Fold 0 - Validation Accuracy : 0.5229\n",
      "Epoch(17) - Fold 0 - Validation Utility score : 436.0682\n",
      "Epoch(17) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(17) - Fold 1 - Validation Accuracy : 0.5206\n",
      "Epoch(17) - Fold 1 - Validation Utility score : 845.0542\n",
      "Epoch(17) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(17) - Fold 2 - Validation Accuracy : 0.5078\n",
      "Epoch(17) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(17) - Fold 3 - Validation Loss : 0.6925\n",
      "Epoch(17) - Fold 3 - Validation Accuracy : 0.5136\n",
      "Epoch(17) - Fold 3 - Validation Utility score : 0.0001\n",
      "Epoch(17) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(17) - Fold 4 - Validation Accuracy : 0.5226\n",
      "Epoch(17) - Fold 4 - Validation Utility score : 1283.5114\n",
      "Epoch(17) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(17) - GLOBAL - Validation Accuracy: 0.5175\n",
      "Epoch(17) - GLOBAL - Validation Utility score: 2564.6339\n",
      "Saving model corresponding to last_utility_score == 2564.633935699788\n",
      "\n",
      "\n",
      "Epoch(18) - Training Loss: 0.6942\n",
      "Epoch(18) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(18) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(18) - Fold 0 - Validation Utility score : 460.8231\n",
      "Epoch(18) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(18) - Fold 1 - Validation Accuracy : 0.5216\n",
      "Epoch(18) - Fold 1 - Validation Utility score : 898.8433\n",
      "Epoch(18) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(18) - Fold 2 - Validation Accuracy : 0.5081\n",
      "Epoch(18) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(18) - Fold 3 - Validation Loss : 0.6925\n",
      "Epoch(18) - Fold 3 - Validation Accuracy : 0.5128\n",
      "Epoch(18) - Fold 3 - Validation Utility score : 0.0127\n",
      "Epoch(18) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(18) - Fold 4 - Validation Accuracy : 0.5232\n",
      "Epoch(18) - Fold 4 - Validation Utility score : 1275.2083\n",
      "Epoch(18) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(18) - GLOBAL - Validation Accuracy: 0.5178\n",
      "Epoch(18) - GLOBAL - Validation Utility score: 2634.8873\n",
      "Saving model corresponding to last_utility_score == 2634.8873318215938\n",
      "\n",
      "\n",
      "Epoch(19) - Training Loss: 0.6940\n",
      "Epoch(19) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(19) - Fold 0 - Validation Accuracy : 0.5244\n",
      "Epoch(19) - Fold 0 - Validation Utility score : 494.3958\n",
      "Epoch(19) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(19) - Fold 1 - Validation Accuracy : 0.5212\n",
      "Epoch(19) - Fold 1 - Validation Utility score : 729.2891\n",
      "Epoch(19) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(19) - Fold 2 - Validation Accuracy : 0.5077\n",
      "Epoch(19) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(19) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(19) - Fold 3 - Validation Accuracy : 0.5128\n",
      "Epoch(19) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(19) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(19) - Fold 4 - Validation Accuracy : 0.5221\n",
      "Epoch(19) - Fold 4 - Validation Utility score : 1196.1635\n",
      "Epoch(19) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(19) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(19) - GLOBAL - Validation Utility score: 2419.8484\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(20) - Training Loss: 0.6937\n",
      "Epoch(20) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(20) - Fold 0 - Validation Accuracy : 0.5230\n",
      "Epoch(20) - Fold 0 - Validation Utility score : 468.5161\n",
      "Epoch(20) - Fold 1 - Validation Loss : 0.6914\n",
      "Epoch(20) - Fold 1 - Validation Accuracy : 0.5209\n",
      "Epoch(20) - Fold 1 - Validation Utility score : 728.1488\n",
      "Epoch(20) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(20) - Fold 2 - Validation Accuracy : 0.5083\n",
      "Epoch(20) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(20) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(20) - Fold 3 - Validation Accuracy : 0.5130\n",
      "Epoch(20) - Fold 3 - Validation Utility score : 0.0088\n",
      "Epoch(20) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(20) - Fold 4 - Validation Accuracy : 0.5222\n",
      "Epoch(20) - Fold 4 - Validation Utility score : 1234.2567\n",
      "Epoch(20) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(20) - GLOBAL - Validation Accuracy: 0.5175\n",
      "Epoch(20) - GLOBAL - Validation Utility score: 2430.9305\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(21) - Training Loss: 0.6935\n",
      "Epoch(21) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(21) - Fold 0 - Validation Accuracy : 0.5234\n",
      "Epoch(21) - Fold 0 - Validation Utility score : 468.6339\n",
      "Epoch(21) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(21) - Fold 1 - Validation Accuracy : 0.5208\n",
      "Epoch(21) - Fold 1 - Validation Utility score : 797.2476\n",
      "Epoch(21) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(21) - Fold 2 - Validation Accuracy : 0.5088\n",
      "Epoch(21) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(21) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(21) - Fold 3 - Validation Accuracy : 0.5127\n",
      "Epoch(21) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(21) - Fold 4 - Validation Loss : 0.6909\n",
      "Epoch(21) - Fold 4 - Validation Accuracy : 0.5223\n",
      "Epoch(21) - Fold 4 - Validation Utility score : 1212.3051\n",
      "Epoch(21) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(21) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(21) - GLOBAL - Validation Utility score: 2478.1867\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(22) - Training Loss: 0.6933\n",
      "Epoch(22) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(22) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(22) - Fold 0 - Validation Utility score : 461.8342\n",
      "Epoch(22) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(22) - Fold 1 - Validation Accuracy : 0.5207\n",
      "Epoch(22) - Fold 1 - Validation Utility score : 761.7895\n",
      "Epoch(22) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(22) - Fold 2 - Validation Accuracy : 0.5089\n",
      "Epoch(22) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(22) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(22) - Fold 3 - Validation Accuracy : 0.5125\n",
      "Epoch(22) - Fold 3 - Validation Utility score : 0.1676\n",
      "Epoch(22) - Fold 4 - Validation Loss : 0.6909\n",
      "Epoch(22) - Fold 4 - Validation Accuracy : 0.5229\n",
      "Epoch(22) - Fold 4 - Validation Utility score : 1231.1502\n",
      "Epoch(22) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(22) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(22) - GLOBAL - Validation Utility score: 2454.9415\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(23) - Training Loss: 0.6930\n",
      "Epoch(23) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(23) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(23) - Fold 0 - Validation Utility score : 455.6177\n",
      "Epoch(23) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(23) - Fold 1 - Validation Accuracy : 0.5208\n",
      "Epoch(23) - Fold 1 - Validation Utility score : 751.0867\n",
      "Epoch(23) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(23) - Fold 2 - Validation Accuracy : 0.5078\n",
      "Epoch(23) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(23) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(23) - Fold 3 - Validation Accuracy : 0.5121\n",
      "Epoch(23) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(23) - Fold 4 - Validation Loss : 0.6910\n",
      "Epoch(23) - Fold 4 - Validation Accuracy : 0.5234\n",
      "Epoch(23) - Fold 4 - Validation Utility score : 1182.2965\n",
      "Epoch(23) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(23) - GLOBAL - Validation Accuracy: 0.5175\n",
      "Epoch(23) - GLOBAL - Validation Utility score: 2389.0009\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(24) - Training Loss: 0.6930\n",
      "Epoch(24) - Fold 0 - Validation Loss : 0.6913\n",
      "Epoch(24) - Fold 0 - Validation Accuracy : 0.5236\n",
      "Epoch(24) - Fold 0 - Validation Utility score : 489.2820\n",
      "Epoch(24) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(24) - Fold 1 - Validation Accuracy : 0.5211\n",
      "Epoch(24) - Fold 1 - Validation Utility score : 824.3962\n",
      "Epoch(24) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(24) - Fold 2 - Validation Accuracy : 0.5075\n",
      "Epoch(24) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(24) - Fold 3 - Validation Loss : 0.6926\n",
      "Epoch(24) - Fold 3 - Validation Accuracy : 0.5125\n",
      "Epoch(24) - Fold 3 - Validation Utility score : 0.2688\n",
      "Epoch(24) - Fold 4 - Validation Loss : 0.6909\n",
      "Epoch(24) - Fold 4 - Validation Accuracy : 0.5232\n",
      "Epoch(24) - Fold 4 - Validation Utility score : 1187.7326\n",
      "Epoch(24) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(24) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(24) - GLOBAL - Validation Utility score: 2501.6797\n",
      "Saving model corresponding to last_utility_score == 2501.6796944104767\n",
      "\n",
      "\n",
      "Epoch(25) - Training Loss: 0.6926\n",
      "Epoch(25) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(25) - Fold 0 - Validation Accuracy : 0.5238\n",
      "Epoch(25) - Fold 0 - Validation Utility score : 481.8170\n",
      "Epoch(25) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(25) - Fold 1 - Validation Accuracy : 0.5216\n",
      "Epoch(25) - Fold 1 - Validation Utility score : 848.1180\n",
      "Epoch(25) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(25) - Fold 2 - Validation Accuracy : 0.5076\n",
      "Epoch(25) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(25) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(25) - Fold 3 - Validation Accuracy : 0.5130\n",
      "Epoch(25) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(25) - Fold 4 - Validation Loss : 0.6909\n",
      "Epoch(25) - Fold 4 - Validation Accuracy : 0.5245\n",
      "Epoch(25) - Fold 4 - Validation Utility score : 1271.7624\n",
      "Epoch(25) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(25) - GLOBAL - Validation Accuracy: 0.5181\n",
      "Epoch(25) - GLOBAL - Validation Utility score: 2601.6973\n",
      "Saving model corresponding to last_utility_score == 2601.697279000122\n",
      "\n",
      "\n",
      "Epoch(26) - Training Loss: 0.6926\n",
      "Epoch(26) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(26) - Fold 0 - Validation Accuracy : 0.5243\n",
      "Epoch(26) - Fold 0 - Validation Utility score : 479.5255\n",
      "Epoch(26) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(26) - Fold 1 - Validation Accuracy : 0.5213\n",
      "Epoch(26) - Fold 1 - Validation Utility score : 906.6262\n",
      "Epoch(26) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(26) - Fold 2 - Validation Accuracy : 0.5083\n",
      "Epoch(26) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(26) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(26) - Fold 3 - Validation Accuracy : 0.5121\n",
      "Epoch(26) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(26) - Fold 4 - Validation Loss : 0.6909\n",
      "Epoch(26) - Fold 4 - Validation Accuracy : 0.5244\n",
      "Epoch(26) - Fold 4 - Validation Utility score : 1267.4948\n",
      "Epoch(26) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(26) - GLOBAL - Validation Accuracy: 0.5181\n",
      "Epoch(26) - GLOBAL - Validation Utility score: 2653.6465\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(27) - Training Loss: 0.6925\n",
      "Epoch(27) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(27) - Fold 0 - Validation Accuracy : 0.5234\n",
      "Epoch(27) - Fold 0 - Validation Utility score : 447.6144\n",
      "Epoch(27) - Fold 1 - Validation Loss : 0.6913\n",
      "Epoch(27) - Fold 1 - Validation Accuracy : 0.5211\n",
      "Epoch(27) - Fold 1 - Validation Utility score : 871.1074\n",
      "Epoch(27) - Fold 2 - Validation Loss : 0.6935\n",
      "Epoch(27) - Fold 2 - Validation Accuracy : 0.5082\n",
      "Epoch(27) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(27) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(27) - Fold 3 - Validation Accuracy : 0.5119\n",
      "Epoch(27) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(27) - Fold 4 - Validation Loss : 0.6909\n",
      "Epoch(27) - Fold 4 - Validation Accuracy : 0.5244\n",
      "Epoch(27) - Fold 4 - Validation Utility score : 1237.8535\n",
      "Epoch(27) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(27) - GLOBAL - Validation Accuracy: 0.5178\n",
      "Epoch(27) - GLOBAL - Validation Utility score: 2556.5752\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(28) - Training Loss: 0.6923\n",
      "Epoch(28) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(28) - Fold 0 - Validation Accuracy : 0.5237\n",
      "Epoch(28) - Fold 0 - Validation Utility score : 514.6222\n",
      "Epoch(28) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(28) - Fold 1 - Validation Accuracy : 0.5216\n",
      "Epoch(28) - Fold 1 - Validation Utility score : 859.8465\n",
      "Epoch(28) - Fold 2 - Validation Loss : 0.6935\n",
      "Epoch(28) - Fold 2 - Validation Accuracy : 0.5077\n",
      "Epoch(28) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(28) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(28) - Fold 3 - Validation Accuracy : 0.5118\n",
      "Epoch(28) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(28) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(28) - Fold 4 - Validation Accuracy : 0.5237\n",
      "Epoch(28) - Fold 4 - Validation Utility score : 1280.8905\n",
      "Epoch(28) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(28) - GLOBAL - Validation Accuracy: 0.5177\n",
      "Epoch(28) - GLOBAL - Validation Utility score: 2655.3591\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(29) - Training Loss: 0.6922\n",
      "Epoch(29) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(29) - Fold 0 - Validation Accuracy : 0.5233\n",
      "Epoch(29) - Fold 0 - Validation Utility score : 469.4083\n",
      "Epoch(29) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(29) - Fold 1 - Validation Accuracy : 0.5212\n",
      "Epoch(29) - Fold 1 - Validation Utility score : 868.0843\n",
      "Epoch(29) - Fold 2 - Validation Loss : 0.6935\n",
      "Epoch(29) - Fold 2 - Validation Accuracy : 0.5074\n",
      "Epoch(29) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(29) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(29) - Fold 3 - Validation Accuracy : 0.5124\n",
      "Epoch(29) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(29) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(29) - Fold 4 - Validation Accuracy : 0.5238\n",
      "Epoch(29) - Fold 4 - Validation Utility score : 1265.1675\n",
      "Epoch(29) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(29) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(29) - GLOBAL - Validation Utility score: 2602.6600\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(30) - Training Loss: 0.6921\n",
      "Epoch(30) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(30) - Fold 0 - Validation Accuracy : 0.5226\n",
      "Epoch(30) - Fold 0 - Validation Utility score : 447.4497\n",
      "Epoch(30) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(30) - Fold 1 - Validation Accuracy : 0.5215\n",
      "Epoch(30) - Fold 1 - Validation Utility score : 783.9711\n",
      "Epoch(30) - Fold 2 - Validation Loss : 0.6935\n",
      "Epoch(30) - Fold 2 - Validation Accuracy : 0.5070\n",
      "Epoch(30) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(30) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(30) - Fold 3 - Validation Accuracy : 0.5125\n",
      "Epoch(30) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(30) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(30) - Fold 4 - Validation Accuracy : 0.5242\n",
      "Epoch(30) - Fold 4 - Validation Utility score : 1267.2137\n",
      "Epoch(30) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(30) - GLOBAL - Validation Accuracy: 0.5176\n",
      "Epoch(30) - GLOBAL - Validation Utility score: 2498.6344\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(31) - Training Loss: 0.6920\n",
      "Epoch(31) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(31) - Fold 0 - Validation Accuracy : 0.5225\n",
      "Epoch(31) - Fold 0 - Validation Utility score : 444.8302\n",
      "Epoch(31) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(31) - Fold 1 - Validation Accuracy : 0.5218\n",
      "Epoch(31) - Fold 1 - Validation Utility score : 828.6693\n",
      "Epoch(31) - Fold 2 - Validation Loss : 0.6934\n",
      "Epoch(31) - Fold 2 - Validation Accuracy : 0.5076\n",
      "Epoch(31) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(31) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(31) - Fold 3 - Validation Accuracy : 0.5122\n",
      "Epoch(31) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(31) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(31) - Fold 4 - Validation Accuracy : 0.5242\n",
      "Epoch(31) - Fold 4 - Validation Utility score : 1267.2051\n",
      "Epoch(31) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(31) - GLOBAL - Validation Accuracy: 0.5177\n",
      "Epoch(31) - GLOBAL - Validation Utility score: 2540.7046\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(32) - Training Loss: 0.6919\n",
      "Epoch(32) - Fold 0 - Validation Loss : 0.6914\n",
      "Epoch(32) - Fold 0 - Validation Accuracy : 0.5227\n",
      "Epoch(32) - Fold 0 - Validation Utility score : 421.5428\n",
      "Epoch(32) - Fold 1 - Validation Loss : 0.6912\n",
      "Epoch(32) - Fold 1 - Validation Accuracy : 0.5223\n",
      "Epoch(32) - Fold 1 - Validation Utility score : 876.0694\n",
      "Epoch(32) - Fold 2 - Validation Loss : 0.6935\n",
      "Epoch(32) - Fold 2 - Validation Accuracy : 0.5073\n",
      "Epoch(32) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(32) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(32) - Fold 3 - Validation Accuracy : 0.5123\n",
      "Epoch(32) - Fold 3 - Validation Utility score : 0.8583\n",
      "Epoch(32) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(32) - Fold 4 - Validation Accuracy : 0.5251\n",
      "Epoch(32) - Fold 4 - Validation Utility score : 1291.9901\n",
      "Epoch(32) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(32) - GLOBAL - Validation Accuracy: 0.5180\n",
      "Epoch(32) - GLOBAL - Validation Utility score: 2590.4606\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(33) - Training Loss: 0.6918\n",
      "Epoch(33) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(33) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(33) - Fold 0 - Validation Utility score : 456.0618\n",
      "Epoch(33) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(33) - Fold 1 - Validation Accuracy : 0.5221\n",
      "Epoch(33) - Fold 1 - Validation Utility score : 776.5767\n",
      "Epoch(33) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(33) - Fold 2 - Validation Accuracy : 0.5078\n",
      "Epoch(33) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(33) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(33) - Fold 3 - Validation Accuracy : 0.5132\n",
      "Epoch(33) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(33) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(33) - Fold 4 - Validation Accuracy : 0.5251\n",
      "Epoch(33) - Fold 4 - Validation Utility score : 1282.9846\n",
      "Epoch(33) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(33) - GLOBAL - Validation Accuracy: 0.5183\n",
      "Epoch(33) - GLOBAL - Validation Utility score: 2515.6232\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(34) - Training Loss: 0.6918\n",
      "Epoch(34) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(34) - Fold 0 - Validation Accuracy : 0.5225\n",
      "Epoch(34) - Fold 0 - Validation Utility score : 441.5469\n",
      "Epoch(34) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(34) - Fold 1 - Validation Accuracy : 0.5220\n",
      "Epoch(34) - Fold 1 - Validation Utility score : 819.4300\n",
      "Epoch(34) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(34) - Fold 2 - Validation Accuracy : 0.5080\n",
      "Epoch(34) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(34) - Fold 3 - Validation Loss : 0.6927\n",
      "Epoch(34) - Fold 3 - Validation Accuracy : 0.5130\n",
      "Epoch(34) - Fold 3 - Validation Utility score : 0.1022\n",
      "Epoch(34) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(34) - Fold 4 - Validation Accuracy : 0.5250\n",
      "Epoch(34) - Fold 4 - Validation Utility score : 1325.0161\n",
      "Epoch(34) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(34) - GLOBAL - Validation Accuracy: 0.5181\n",
      "Epoch(34) - GLOBAL - Validation Utility score: 2586.0952\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(35) - Training Loss: 0.6916\n",
      "Epoch(35) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(35) - Fold 0 - Validation Accuracy : 0.5223\n",
      "Epoch(35) - Fold 0 - Validation Utility score : 446.1676\n",
      "Epoch(35) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(35) - Fold 1 - Validation Accuracy : 0.5221\n",
      "Epoch(35) - Fold 1 - Validation Utility score : 841.6081\n",
      "Epoch(35) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(35) - Fold 2 - Validation Accuracy : 0.5078\n",
      "Epoch(35) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(35) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(35) - Fold 3 - Validation Accuracy : 0.5134\n",
      "Epoch(35) - Fold 3 - Validation Utility score : 1.0460\n",
      "Epoch(35) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(35) - Fold 4 - Validation Accuracy : 0.5250\n",
      "Epoch(35) - Fold 4 - Validation Utility score : 1314.9653\n",
      "Epoch(35) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(35) - GLOBAL - Validation Accuracy: 0.5181\n",
      "Epoch(35) - GLOBAL - Validation Utility score: 2603.7870\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(36) - Training Loss: 0.6916\n",
      "Epoch(36) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(36) - Fold 0 - Validation Accuracy : 0.5222\n",
      "Epoch(36) - Fold 0 - Validation Utility score : 459.1791\n",
      "Epoch(36) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(36) - Fold 1 - Validation Accuracy : 0.5215\n",
      "Epoch(36) - Fold 1 - Validation Utility score : 846.9027\n",
      "Epoch(36) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(36) - Fold 2 - Validation Accuracy : 0.5077\n",
      "Epoch(36) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(36) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(36) - Fold 3 - Validation Accuracy : 0.5122\n",
      "Epoch(36) - Fold 3 - Validation Utility score : 0.1217\n",
      "Epoch(36) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(36) - Fold 4 - Validation Accuracy : 0.5240\n",
      "Epoch(36) - Fold 4 - Validation Utility score : 1259.5425\n",
      "Epoch(36) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(36) - GLOBAL - Validation Accuracy: 0.5175\n",
      "Epoch(36) - GLOBAL - Validation Utility score: 2565.7459\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(37) - Training Loss: 0.6916\n",
      "Epoch(37) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(37) - Fold 0 - Validation Accuracy : 0.5223\n",
      "Epoch(37) - Fold 0 - Validation Utility score : 446.0768\n",
      "Epoch(37) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(37) - Fold 1 - Validation Accuracy : 0.5216\n",
      "Epoch(37) - Fold 1 - Validation Utility score : 799.3050\n",
      "Epoch(37) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(37) - Fold 2 - Validation Accuracy : 0.5082\n",
      "Epoch(37) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(37) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(37) - Fold 3 - Validation Accuracy : 0.5125\n",
      "Epoch(37) - Fold 3 - Validation Utility score : 0.0947\n",
      "Epoch(37) - Fold 4 - Validation Loss : 0.6908\n",
      "Epoch(37) - Fold 4 - Validation Accuracy : 0.5256\n",
      "Epoch(37) - Fold 4 - Validation Utility score : 1288.9014\n",
      "Epoch(37) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(37) - GLOBAL - Validation Accuracy: 0.5181\n",
      "Epoch(37) - GLOBAL - Validation Utility score: 2534.3779\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(38) - Training Loss: 0.6915\n",
      "Epoch(38) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(38) - Fold 0 - Validation Accuracy : 0.5233\n",
      "Epoch(38) - Fold 0 - Validation Utility score : 436.2651\n",
      "Epoch(38) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(38) - Fold 1 - Validation Accuracy : 0.5224\n",
      "Epoch(38) - Fold 1 - Validation Utility score : 843.3058\n",
      "Epoch(38) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(38) - Fold 2 - Validation Accuracy : 0.5076\n",
      "Epoch(38) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(38) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(38) - Fold 3 - Validation Accuracy : 0.5119\n",
      "Epoch(38) - Fold 3 - Validation Utility score : 0.2044\n",
      "Epoch(38) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(38) - Fold 4 - Validation Accuracy : 0.5261\n",
      "Epoch(38) - Fold 4 - Validation Utility score : 1315.8282\n",
      "Epoch(38) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(38) - GLOBAL - Validation Accuracy: 0.5183\n",
      "Epoch(38) - GLOBAL - Validation Utility score: 2595.6034\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(39) - Training Loss: 0.6914\n",
      "Epoch(39) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(39) - Fold 0 - Validation Accuracy : 0.5232\n",
      "Epoch(39) - Fold 0 - Validation Utility score : 416.6349\n",
      "Epoch(39) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(39) - Fold 1 - Validation Accuracy : 0.5227\n",
      "Epoch(39) - Fold 1 - Validation Utility score : 904.9045\n",
      "Epoch(39) - Fold 2 - Validation Loss : 0.6936\n",
      "Epoch(39) - Fold 2 - Validation Accuracy : 0.5080\n",
      "Epoch(39) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(39) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(39) - Fold 3 - Validation Accuracy : 0.5126\n",
      "Epoch(39) - Fold 3 - Validation Utility score : 0.2365\n",
      "Epoch(39) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(39) - Fold 4 - Validation Accuracy : 0.5259\n",
      "Epoch(39) - Fold 4 - Validation Utility score : 1296.5601\n",
      "Epoch(39) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(39) - GLOBAL - Validation Accuracy: 0.5185\n",
      "Epoch(39) - GLOBAL - Validation Utility score: 2618.3360\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(40) - Training Loss: 0.6914\n",
      "Epoch(40) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(40) - Fold 0 - Validation Accuracy : 0.5233\n",
      "Epoch(40) - Fold 0 - Validation Utility score : 442.7315\n",
      "Epoch(40) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(40) - Fold 1 - Validation Accuracy : 0.5227\n",
      "Epoch(40) - Fold 1 - Validation Utility score : 913.6678\n",
      "Epoch(40) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(40) - Fold 2 - Validation Accuracy : 0.5085\n",
      "Epoch(40) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(40) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(40) - Fold 3 - Validation Accuracy : 0.5127\n",
      "Epoch(40) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(40) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(40) - Fold 4 - Validation Accuracy : 0.5254\n",
      "Epoch(40) - Fold 4 - Validation Utility score : 1293.0126\n",
      "Epoch(40) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(40) - GLOBAL - Validation Accuracy: 0.5185\n",
      "Epoch(40) - GLOBAL - Validation Utility score: 2649.4119\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(41) - Training Loss: 0.6914\n",
      "Epoch(41) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(41) - Fold 0 - Validation Accuracy : 0.5237\n",
      "Epoch(41) - Fold 0 - Validation Utility score : 442.2042\n",
      "Epoch(41) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(41) - Fold 1 - Validation Accuracy : 0.5231\n",
      "Epoch(41) - Fold 1 - Validation Utility score : 889.5470\n",
      "Epoch(41) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(41) - Fold 2 - Validation Accuracy : 0.5082\n",
      "Epoch(41) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(41) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(41) - Fold 3 - Validation Accuracy : 0.5134\n",
      "Epoch(41) - Fold 3 - Validation Utility score : 0.0960\n",
      "Epoch(41) - Fold 4 - Validation Loss : 0.6906\n",
      "Epoch(41) - Fold 4 - Validation Accuracy : 0.5264\n",
      "Epoch(41) - Fold 4 - Validation Utility score : 1332.5733\n",
      "Epoch(41) - GLOBAL - Validation Loss: 0.6919\n",
      "Epoch(41) - GLOBAL - Validation Accuracy: 0.5190\n",
      "Epoch(41) - GLOBAL - Validation Utility score: 2664.4205\n",
      "Intermediate early stopping : vepoch_loss = 0.6919, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(42) - Training Loss: 0.6913\n",
      "Epoch(42) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(42) - Fold 0 - Validation Accuracy : 0.5228\n",
      "Epoch(42) - Fold 0 - Validation Utility score : 429.7858\n",
      "Epoch(42) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(42) - Fold 1 - Validation Accuracy : 0.5222\n",
      "Epoch(42) - Fold 1 - Validation Utility score : 896.6974\n",
      "Epoch(42) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(42) - Fold 2 - Validation Accuracy : 0.5079\n",
      "Epoch(42) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(42) - Fold 3 - Validation Loss : 0.6928\n",
      "Epoch(42) - Fold 3 - Validation Accuracy : 0.5125\n",
      "Epoch(42) - Fold 3 - Validation Utility score : 0.0201\n",
      "Epoch(42) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(42) - Fold 4 - Validation Accuracy : 0.5254\n",
      "Epoch(42) - Fold 4 - Validation Utility score : 1300.9728\n",
      "Epoch(42) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(42) - GLOBAL - Validation Accuracy: 0.5182\n",
      "Epoch(42) - GLOBAL - Validation Utility score: 2627.4761\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(43) - Training Loss: 0.6913\n",
      "Epoch(43) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(43) - Fold 0 - Validation Accuracy : 0.5226\n",
      "Epoch(43) - Fold 0 - Validation Utility score : 391.3889\n",
      "Epoch(43) - Fold 1 - Validation Loss : 0.6911\n",
      "Epoch(43) - Fold 1 - Validation Accuracy : 0.5228\n",
      "Epoch(43) - Fold 1 - Validation Utility score : 872.0130\n",
      "Epoch(43) - Fold 2 - Validation Loss : 0.6937\n",
      "Epoch(43) - Fold 2 - Validation Accuracy : 0.5087\n",
      "Epoch(43) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(43) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(43) - Fold 3 - Validation Accuracy : 0.5135\n",
      "Epoch(43) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(43) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(43) - Fold 4 - Validation Accuracy : 0.5254\n",
      "Epoch(43) - Fold 4 - Validation Utility score : 1335.7657\n",
      "Epoch(43) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(43) - GLOBAL - Validation Accuracy: 0.5186\n",
      "Epoch(43) - GLOBAL - Validation Utility score: 2599.1676\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(44) - Training Loss: 0.6912\n",
      "Epoch(44) - Fold 0 - Validation Loss : 0.6915\n",
      "Epoch(44) - Fold 0 - Validation Accuracy : 0.5228\n",
      "Epoch(44) - Fold 0 - Validation Utility score : 388.7169\n",
      "Epoch(44) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(44) - Fold 1 - Validation Accuracy : 0.5228\n",
      "Epoch(44) - Fold 1 - Validation Utility score : 806.9117\n",
      "Epoch(44) - Fold 2 - Validation Loss : 0.6938\n",
      "Epoch(44) - Fold 2 - Validation Accuracy : 0.5075\n",
      "Epoch(44) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(44) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(44) - Fold 3 - Validation Accuracy : 0.5133\n",
      "Epoch(44) - Fold 3 - Validation Utility score : 0.0082\n",
      "Epoch(44) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(44) - Fold 4 - Validation Accuracy : 0.5255\n",
      "Epoch(44) - Fold 4 - Validation Utility score : 1378.0258\n",
      "Epoch(44) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(44) - GLOBAL - Validation Accuracy: 0.5184\n",
      "Epoch(44) - GLOBAL - Validation Utility score: 2573.6627\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "\n",
      "\n",
      "Epoch(45) - Training Loss: 0.6912\n",
      "Epoch(45) - Fold 0 - Validation Loss : 0.6916\n",
      "Epoch(45) - Fold 0 - Validation Accuracy : 0.5226\n",
      "Epoch(45) - Fold 0 - Validation Utility score : 391.6375\n",
      "Epoch(45) - Fold 1 - Validation Loss : 0.6910\n",
      "Epoch(45) - Fold 1 - Validation Accuracy : 0.5221\n",
      "Epoch(45) - Fold 1 - Validation Utility score : 867.1320\n",
      "Epoch(45) - Fold 2 - Validation Loss : 0.6939\n",
      "Epoch(45) - Fold 2 - Validation Accuracy : 0.5073\n",
      "Epoch(45) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(45) - Fold 3 - Validation Loss : 0.6929\n",
      "Epoch(45) - Fold 3 - Validation Accuracy : 0.5129\n",
      "Epoch(45) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(45) - Fold 4 - Validation Loss : 0.6907\n",
      "Epoch(45) - Fold 4 - Validation Accuracy : 0.5251\n",
      "Epoch(45) - Fold 4 - Validation Utility score : 1317.1224\n",
      "Epoch(45) - GLOBAL - Validation Loss: 0.6920\n",
      "Epoch(45) - GLOBAL - Validation Accuracy: 0.5180\n",
      "Epoch(45) - GLOBAL - Validation Utility score: 2575.8920\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6919\n",
      "Meet Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19394<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_131403-mcyrv4ea/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_131403-mcyrv4ea/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Global train/loss</td><td>0.69119</td></tr><tr><td>Global valid/Loss</td><td>0.69199</td></tr><tr><td>Global valid/Accuracy</td><td>0.51801</td></tr><tr><td>Global valid/Utility</td><td>2575.89196</td></tr><tr><td>Fold valid Loss/Loss fold 0</td><td>0.69156</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 0</td><td>0.52264</td></tr><tr><td>Fold valid Utility/Utility fold 0</td><td>391.63754</td></tr><tr><td>Fold valid Loss/Loss fold 1</td><td>0.691</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 1</td><td>0.52211</td></tr><tr><td>Fold valid Utility/Utility fold 1</td><td>867.13199</td></tr><tr><td>Fold valid Loss/Loss fold 2</td><td>0.69386</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 2</td><td>0.50734</td></tr><tr><td>Fold valid Utility/Utility fold 2</td><td>-0.0</td></tr><tr><td>Fold valid Loss/Loss fold 3</td><td>0.69285</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 3</td><td>0.51288</td></tr><tr><td>Fold valid Utility/Utility fold 3</td><td>-0.0</td></tr><tr><td>Fold valid Loss/Loss fold 4</td><td>0.69067</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 4</td><td>0.5251</td></tr><tr><td>Fold valid Utility/Utility fold 4</td><td>1317.12242</td></tr><tr><td>Best accuracy</td><td>0.51809</td></tr><tr><td>Best utility</td><td>2601.69728</td></tr><tr><td>_runtime</td><td>2162</td></tr><tr><td>_timestamp</td><td>1613825405</td></tr><tr><td>_step</td><td>45</td></tr><tr><td>Final utility score</td><td>{'utility_score': 26...</td></tr><tr><td>Batch size</td><td>145907</td></tr><tr><td>Patience</td><td>20</td></tr><tr><td>Number of epochs</td><td>1000</td></tr><tr><td>Best epoch</td><td>25</td></tr><tr><td>Number of parameters per layer</td><td>[132, 132, 21516, 16...</td></tr><tr><td>Model architecture</td><td>Model_Resnet(<BR>  (...</td></tr><tr><td>comment</td><td>All folds MLP withOU...</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Global train/loss</td><td>█▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Global valid/Loss</td><td>█▆▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Global valid/Accuracy</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇██▇█▇▇▇▇▇███▇▇▇████▇███████</td></tr><tr><td>Global valid/Utility</td><td>▁▂▂▃▄▅▅▆▆▆▇▇█▇▇██▇▇▇▇▇███████▇██████████</td></tr><tr><td>Fold valid Loss/Loss fold 0</td><td>█▆▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▂</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 0</td><td>▁▄▅▅▆▆▇▇▇▇▇▇███████████████▇██▇▇▇█████▇█</td></tr><tr><td>Fold valid Utility/Utility fold 0</td><td>▁▂▂▃▅▅▅▅▅▇▇▇▆▆▆▇▇█▇▇▇███▇█▇▇▇▇▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>Fold valid Loss/Loss fold 1</td><td>█▆▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 1</td><td>▁▄▆▆▇▆▆▇▇▇▇▇███▇█▇▇▇▇▇█▇▇█▇█████▇███████</td></tr><tr><td>Fold valid Utility/Utility fold 1</td><td>▁▁▂▃▄▄▄▆▆▆▆▇▇▇▇▇█▇▇▇▇▇▇████▇█▇▇▇▇▇█████▇</td></tr><tr><td>Fold valid Loss/Loss fold 2</td><td>▅▃▁▂▂▂▂▁▂▂▂▃▂▂▃▄▄▄▄▄▄▄▄▄▅▅▅▄▅▆▆▆▇▆▆▇▇███</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 2</td><td>▁▂▂▅▃▂▂▅▄▇█▆▆▅▅▅▆▅▆▇▅▄▅▆▆▅▄▅▄▅▅▅▅▄▅▇▆▅▇▄</td></tr><tr><td>Fold valid Utility/Utility fold 2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold valid Loss/Loss fold 3</td><td>█▇▅▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 3</td><td>▁▃▅▅▆▆▆▇▇▇██████████▇▇█▇▇▇▇▇▇███▇▇███▇██</td></tr><tr><td>Fold valid Utility/Utility fold 3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▇▁▂█▂▂▃▁▂▁▁▁</td></tr><tr><td>Fold valid Loss/Loss fold 4</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold valid Accuracy/Accuracy fold 4</td><td>▁▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇███████</td></tr><tr><td>Fold valid Utility/Utility fold 4</td><td>▁▃▂▄▄▆▆▆▇▅▆▇█▇▇▇▇▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██</td></tr><tr><td>Best accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇██████████</td></tr><tr><td>Best utility</td><td>▁▂▂▃▄▅▅▆▆▆▇▇▇████████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 231 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">pretty-sweep-2</strong>: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/mcyrv4ea\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/mcyrv4ea</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training summary:\n",
      "{'utility_score': 2601.697279000122, 'utility_scores': [481.81695882173096, 848.117969407452, -0.0, -0.0, 1271.7623507709386], 'utility_score_std': 492.96356911381145, 'accuracy_scores': [0.5238338223412851, 0.5215584415584416, 0.5076243898085149, 0.5129790646631774, 0.5244752782081984]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jtckmx3u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 462460\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3502928327764403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007069189470562998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_autoenc: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00013772757386708103\n",
      "2021-02-20 13:50:11.379055: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-02-20 13:50:11.379076: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.19<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">splendid-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/sweeps/11zvcrka\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/sweeps/11zvcrka</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/jtckmx3u\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/jtckmx3u</a><br/>\n",
       "                Run data is saved locally in <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_135010-jtckmx3u</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Run config :\n",
      "config: {'activation_function': 'leakyrelu', 'batch_size': 462460, 'dropout': 0.3502928327764403, 'hidden_size': 215, 'learning_rate': 0.0007069189470562998, 'use_autoenc': 'None', 'weight_decay': 0.00013772757386708103, 'epochs': 1000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23319<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_135010-jtckmx3u/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210220_135010-jtckmx3u/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">splendid-sweep-3</strong>: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/jtckmx3u\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/jtckmx3u</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run jtckmx3u errored: RuntimeError('CUDA out of memory. Tried to allocate 760.00 MiB (GPU 0; 23.69 GiB total capacity; 18.47 GiB already allocated; 724.94 MiB free; 19.10 GiB reserved in total by PyTorch)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jtckmx3u errored: RuntimeError('CUDA out of memory. Tried to allocate 760.00 MiB (GPU 0; 23.69 GiB total capacity; 18.47 GiB already allocated; 724.94 MiB free; 19.10 GiB reserved in total by PyTorch)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "if (DO_SWEEP == True):\n",
    "    wandb.agent(sweep_id, function=train, project=\"janestreet-mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 976663<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210218_012251-hcfduko4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/francois/coding/OC/PJ9/wandb/run-20210218_012251-hcfduko4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>23</td></tr><tr><td>_timestamp</td><td>1613607794</td></tr><tr><td>_step</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">dutiful-sweep-2</strong>: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/hcfduko4\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/hcfduko4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if (DO_SINGLE_TRAIN == True):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2579 with shuffle and RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.onnx.export(model, batch, MODEL_FILE_ONNX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note utility score précédent :  \n",
    "\n",
    "{'utility_score': 2699.3290911247423, 'utility_scores': [433.21587581983806, 842.8579165327393, -0.0, -0.0, 1423.255298772165], 'utility_score_std': 541.5654345633293, 'accuracy_scores': [0.5243157432212159, 0.5228992628992629, 0.5123754728769456, 0.5104606336878834, 0.5277785603606142]}\n",
    "\n",
    "(avec std scale)\n",
    "\n",
    "\n",
    "Training summary:\n",
    "{'utility_score': 2697.374406045479, 'utility_scores': [448.22515142892547, 938.181355255796, -0.0, -0.0, 1310.9678993607574], 'utility_score_std': 518.5674763422022, 'accuracy_scores': [0.5212753894345934, 0.5206107406107406, 0.5085924573123425, 0.5062820079914457, 0.525750105648683]}\n",
    "\n",
    "\n",
    "Essayer : \n",
    "> avec features supplémentaires  \n",
    "> augmenter le dropout de la couche avec bcp de 0, ou supprimer la couche  \n",
    "> bouger le weight decay : essayer 1e-5, et 1e-3  \n",
    "> augmenter la taille du batch  \n",
    "> label smoothing   (voir loss_fn = SmoothBCEwLogits(smoothing=0.005)  dans janestreet_kaggle....)  \n",
    "> différents triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_load = nn.Sequential(\n",
    "        #nn.Dropout(0.2),\n",
    "        nn.Linear(len(FEATURES_LIST_TOTRAIN), 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 60),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(60, 30),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "       \n",
    "        nn.Linear(30, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).double().to('cuda')\n",
    "    \n",
    "model_load.load_state_dict(torch.load(f'model_NN_allfolds_V1.pt',map_location=torch.device('cuda')))\n",
    "'''\n",
    "\n",
    "#model_load.eval()\n",
    "#print(accuracy_score(ts_test_y.cpu().numpy(), (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n",
    "#\n",
    "#model_load.eval()\n",
    "#print(utility_function(df.loc[test_index], (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, fill NA with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009838564545944745,\n",
       " 0.38557755173112973,\n",
       " 0.35768747744650975,\n",
       " 0.00891916614596665,\n",
       " 0.0041500560373424495,\n",
       " -0.0037146189993207766,\n",
       " -0.012589244366156346,\n",
       " 0.051776552018932685,\n",
       " 0.026828095990947754,\n",
       " 0.24881331937245502,\n",
       " 0.18234851148590248,\n",
       " 0.08912156181298928,\n",
       " 0.049485535154017296,\n",
       " 0.14310535183278583,\n",
       " 0.08902722352210106,\n",
       " 0.21167757450938102,\n",
       " 0.146300650876364,\n",
       " 0.12121931699105562,\n",
       " 0.11358210894993667,\n",
       " 0.2938148492026861,\n",
       " 0.26876788737703755,\n",
       " 0.18691131282167164,\n",
       " 0.1769785779830002,\n",
       " 0.25244128771902047,\n",
       " 0.23856075429165213,\n",
       " 0.29407078502434514,\n",
       " 0.273177703966479,\n",
       " 0.13548298050171867,\n",
       " 0.16087630644126466,\n",
       " 0.32189235718153003,\n",
       " 0.3425343272966006,\n",
       " 0.2205604165416505,\n",
       " 0.25013120792951216,\n",
       " 0.3082216783372597,\n",
       " 0.3353533199754549,\n",
       " 0.34145307300650396,\n",
       " 0.36582532760649067,\n",
       " 0.029320465264380657,\n",
       " 0.02289177995103487,\n",
       " 0.04002162079212139,\n",
       " 0.05074972651124518,\n",
       " 0.4450543980970518,\n",
       " 0.36018357114469624,\n",
       " 0.34602868865463743,\n",
       " 0.4115306048129169,\n",
       " 0.43803102237933605,\n",
       " 0.47611584684954844,\n",
       " 0.3478667314970123,\n",
       " 0.4996310121501766,\n",
       " 0.5640008775247856,\n",
       " 0.5122603244317466,\n",
       " 0.45738658177408503,\n",
       " 0.04574377016024373,\n",
       " 0.36269998611838594,\n",
       " 0.35886983636928554,\n",
       " 0.652597182609052,\n",
       " 0.8049459750150331,\n",
       " 0.6613497470714732,\n",
       " 0.679812464521779,\n",
       " 0.7625897106372482,\n",
       " 0.5563956951455176,\n",
       " 0.5581652498654769,\n",
       " 0.5455413826521486,\n",
       " 0.546778335577234,\n",
       " 0.43505876086903456,\n",
       " 0.6075653248439333,\n",
       " 0.6085039517279847,\n",
       " 0.5951947177318683,\n",
       " 0.5959424881601522,\n",
       " 0.36954104040881697,\n",
       " 0.24337091134397526,\n",
       " 0.33227354399541986,\n",
       " 0.005393298121375514,\n",
       " -0.03286771852370292,\n",
       " -0.00020445421701550574,\n",
       " -0.019091946690617916,\n",
       " -0.031898281803949866,\n",
       " -0.07680002772370834,\n",
       " -0.00605952373252944,\n",
       " -0.035434597323818526,\n",
       " -0.002099458966825259,\n",
       " -0.014418274525843731,\n",
       " -0.03461507559647192,\n",
       " -0.0800853052057879,\n",
       " 0.3982158398248166,\n",
       " 0.5578243392749649,\n",
       " 0.4024042638598482,\n",
       " 0.44445105036718896,\n",
       " 0.5140947514539359,\n",
       " 0.40051646364922716,\n",
       " 0.41025294617564356,\n",
       " 0.5205121351461534,\n",
       " 0.4050807138243217,\n",
       " 0.4088298932713245,\n",
       " 0.42889387778006083,\n",
       " 0.41763183586073954,\n",
       " 0.40226847198530646,\n",
       " 0.5590945852445534,\n",
       " 0.40710700821605167,\n",
       " 0.4368573700084827,\n",
       " 0.5001226990638947,\n",
       " 0.40856460238921344,\n",
       " 0.40506301819893675,\n",
       " 0.48140573756748783,\n",
       " 0.40165573205168165,\n",
       " 0.40706131905131204,\n",
       " 0.4530453026311098,\n",
       " 0.41501192155955596,\n",
       " 0.39999263512557864,\n",
       " 0.4165405195231404,\n",
       " 0.4007356675614491,\n",
       " 0.40687807118570485,\n",
       " 0.41228491406734663,\n",
       " 0.4026373333613518,\n",
       " 0.40711205681095564,\n",
       " 0.37341742416551793,\n",
       " 0.40443262685246384,\n",
       " 0.40103337181422305,\n",
       " 0.38581705373929415,\n",
       " 0.41559987309867774,\n",
       " 0.33512703226889273,\n",
       " 0.2687757036832287,\n",
       " 0.34355231662463237,\n",
       " 0.27999728178893696,\n",
       " 0.33515369461396705,\n",
       " 0.24487524464105123,\n",
       " 0.3391777949954427,\n",
       " 0.2323808666181667,\n",
       " 0.3425608266217579,\n",
       " 0.24561818215100586]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, normalize with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0088,  0.3957,  0.3306,  0.0092,  0.0034, -0.0050, -0.0146,  0.0553,\n",
       "         0.0251,  0.2647,  0.1671,  0.0949,  0.0445,  0.1525,  0.0800,  0.2217,\n",
       "         0.1283,  0.1218,  0.1096,  0.2977,  0.2646,  0.1881,  0.1725,  0.2547,\n",
       "         0.2327,  0.2979,  0.2685,  0.1399,  0.1629,  0.3306,  0.3439,  0.2268,\n",
       "         0.2519,  0.3164,  0.3360,  0.3528,  0.3677,  0.0265,  0.0186,  0.0432,\n",
       "         0.0530,  0.4542,  0.3776,  0.4162,  0.4393,  0.4865,  0.4921,  0.3684,\n",
       "         0.5014,  0.5438,  0.5307,  0.4567,  0.0565,  0.3890,  0.3769,  0.7755,\n",
       "         0.9247,  0.7859,  0.8085,  0.8990,  0.5534,  0.5555,  0.5592,  0.5614,\n",
       "         0.4423,  0.6188,  0.6172,  0.5977,  0.5981,  0.3774,  0.2389,  0.3080,\n",
       "         0.0041, -0.0322, -0.0016, -0.0199, -0.0316, -0.0932, -0.0081, -0.0358,\n",
       "        -0.0025, -0.0149, -0.0350, -0.1015,  0.3934,  0.5416,  0.3924,  0.4281,\n",
       "         0.4976,  0.3994,  0.4332,  0.5235,  0.4224,  0.4221,  0.4348,  0.4547,\n",
       "         0.3984,  0.5422,  0.3973,  0.4259,  0.4865,  0.4105,  0.4340,  0.4839,\n",
       "         0.4168,  0.4198,  0.4614,  0.4553,  0.3950,  0.3824,  0.3900,  0.3918,\n",
       "         0.3807,  0.4035,  0.4320,  0.3767,  0.4279,  0.4207,  0.3952,  0.4465,\n",
       "         0.3616,  0.2999,  0.3709,  0.3035,  0.3601,  0.2778,  0.3743,  0.2571,\n",
       "         0.3739,  0.2682], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  2.5724,  2.4543,  1.9501,  1.7327,  1.7503,  1.6707,  1.6297,\n",
       "         1.8147,  2.4146,  1.7088,  1.6546,  2.3538,  2.2777,  2.0171,  1.8925,\n",
       "         2.2236,  1.5821,  1.9931,  1.7196,  1.9315,  2.4339,  1.7864,  2.1404,\n",
       "         2.6117,  2.0852,  2.2918,  1.4115,  1.8827,  1.7403,  2.0817,  1.6850,\n",
       "         2.4817,  1.9481,  2.0704,  2.4316,  2.2853,  2.0566,  2.1225,  1.6162,\n",
       "         2.3138,  1.9912,  2.4127,  2.3259,  2.7887,  1.9650,  2.8462,  2.2005,\n",
       "         3.0387,  3.5713,  3.7193,  2.8348,  1.8877,  2.2007,  1.9352,  7.1295,\n",
       "        11.0556,  7.5957,  8.1579,  9.9465,  2.2164,  1.9947,  2.1712,  2.2567,\n",
       "         2.3735,  2.1915,  1.7553,  2.5883,  2.5263,  2.2991,  2.4406,  1.8076,\n",
       "         1.8125,  2.1476,  1.7792,  1.9738,  2.2283,  2.6795,  2.1807,  1.7949,\n",
       "         1.7688,  2.2805,  1.9767,  2.4821,  1.9748,  2.4948,  1.9679,  2.6387,\n",
       "         2.5943,  2.6160,  2.0864,  2.6942,  2.0670,  2.2104,  2.1212,  2.5054,\n",
       "         2.1243,  2.5619,  2.3017,  2.1175,  2.2689,  2.6136,  2.3357,  2.2557,\n",
       "         1.8389,  2.0931,  2.7896,  2.4935,  2.4409,  2.5761,  1.9721,  1.9469,\n",
       "         2.6040,  2.8786,  2.1073,  2.3353,  2.3193,  2.5076,  2.4064,  2.0617,\n",
       "         1.9008,  2.1498,  2.1102,  1.9852,  1.7631,  2.2454,  2.5548,  1.8003,\n",
       "         2.3295,  1.7714], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
