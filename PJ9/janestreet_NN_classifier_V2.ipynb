{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch classifier notebook\n",
    "\n",
    "V1 : only 1 split. First implementation  \n",
    "V2 : with all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET_INPUT_FILE = 'train.csv'\n",
    "\n",
    "FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)]\n",
    "\n",
    "#pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Behavior\n",
    "seed = 42\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Torch RNG\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)\n",
    "# CuDA Determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50000\n",
    "#BATCH_SIZE = 80000\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accounts for variable instance counts in each split by dividing utility_pi by number of instances (but this has been removed)\n",
    "# It also does some copy of dataframe to prevent memory overwrite\n",
    "def utility_function(df_test, df_test_predictions):\n",
    "    df_test_copy = df_test.copy(deep=True)\n",
    "    df_test_copy.loc[:, 'utility_pj'] = df_test_copy['weight'] * df_test_copy['resp'] * df_test_predictions\n",
    "    #df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum() / df_test_copy.groupby('date')['utility_pj'].count()\n",
    "    df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum()\n",
    "\n",
    "    nb_unique_dates = df_test_utility_pi.shape[0]\n",
    "    t = (df_test_utility_pi.sum() / np.sqrt(df_test_utility_pi.pow(2).sum())) * (np.sqrt(250 / np.abs(nb_unique_dates)))\n",
    "    u = min(max(t, 0), 6) * df_test_utility_pi.sum()\n",
    "    del df_test_copy\n",
    "    \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "    \n",
    "df = pd.read_csv(DATASET_INPUT_FILE)\n",
    "df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = PurgedGroupTimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    #n_splits=5,\n",
    "    #max_train_group_size=150,\n",
    "    max_train_group_size=180,\n",
    "    group_gap=20,\n",
    "    max_test_group_size=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index, test_index = next(cv.split(df, (df['resp'] > 0)*1, df['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.loc[train_index, 'resp'] > 0).astype(np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(f_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Sum of model parameters:')\n",
    "#[print(p.sum()) for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter()\n",
    "\n",
    "#writer.add_text('test', 'test:'  + str(model).replace('\\n', '<BR>'))\n",
    "\n",
    "#writer.flush()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=3\n",
    "\n",
    "utility_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(cv.split(df, (df['resp'] > 0)*1, df['date'])):\n",
    "    print('********* Fold : {}'.format(fold))\n",
    "    \n",
    "    ts_train = torch.tensor(df.loc[train_index, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    ts_test = torch.tensor(df.loc[test_index, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    ts_train_y = torch.tensor((df.loc[train_index, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "    ts_test_y = torch.tensor((df.loc[test_index, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "  \n",
    "    train_dataset = torch.utils.data.TensorDataset(ts_train, ts_train_y)\n",
    "    test_dataset = torch.utils.data.TensorDataset(ts_test, ts_test_y)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE) # pin_memory : VOIR RESULTAT\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    " \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(len(FEATURES_LIST_TOTRAIN), 130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(130, 130),\n",
    "        nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(130, 130),\n",
    "        nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(130, 130),\n",
    "        nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(130, 130),\n",
    "        nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(130, 50),\n",
    "        nn.BatchNorm1d(50),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(50, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).double().to('cuda')\n",
    "    \n",
    "    print('Number of model parameters :')\n",
    "    numel_list = [p.numel() for p in model.parameters()]\n",
    "    sum(numel_list), numel_list\n",
    "\n",
    "    loss_fn = nn.BCELoss().to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "  \n",
    "    model.eval()\n",
    "    start_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    start_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    print('Start Validation Accuracy: {:.4f}'.format(start_accuracy))\n",
    "    print('Start Validation Utility: {:.4f}'.format(start_utility_score))\n",
    "\n",
    "    Val_Loss = 0\n",
    "    N_Samples = 0\n",
    "\n",
    "    the_last_loss = 100\n",
    "    the_last_utility_score = 0\n",
    "    the_last_accuracy = 0\n",
    "    trigger_times=0\n",
    "    early_stopping_met = False\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS): \n",
    "        running_loss = 0.0        \n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # update local train loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # update global train loss\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print('Epoch({}) - Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "        \n",
    "        writer.add_scalar(f\"Fold {fold} Loss/train\", epoch_loss, epoch)\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "        vrunning_loss = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch in test_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "\n",
    "            vrunning_loss += loss.item() * inputs.size(0)\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "        # update epoch loss\n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        print('Epoch({}) - Validation Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "\n",
    "        #print(f'Sum of model parameters ({epoch}):')\n",
    "        #[print(p.sum()) for p in model.parameters()]\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vepoch_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "        print('Epoch({}) - Validation Accuracy: {:.4f}'.format(epoch, vepoch_accuracy))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vepoch_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "        print('Epoch({}) - Validation Utility score: {:.4f}'.format(epoch, vepoch_utility_score))\n",
    "\n",
    "        writer.add_scalar(f\"Fold {fold}/valid/Loss\", vepoch_loss, epoch)\n",
    "        writer.add_scalar(f\"Fold {fold}/valid/Accuracy\", vepoch_accuracy, epoch)\n",
    "        writer.add_scalar(f\"Fold {fold}/valid/Utility\", vepoch_utility_score, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        # Check if Early Stopping\n",
    "        #if vepoch_loss > the_last_loss:\n",
    "        if (vepoch_utility_score < the_last_utility_score) and (vepoch_loss > the_last_loss) and (vepoch_accuracy < the_last_accuracy):\n",
    "            trigger_times += 1\n",
    "\n",
    "            print(f'Intermediate early stopping : vepoch_loss = {vepoch_loss:.4f}, the_last_loss={the_last_loss:.4f}')\n",
    "            print(f'Intermediate early stopping : vepoch_accuracy = {vepoch_accuracy:.4f}, the_last_utility_score={the_last_accuracy:.4f}')\n",
    "            print(f'Intermediate early stopping : vepoch_utility_score = {vepoch_utility_score:.4f}, the_last_utility_score={the_last_utility_score:.4f}')\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Meet Early stopping!')\n",
    "                early_stopping_met = True\n",
    "                ##torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "                break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "            the_last_loss = vepoch_loss\n",
    "            the_last_utility_score = vepoch_utility_score\n",
    "            the_last_accuracy = vepoch_accuracy\n",
    "            the_best_epoch = epoch\n",
    "\n",
    "            # Save model for the best version so far\n",
    "            print(f'Saving model corresponding to last_utility_score == {the_last_utility_score}')\n",
    "            torch.save(model.state_dict(), f'model_NN_fold{fold}_V2.pt')\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    # Update global loss\n",
    "    Val_Loss += vepoch_loss * num_samples\n",
    "\n",
    "    # Update global # of samples \n",
    "    N_Samples += num_samples\n",
    "\n",
    "    if (early_stopping_met == False):\n",
    "        print(\"Didn't meet early stopping : saving final model\")\n",
    "        # Save model if don't meet early stopping\n",
    "        torch.save(model.state_dict(), f'model_NN_fold{fold}_V2.pt')\n",
    "        \n",
    "    utility_scores.append(the_last_utility_score)\n",
    "    accuracy_scores.append(the_last_accuracy)\n",
    "    writer.add_text(f\"Fold {fold}/valid/Utility\", f\"Best utility: {the_last_utility_score}\", the_best_epoch)\n",
    "    \n",
    "scores_results = {'utility_score': sum(utility_scores), 'utility_scores': utility_scores, 'utility_score_std': np.std(utility_scores), 'accuracy_scores': accuracy_scores}\n",
    "\n",
    "writer.add_text('Final utility score', str(scores_results))\n",
    "writer.add_text('Batch size', str(BATCH_SIZE))\n",
    "writer.add_text('Patience', str(patience))\n",
    "writer.add_text('Number of epochs', str(NUM_EPOCHS))\n",
    "writer.add_text('Number of parameters per layer', str([p.numel() for p in model.parameters()]))\n",
    "writer.add_text('Model architecture', str(model).replace('\\n', '<BR>'))\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print('Training summary:')\n",
    "print(scores_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec patience 4 : total utility score = 3959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = model = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(len(FEATURES_LIST_TOTRAIN), 130),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(130, 130),\n",
    "    nn.BatchNorm1d(130),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(130, 130),\n",
    "    nn.BatchNorm1d(130),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(130, 130),\n",
    "    nn.BatchNorm1d(130),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(130, 130),\n",
    "    nn.BatchNorm1d(130),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(130, 50),\n",
    "    nn.BatchNorm1d(50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(50, 1),\n",
    "    nn.Sigmoid(),\n",
    ").double().to('cuda')\n",
    "\n",
    "model_load.load_state_dict(torch.load(f'model_NN_fold1_V2.pt',map_location=torch.device('cuda')))\n",
    "\n",
    "model_load.eval()\n",
    "print(accuracy_score(ts_test_y.cpu().numpy(), (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n",
    "\n",
    "model_load.eval()\n",
    "print(utility_function(df.loc[test_index], (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
