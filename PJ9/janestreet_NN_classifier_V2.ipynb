{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch classifier notebook\n",
    "\n",
    "V1 : only 1 split. First implementation  \n",
    "V2 : with all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET_INPUT_FILE = 'train.csv'\n",
    "\n",
    "FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)]\n",
    "\n",
    "#pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Behavior\n",
    "seed = 42\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Torch RNG\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)\n",
    "# CuDA Determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50000\n",
    "#BATCH_SIZE = 80000\n",
    "NUM_EPOCHS = 100\n",
    "MODEL_COMMENT = \"Less layers, dropout 0.3, shuffle=True, early stop on loss only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accounts for variable instance counts in each split by dividing utility_pi by number of instances (but this has been removed)\n",
    "# It also does some copy of dataframe to prevent memory overwrite\n",
    "def utility_function(df_test, df_test_predictions):\n",
    "    df_test_copy = df_test.copy(deep=True)\n",
    "    df_test_copy.loc[:, 'utility_pj'] = df_test_copy['weight'] * df_test_copy['resp'] * df_test_predictions\n",
    "    #df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum() / df_test_copy.groupby('date')['utility_pj'].count()\n",
    "    df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum()\n",
    "\n",
    "    nb_unique_dates = df_test_utility_pi.shape[0]\n",
    "    t = (df_test_utility_pi.sum() / np.sqrt(df_test_utility_pi.pow(2).sum())) * (np.sqrt(250 / np.abs(nb_unique_dates)))\n",
    "    u = min(max(t, 0), 6) * df_test_utility_pi.sum()\n",
    "    del df_test_copy\n",
    "    \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.device"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 3090'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "    \n",
    "df = pd.read_csv(DATASET_INPUT_FILE)\n",
    "df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = PurgedGroupTimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    max_train_group_size=180,\n",
    "    group_gap=20,\n",
    "    max_test_group_size=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index, test_index = next(cv.split(df, (df['resp'] > 0)*1, df['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.loc[train_index, 'resp'] > 0).astype(np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130,)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(f_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Sum of model parameters:')\n",
    "#[print(p.sum()) for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter()\n",
    "\n",
    "#writer.add_text('test', 'test:'  + str(model).replace('\\n', '<BR>'))\n",
    "\n",
    "#writer.flush()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Fold : 0\n",
      "Number of model parameters :\n",
      "Epoch(0) - Training Loss: 0.6937\n",
      "Epoch(0) - Validation Loss: 0.6925\n",
      "Epoch(0) - Validation Accuracy: 0.5121\n",
      "Epoch(0) - Validation Utility score: 219.4475\n",
      "Saving model corresponding to last_utility_score == 219.44753953813495\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.6914\n",
      "Epoch(1) - Validation Loss: 0.6923\n",
      "Epoch(1) - Validation Accuracy: 0.5142\n",
      "Epoch(1) - Validation Utility score: 194.3954\n",
      "Saving model corresponding to last_utility_score == 194.39538663472575\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.6907\n",
      "Epoch(2) - Validation Loss: 0.6921\n",
      "Epoch(2) - Validation Accuracy: 0.5160\n",
      "Epoch(2) - Validation Utility score: 176.8664\n",
      "Saving model corresponding to last_utility_score == 176.8664197009653\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.6903\n",
      "Epoch(3) - Validation Loss: 0.6920\n",
      "Epoch(3) - Validation Accuracy: 0.5168\n",
      "Epoch(3) - Validation Utility score: 204.9829\n",
      "Saving model corresponding to last_utility_score == 204.9828911495038\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.6899\n",
      "Epoch(4) - Validation Loss: 0.6920\n",
      "Epoch(4) - Validation Accuracy: 0.5164\n",
      "Epoch(4) - Validation Utility score: 147.3380\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6920\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 0.6895\n",
      "Epoch(5) - Validation Loss: 0.6920\n",
      "Epoch(5) - Validation Accuracy: 0.5167\n",
      "Epoch(5) - Validation Utility score: 285.1842\n",
      "Saving model corresponding to last_utility_score == 285.1841578916163\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 0.6892\n",
      "Epoch(6) - Validation Loss: 0.6920\n",
      "Epoch(6) - Validation Accuracy: 0.5177\n",
      "Epoch(6) - Validation Utility score: 302.5885\n",
      "Saving model corresponding to last_utility_score == 302.58850429753767\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 0.6887\n",
      "Epoch(7) - Validation Loss: 0.6920\n",
      "Epoch(7) - Validation Accuracy: 0.5175\n",
      "Epoch(7) - Validation Utility score: 374.3854\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6920\n",
      "\n",
      "\n",
      "Epoch(8) - Training Loss: 0.6884\n",
      "Epoch(8) - Validation Loss: 0.6920\n",
      "Epoch(8) - Validation Accuracy: 0.5170\n",
      "Epoch(8) - Validation Utility score: 307.6641\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6920\n",
      "\n",
      "\n",
      "Epoch(9) - Training Loss: 0.6881\n",
      "Epoch(9) - Validation Loss: 0.6920\n",
      "Epoch(9) - Validation Accuracy: 0.5172\n",
      "Epoch(9) - Validation Utility score: 420.7345\n",
      "Intermediate early stopping : vepoch_loss = 0.6920, the_last_loss=0.6920\n",
      "Meet Early stopping!\n",
      "********* Fold : 1\n",
      "Number of model parameters :\n",
      "Epoch(0) - Training Loss: 0.6939\n",
      "Epoch(0) - Validation Loss: 0.6925\n",
      "Epoch(0) - Validation Accuracy: 0.5132\n",
      "Epoch(0) - Validation Utility score: 343.4594\n",
      "Saving model corresponding to last_utility_score == 343.4593955875731\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.6923\n",
      "Epoch(1) - Validation Loss: 0.6922\n",
      "Epoch(1) - Validation Accuracy: 0.5155\n",
      "Epoch(1) - Validation Utility score: 483.0280\n",
      "Saving model corresponding to last_utility_score == 483.0279996762193\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.6917\n",
      "Epoch(2) - Validation Loss: 0.6922\n",
      "Epoch(2) - Validation Accuracy: 0.5173\n",
      "Epoch(2) - Validation Utility score: 557.4295\n",
      "Saving model corresponding to last_utility_score == 557.4294669576022\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.6914\n",
      "Epoch(3) - Validation Loss: 0.6921\n",
      "Epoch(3) - Validation Accuracy: 0.5167\n",
      "Epoch(3) - Validation Utility score: 564.2209\n",
      "Saving model corresponding to last_utility_score == 564.2209424768353\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.6909\n",
      "Epoch(4) - Validation Loss: 0.6921\n",
      "Epoch(4) - Validation Accuracy: 0.5164\n",
      "Epoch(4) - Validation Utility score: 578.7531\n",
      "Saving model corresponding to last_utility_score == 578.7531444847294\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 0.6906\n",
      "Epoch(5) - Validation Loss: 0.6921\n",
      "Epoch(5) - Validation Accuracy: 0.5177\n",
      "Epoch(5) - Validation Utility score: 654.5827\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6921\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 0.6904\n",
      "Epoch(6) - Validation Loss: 0.6921\n",
      "Epoch(6) - Validation Accuracy: 0.5162\n",
      "Epoch(6) - Validation Utility score: 545.5915\n",
      "Intermediate early stopping : vepoch_loss = 0.6921, the_last_loss=0.6921\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 0.6900\n",
      "Epoch(7) - Validation Loss: 0.6922\n",
      "Epoch(7) - Validation Accuracy: 0.5163\n",
      "Epoch(7) - Validation Utility score: 582.1624\n",
      "Intermediate early stopping : vepoch_loss = 0.6922, the_last_loss=0.6921\n",
      "Meet Early stopping!\n",
      "********* Fold : 2\n",
      "Number of model parameters :\n",
      "Epoch(0) - Training Loss: 0.6936\n",
      "Epoch(0) - Validation Loss: 0.6927\n",
      "Epoch(0) - Validation Accuracy: 0.5101\n",
      "Epoch(0) - Validation Utility score: 396.1261\n",
      "Saving model corresponding to last_utility_score == 396.12610396040566\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.6921\n",
      "Epoch(1) - Validation Loss: 0.6926\n",
      "Epoch(1) - Validation Accuracy: 0.5140\n",
      "Epoch(1) - Validation Utility score: 519.0842\n",
      "Saving model corresponding to last_utility_score == 519.0841669967564\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.6916\n",
      "Epoch(2) - Validation Loss: 0.6926\n",
      "Epoch(2) - Validation Accuracy: 0.5130\n",
      "Epoch(2) - Validation Utility score: 468.2840\n",
      "Saving model corresponding to last_utility_score == 468.2840258129403\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.6913\n",
      "Epoch(3) - Validation Loss: 0.6923\n",
      "Epoch(3) - Validation Accuracy: 0.5167\n",
      "Epoch(3) - Validation Utility score: 403.0219\n",
      "Saving model corresponding to last_utility_score == 403.0218692833862\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.6910\n",
      "Epoch(4) - Validation Loss: 0.6923\n",
      "Epoch(4) - Validation Accuracy: 0.5163\n",
      "Epoch(4) - Validation Utility score: 575.6632\n",
      "Saving model corresponding to last_utility_score == 575.663181177165\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 0.6907\n",
      "Epoch(5) - Validation Loss: 0.6923\n",
      "Epoch(5) - Validation Accuracy: 0.5167\n",
      "Epoch(5) - Validation Utility score: 469.8740\n",
      "Intermediate early stopping : vepoch_loss = 0.6923, the_last_loss=0.6923\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 0.6906\n",
      "Epoch(6) - Validation Loss: 0.6923\n",
      "Epoch(6) - Validation Accuracy: 0.5143\n",
      "Epoch(6) - Validation Utility score: 803.2705\n",
      "Intermediate early stopping : vepoch_loss = 0.6923, the_last_loss=0.6923\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 0.6903\n",
      "Epoch(7) - Validation Loss: 0.6924\n",
      "Epoch(7) - Validation Accuracy: 0.5149\n",
      "Epoch(7) - Validation Utility score: 434.6026\n",
      "Intermediate early stopping : vepoch_loss = 0.6924, the_last_loss=0.6923\n",
      "Meet Early stopping!\n",
      "********* Fold : 3\n",
      "Number of model parameters :\n",
      "Epoch(0) - Training Loss: 0.6936\n",
      "Epoch(0) - Validation Loss: 0.6929\n",
      "Epoch(0) - Validation Accuracy: 0.5102\n",
      "Epoch(0) - Validation Utility score: 19.0771\n",
      "Saving model corresponding to last_utility_score == 19.077134275717476\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.6926\n",
      "Epoch(1) - Validation Loss: 0.6929\n",
      "Epoch(1) - Validation Accuracy: 0.5116\n",
      "Epoch(1) - Validation Utility score: 30.1685\n",
      "Saving model corresponding to last_utility_score == 30.168470793952437\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.6923\n",
      "Epoch(2) - Validation Loss: 0.6930\n",
      "Epoch(2) - Validation Accuracy: 0.5105\n",
      "Epoch(2) - Validation Utility score: 58.7129\n",
      "Intermediate early stopping : vepoch_loss = 0.6930, the_last_loss=0.6929\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.6919\n",
      "Epoch(3) - Validation Loss: 0.6929\n",
      "Epoch(3) - Validation Accuracy: 0.5104\n",
      "Epoch(3) - Validation Utility score: 52.0755\n",
      "Intermediate early stopping : vepoch_loss = 0.6929, the_last_loss=0.6929\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.6917\n",
      "Epoch(4) - Validation Loss: 0.6930\n",
      "Epoch(4) - Validation Accuracy: 0.5108\n",
      "Epoch(4) - Validation Utility score: 57.2524\n",
      "Intermediate early stopping : vepoch_loss = 0.6930, the_last_loss=0.6929\n",
      "Meet Early stopping!\n",
      "********* Fold : 4\n",
      "Number of model parameters :\n",
      "Epoch(0) - Training Loss: 0.6938\n",
      "Epoch(0) - Validation Loss: 0.6927\n",
      "Epoch(0) - Validation Accuracy: 0.5119\n",
      "Epoch(0) - Validation Utility score: 1443.9642\n",
      "Saving model corresponding to last_utility_score == 1443.9641943981767\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.6927\n",
      "Epoch(1) - Validation Loss: 0.6923\n",
      "Epoch(1) - Validation Accuracy: 0.5137\n",
      "Epoch(1) - Validation Utility score: 1869.5099\n",
      "Saving model corresponding to last_utility_score == 1869.509912975814\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 0.6923\n",
      "Epoch(2) - Validation Loss: 0.6921\n",
      "Epoch(2) - Validation Accuracy: 0.5171\n",
      "Epoch(2) - Validation Utility score: 2068.0165\n",
      "Saving model corresponding to last_utility_score == 2068.01649465909\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 0.6920\n",
      "Epoch(3) - Validation Loss: 0.6919\n",
      "Epoch(3) - Validation Accuracy: 0.5183\n",
      "Epoch(3) - Validation Utility score: 2247.3395\n",
      "Saving model corresponding to last_utility_score == 2247.339467111302\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 0.6918\n",
      "Epoch(4) - Validation Loss: 0.6918\n",
      "Epoch(4) - Validation Accuracy: 0.5185\n",
      "Epoch(4) - Validation Utility score: 2322.7028\n",
      "Saving model corresponding to last_utility_score == 2322.702766238208\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 0.6915\n",
      "Epoch(5) - Validation Loss: 0.6917\n",
      "Epoch(5) - Validation Accuracy: 0.5195\n",
      "Epoch(5) - Validation Utility score: 2560.3813\n",
      "Saving model corresponding to last_utility_score == 2560.3812996030774\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 0.6914\n",
      "Epoch(6) - Validation Loss: 0.6917\n",
      "Epoch(6) - Validation Accuracy: 0.5195\n",
      "Epoch(6) - Validation Utility score: 2607.6664\n",
      "Saving model corresponding to last_utility_score == 2607.6664395215357\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 0.6912\n",
      "Epoch(7) - Validation Loss: 0.6915\n",
      "Epoch(7) - Validation Accuracy: 0.5199\n",
      "Epoch(7) - Validation Utility score: 2740.1888\n",
      "Saving model corresponding to last_utility_score == 2740.1887515340577\n",
      "\n",
      "\n",
      "Epoch(8) - Training Loss: 0.6910\n",
      "Epoch(8) - Validation Loss: 0.6914\n",
      "Epoch(8) - Validation Accuracy: 0.5212\n",
      "Epoch(8) - Validation Utility score: 2807.7048\n",
      "Saving model corresponding to last_utility_score == 2807.704811088864\n",
      "\n",
      "\n",
      "Epoch(9) - Training Loss: 0.6906\n",
      "Epoch(9) - Validation Loss: 0.6914\n",
      "Epoch(9) - Validation Accuracy: 0.5203\n",
      "Epoch(9) - Validation Utility score: 2645.9496\n",
      "Intermediate early stopping : vepoch_loss = 0.6914, the_last_loss=0.6914\n",
      "\n",
      "\n",
      "Epoch(10) - Training Loss: 0.6904\n",
      "Epoch(10) - Validation Loss: 0.6913\n",
      "Epoch(10) - Validation Accuracy: 0.5205\n",
      "Epoch(10) - Validation Utility score: 2730.5681\n",
      "Saving model corresponding to last_utility_score == 2730.5681094987717\n",
      "\n",
      "\n",
      "Epoch(11) - Training Loss: 0.6901\n",
      "Epoch(11) - Validation Loss: 0.6912\n",
      "Epoch(11) - Validation Accuracy: 0.5215\n",
      "Epoch(11) - Validation Utility score: 2756.4728\n",
      "Saving model corresponding to last_utility_score == 2756.4727739975956\n",
      "\n",
      "\n",
      "Epoch(12) - Training Loss: 0.6898\n",
      "Epoch(12) - Validation Loss: 0.6914\n",
      "Epoch(12) - Validation Accuracy: 0.5199\n",
      "Epoch(12) - Validation Utility score: 2398.7451\n",
      "Intermediate early stopping : vepoch_loss = 0.6914, the_last_loss=0.6912\n",
      "\n",
      "\n",
      "Epoch(13) - Training Loss: 0.6895\n",
      "Epoch(13) - Validation Loss: 0.6914\n",
      "Epoch(13) - Validation Accuracy: 0.5202\n",
      "Epoch(13) - Validation Utility score: 2475.9189\n",
      "Intermediate early stopping : vepoch_loss = 0.6914, the_last_loss=0.6912\n",
      "\n",
      "\n",
      "Epoch(14) - Training Loss: 0.6893\n",
      "Epoch(14) - Validation Loss: 0.6914\n",
      "Epoch(14) - Validation Accuracy: 0.5210\n",
      "Epoch(14) - Validation Utility score: 2433.0744\n",
      "Intermediate early stopping : vepoch_loss = 0.6914, the_last_loss=0.6912\n",
      "Meet Early stopping!\n",
      "Training summary:\n",
      "{'utility_score': 4243.64607475098, 'utility_scores': [302.58850429753767, 578.7531444847294, 575.663181177165, 30.168470793952437, 2756.4727739975956], 'utility_score_std': 975.2178210243251, 'accuracy_scores': [0.517739760335992, 0.5164027669890403, 0.5163386356566387, 0.5115535532626072, 0.521482893853054]}\n"
     ]
    }
   ],
   "source": [
    "patience=3\n",
    "\n",
    "utility_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(cv.split(df, (df['resp'] > 0)*1, df['date'])):\n",
    "    print('********* Fold : {}'.format(fold))\n",
    "    \n",
    "    ts_train = torch.tensor(df.loc[train_index, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    ts_test = torch.tensor(df.loc[test_index, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    ts_train_y = torch.tensor((df.loc[train_index, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "    ts_test_y = torch.tensor((df.loc[test_index, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "  \n",
    "    train_dataset = torch.utils.data.TensorDataset(ts_train, ts_train_y)\n",
    "    test_dataset = torch.utils.data.TensorDataset(ts_test, ts_test_y)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) # pin_memory : VOIR RESULTAT\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    " \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        #nn.Dropout(0.2),\n",
    "        nn.Linear(len(FEATURES_LIST_TOTRAIN), 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(130, 60),\n",
    "        #nn.BatchNorm1d(60),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(60, 30),\n",
    "        #nn.BatchNorm1d(50),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "       \n",
    "        nn.Linear(30, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).double().to('cuda')\n",
    "    \n",
    "    print('Number of model parameters :')\n",
    "    numel_list = [p.numel() for p in model.parameters()]\n",
    "    sum(numel_list), numel_list\n",
    "\n",
    "    loss_fn = nn.BCELoss().to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "  \n",
    "    model.eval()\n",
    "    #start_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #start_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #print('Start Validation Accuracy: {:.4f}'.format(start_accuracy))\n",
    "    #print('Start Validation Utility: {:.4f}'.format(start_utility_score))\n",
    "\n",
    "    Val_Loss = 0\n",
    "    N_Samples = 0\n",
    "\n",
    "    the_last_loss = 100\n",
    "    the_last_utility_score = 0\n",
    "    the_last_accuracy = 0\n",
    "    trigger_times=0\n",
    "    early_stopping_met = False\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS): \n",
    "        running_loss = 0.0        \n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # update local train loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # update global train loss\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print('Epoch({}) - Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "        \n",
    "        writer.add_scalar(f\"Fold {fold} Loss/train\", epoch_loss, epoch)\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "        vrunning_loss = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        for batch in test_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "\n",
    "            vrunning_loss += loss.item() * inputs.size(0)\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "        # update epoch loss\n",
    "        vepoch_loss = vrunning_loss/num_samples\n",
    "        print('Epoch({}) - Validation Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "\n",
    "        #print(f'Sum of model parameters ({epoch}):')\n",
    "        #[print(p.sum()) for p in model.parameters()]\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vepoch_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "        print('Epoch({}) - Validation Accuracy: {:.4f}'.format(epoch, vepoch_accuracy))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vepoch_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "        print('Epoch({}) - Validation Utility score: {:.4f}'.format(epoch, vepoch_utility_score))\n",
    "\n",
    "        writer.add_scalar(f\"Fold {fold}/valid/Loss\", vepoch_loss, epoch)\n",
    "        writer.add_scalar(f\"Fold {fold}/valid/Accuracy\", vepoch_accuracy, epoch)\n",
    "        writer.add_scalar(f\"Fold {fold}/valid/Utility\", vepoch_utility_score, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        # Check if Early Stopping\n",
    "        #if vepoch_loss > the_last_loss:\n",
    "        #if (vepoch_utility_score < the_last_utility_score) and (vepoch_loss > the_last_loss) and (vepoch_accuracy < the_last_accuracy):\n",
    "        if (vepoch_loss > the_last_loss):\n",
    "            trigger_times += 1\n",
    "\n",
    "            print(f'Intermediate early stopping : vepoch_loss = {vepoch_loss:.4f}, the_last_loss={the_last_loss:.4f}')\n",
    "            #print(f'Intermediate early stopping : vepoch_accuracy = {vepoch_accuracy:.4f}, the_last_utility_score={the_last_accuracy:.4f}')\n",
    "            #print(f'Intermediate early stopping : vepoch_utility_score = {vepoch_utility_score:.4f}, the_last_utility_score={the_last_utility_score:.4f}')\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Meet Early stopping!')\n",
    "                early_stopping_met = True\n",
    "                ##torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "                break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "            the_last_loss = vepoch_loss\n",
    "            the_last_utility_score = vepoch_utility_score\n",
    "            the_last_accuracy = vepoch_accuracy\n",
    "            the_best_epoch = epoch\n",
    "\n",
    "            # Save model for the best version so far\n",
    "            print(f'Saving model corresponding to last_utility_score == {the_last_utility_score}')\n",
    "            torch.save(model.state_dict(), f'model_NN_fold{fold}_V2.pt')\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    # Update global loss\n",
    "    Val_Loss += vepoch_loss * num_samples\n",
    "\n",
    "    # Update global # of samples \n",
    "    N_Samples += num_samples\n",
    "\n",
    "    if (early_stopping_met == False):\n",
    "        print(\"Didn't meet early stopping : saving final model\")\n",
    "        # Save model if don't meet early stopping\n",
    "        torch.save(model.state_dict(), f'model_NN_fold{fold}_V2.pt')\n",
    "        \n",
    "    utility_scores.append(the_last_utility_score)\n",
    "    accuracy_scores.append(the_last_accuracy)\n",
    "    writer.add_text(f\"Fold {fold}/valid/Utility\", f\"Best utility: {the_last_utility_score}\", the_best_epoch)\n",
    "    \n",
    "scores_results = {'utility_score': sum(utility_scores), 'utility_scores': utility_scores, 'utility_score_std': np.std(utility_scores), 'accuracy_scores': accuracy_scores}\n",
    "\n",
    "writer.add_text('Final utility score', str(scores_results))\n",
    "writer.add_text('Batch size', str(BATCH_SIZE))\n",
    "writer.add_text('Patience', str(patience))\n",
    "writer.add_text('Number of epochs', str(NUM_EPOCHS))\n",
    "writer.add_text('Number of parameters per layer', str([p.numel() for p in model.parameters()]))\n",
    "writer.add_text('Model architecture', str(model).replace('\\n', '<BR>'))\n",
    "writer.add_text('Comment', MODEL_COMMENT)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print('Training summary:')\n",
    "print(scores_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5210155043318652"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2433.0743762245115"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec patience 4 : total utility score = 3959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521482893853054\n",
      "2756.4727739975956\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_load = nn.Sequential(\n",
    "        #nn.Dropout(0.2),\n",
    "        nn.Linear(len(FEATURES_LIST_TOTRAIN), 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(130, 60),\n",
    "        #nn.BatchNorm1d(60),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(60, 30),\n",
    "        #nn.BatchNorm1d(50),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "       \n",
    "        nn.Linear(30, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).double().to('cuda')\n",
    "    \n",
    "model_load.load_state_dict(torch.load(f'model_NN_fold4_V2.pt',map_location=torch.device('cuda')))\n",
    "\n",
    "model_load.eval()\n",
    "print(accuracy_score(ts_test_y.cpu().numpy(), (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n",
    "\n",
    "model_load.eval()\n",
    "print(utility_function(df.loc[test_index], (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
