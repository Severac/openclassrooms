START OF OPTIMIZATION:
Run 0 since program start
2020-12-19 23:01:29.228687
Found saved Trials! Loading...
Rerunning from 4 trials to add another one.
 80%|████████  | 4/5 [00:00<?, ?trial/s, best loss=?]                                                     New call of f
 80%|████████  | 4/5 [00:00<00:00, 246.71trial/s, best loss=?]                                                              New call of hyperopt_train_test
 80%|████████  | 4/5 [00:00<00:00, 244.46trial/s, best loss=?]                                                              Model used for fitting:
 80%|████████  | 4/5 [01:32<00:23, 23.03s/trial, best loss=?]                                                             XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 80%|████████  | 4/5 [01:32<00:23, 23.04s/trial, best loss=?]                                                             [23:03:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 80%|████████  | 4/5 [01:35<00:23, 23.78s/trial, best loss=?]                                                             predict called
 80%|████████  | 4/5 [01:59<00:29, 29.96s/trial, best loss=?]                                                             Type of X:
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             Shape of X:
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             (259530, 139)
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             Type of y:
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             model fitted ?
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             True
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             y is not None
 80%|████████  | 4/5 [02:02<00:30, 30.70s/trial, best loss=?]                                                             Model used for fitting:
 80%|████████  | 4/5 [02:07<00:31, 31.81s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 80%|████████  | 4/5 [02:07<00:31, 31.81s/trial, best loss=?]                                                             [23:03:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 80%|████████  | 4/5 [02:09<00:32, 32.37s/trial, best loss=?]                                                             predict called
 80%|████████  | 4/5 [02:32<00:38, 38.03s/trial, best loss=?]                                                             Type of X:
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             Shape of X:
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             (271631, 139)
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             Type of y:
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             model fitted ?
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             True
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             y is not None
 80%|████████  | 4/5 [02:35<00:38, 38.82s/trial, best loss=?]                                                             Model used for fitting:
 80%|████████  | 4/5 [02:39<00:39, 39.92s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 80%|████████  | 4/5 [02:39<00:39, 39.92s/trial, best loss=?]                                                             [23:04:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 80%|████████  | 4/5 [02:41<00:40, 40.48s/trial, best loss=?]                                                             predict called
 80%|████████  | 4/5 [03:04<00:46, 46.18s/trial, best loss=?]                                                             Type of X:
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             Shape of X:
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             (286101, 139)
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             Type of y:
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             model fitted ?
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             True
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             y is not None
 80%|████████  | 4/5 [03:08<00:47, 47.00s/trial, best loss=?]                                                             Model used for fitting:
 80%|████████  | 4/5 [03:12<00:48, 48.20s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 80%|████████  | 4/5 [03:12<00:48, 48.20s/trial, best loss=?]                                                             [23:04:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 80%|████████  | 4/5 [03:15<00:48, 48.80s/trial, best loss=?]                                                             predict called
 80%|████████  | 4/5 [03:38<00:54, 54.67s/trial, best loss=?]                                                             Type of X:
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             Shape of X:
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             (289002, 139)
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             Type of y:
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             model fitted ?
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             True
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             y is not None
 80%|████████  | 4/5 [03:42<00:55, 55.50s/trial, best loss=?]                                                             Model used for fitting:
 80%|████████  | 4/5 [03:47<00:56, 56.75s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 80%|████████  | 4/5 [03:47<00:56, 56.75s/trial, best loss=?]                                                             [23:05:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 80%|████████  | 4/5 [03:49<00:57, 57.37s/trial, best loss=?]                                                             predict called
 80%|████████  | 4/5 [04:13<01:03, 63.40s/trial, best loss=?]                                                             Type of X:
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             Shape of X:
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             (323071, 139)
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             Type of y:
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             model fitted ?
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             True
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]                                                             y is not None
 80%|████████  | 4/5 [04:17<01:04, 64.40s/trial, best loss=?]100%|██████████| 5/5 [04:18<00:00, 258.54s/trial, best loss: -2752.0309103460922]100%|██████████| 5/5 [04:18<00:00, 51.71s/trial, best loss: -2752.0309103460922] 
Best model so far :
{'colsample_bytree': 2, 'features': 0, 'learning_rate': 2, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 2, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 1 since program start
2020-12-19 23:05:47.980777
Found saved Trials! Loading...
Rerunning from 5 trials to add another one.
 83%|████████▎ | 5/6 [00:00<?, ?trial/s, best loss=?]                                                     New call of f
 83%|████████▎ | 5/6 [00:00<00:00, 310.57trial/s, best loss=?]                                                              New call of hyperopt_train_test
 83%|████████▎ | 5/6 [00:00<00:00, 307.35trial/s, best loss=?]                                                              Model used for fitting:
 83%|████████▎ | 5/6 [01:30<00:18, 18.18s/trial, best loss=?]                                                             XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 83%|████████▎ | 5/6 [01:30<00:18, 18.18s/trial, best loss=?]                                                             [23:07:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 5/6 [01:34<00:18, 18.92s/trial, best loss=?]                                                             predict called
 83%|████████▎ | 5/6 [05:17<01:03, 63.58s/trial, best loss=?]                                                             Type of X:
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             Shape of X:
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             (259530, 139)
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             Type of y:
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             model fitted ?
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             True
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             y is not None
 83%|████████▎ | 5/6 [05:24<01:04, 64.86s/trial, best loss=?]                                                             Model used for fitting:
 83%|████████▎ | 5/6 [05:28<01:05, 65.73s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 5/6 [05:28<01:05, 65.73s/trial, best loss=?]                                                             [23:11:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 5/6 [05:31<01:06, 66.31s/trial, best loss=?]                                                             predict called
 83%|████████▎ | 5/6 [08:48<01:45, 105.75s/trial, best loss=?]                                                              Type of X:
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              Shape of X:
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              (271631, 139)
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              Type of y:
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              model fitted ?
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              True
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              y is not None
 83%|████████▎ | 5/6 [08:55<01:47, 107.02s/trial, best loss=?]                                                              Model used for fitting:
 83%|████████▎ | 5/6 [08:59<01:47, 107.88s/trial, best loss=?]                                                              XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 5/6 [08:59<01:47, 107.89s/trial, best loss=?]                                                              [23:14:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 5/6 [09:02<01:48, 108.45s/trial, best loss=?]                                                              predict called
 83%|████████▎ | 5/6 [12:22<02:28, 148.53s/trial, best loss=?]                                                              Type of X:
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              Shape of X:
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              (286101, 139)
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              Type of y:
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              model fitted ?
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              True
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              y is not None
 83%|████████▎ | 5/6 [12:29<02:29, 149.94s/trial, best loss=?]                                                              Model used for fitting:
 83%|████████▎ | 5/6 [12:34<02:30, 150.91s/trial, best loss=?]                                                              XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 5/6 [12:34<02:30, 150.91s/trial, best loss=?]                                                              [23:18:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 5/6 [12:37<02:31, 151.57s/trial, best loss=?]                                                              predict called
 83%|████████▎ | 5/6 [16:11<03:14, 194.39s/trial, best loss=?]                                                              Type of X:
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              Shape of X:
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              (289002, 139)
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              Type of y:
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              model fitted ?
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              True
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              y is not None
 83%|████████▎ | 5/6 [16:19<03:15, 195.90s/trial, best loss=?]                                                              Model used for fitting:
 83%|████████▎ | 5/6 [16:24<03:16, 196.94s/trial, best loss=?]                                                              XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 5/6 [16:24<03:16, 196.94s/trial, best loss=?]                                                              [23:22:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 5/6 [16:27<03:17, 197.59s/trial, best loss=?]                                                              predict called
 83%|████████▎ | 5/6 [20:07<04:01, 241.59s/trial, best loss=?]                                                              Type of X:
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              Shape of X:
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              (323071, 139)
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              Type of y:
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              model fitted ?
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              True
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]                                                              y is not None
 83%|████████▎ | 5/6 [20:16<04:03, 243.28s/trial, best loss=?]100%|██████████| 6/6 [20:17<00:00, 1217.47s/trial, best loss: -2752.0309103460922]100%|██████████| 6/6 [20:17<00:00, 202.91s/trial, best loss: -2752.0309103460922] 
Best model so far :
{'colsample_bytree': 2, 'features': 0, 'learning_rate': 2, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 2, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 2 since program start
2020-12-19 23:26:05.500854
Found saved Trials! Loading...
Rerunning from 6 trials to add another one.
 86%|████████▌ | 6/7 [00:00<?, ?trial/s, best loss=?]                                                     New call of f
 86%|████████▌ | 6/7 [00:00<00:00, 369.52trial/s, best loss=?]                                                              New call of hyperopt_train_test
 86%|████████▌ | 6/7 [00:00<00:00, 365.53trial/s, best loss=?]                                                              Model used for fitting:
 86%|████████▌ | 6/7 [01:32<00:15, 15.43s/trial, best loss=?]                                                             XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 86%|████████▌ | 6/7 [01:32<00:15, 15.43s/trial, best loss=?]                                                             [23:27:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 6/7 [01:35<00:15, 15.89s/trial, best loss=?]                                                             predict called
 86%|████████▌ | 6/7 [01:48<00:18, 18.14s/trial, best loss=?]                                                             Type of X:
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             Shape of X:
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             (259530, 139)
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             Type of y:
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             model fitted ?
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             True
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             y is not None
 86%|████████▌ | 6/7 [01:50<00:18, 18.42s/trial, best loss=?]                                                             Model used for fitting:
 86%|████████▌ | 6/7 [01:55<00:19, 19.18s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 6/7 [01:55<00:19, 19.18s/trial, best loss=?]                                                             [23:28:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 6/7 [01:57<00:19, 19.56s/trial, best loss=?]                                                             predict called
 86%|████████▌ | 6/7 [02:09<00:21, 21.66s/trial, best loss=?]                                                             Type of X:
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             Shape of X:
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             (271631, 139)
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             Type of y:
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             model fitted ?
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             True
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             y is not None
 86%|████████▌ | 6/7 [02:11<00:21, 21.96s/trial, best loss=?]                                                             Model used for fitting:
 86%|████████▌ | 6/7 [02:16<00:22, 22.71s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 6/7 [02:16<00:22, 22.71s/trial, best loss=?]                                                             [23:28:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 6/7 [02:18<00:23, 23.08s/trial, best loss=?]                                                             predict called
 86%|████████▌ | 6/7 [02:31<00:25, 25.19s/trial, best loss=?]                                                             Type of X:
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             Shape of X:
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             (286101, 139)
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             Type of y:
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             model fitted ?
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             True
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             y is not None
 86%|████████▌ | 6/7 [02:33<00:25, 25.51s/trial, best loss=?]                                                             Model used for fitting:
 86%|████████▌ | 6/7 [02:37<00:26, 26.33s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 6/7 [02:37<00:26, 26.33s/trial, best loss=?]                                                             [23:28:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 6/7 [02:40<00:26, 26.74s/trial, best loss=?]                                                             predict called
 86%|████████▌ | 6/7 [02:53<00:28, 28.92s/trial, best loss=?]                                                             Type of X:
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             Shape of X:
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             (289002, 139)
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             Type of y:
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             model fitted ?
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             True
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             y is not None
 86%|████████▌ | 6/7 [02:55<00:29, 29.24s/trial, best loss=?]                                                             Model used for fitting:
 86%|████████▌ | 6/7 [03:00<00:30, 30.10s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 6/7 [03:00<00:30, 30.10s/trial, best loss=?]                                                             [23:29:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 6/7 [03:03<00:30, 30.52s/trial, best loss=?]                                                             predict called
 86%|████████▌ | 6/7 [03:16<00:32, 32.71s/trial, best loss=?]                                                             Type of X:
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             Shape of X:
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             (323071, 139)
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             Type of y:
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             model fitted ?
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             True
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]                                                             y is not None
 86%|████████▌ | 6/7 [03:18<00:33, 33.12s/trial, best loss=?]100%|██████████| 7/7 [03:19<00:00, 199.76s/trial, best loss: -2752.0309103460922]100%|██████████| 7/7 [03:19<00:00, 28.54s/trial, best loss: -2752.0309103460922] 
Best model so far :
{'colsample_bytree': 2, 'features': 0, 'learning_rate': 2, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 2, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 3 since program start
2020-12-19 23:29:25.364089
Found saved Trials! Loading...
Rerunning from 7 trials to add another one.
 88%|████████▊ | 7/8 [00:00<?, ?trial/s, best loss=?]                                                     New call of f
 88%|████████▊ | 7/8 [00:00<00:00, 439.30trial/s, best loss=?]                                                              New call of hyperopt_train_test
 88%|████████▊ | 7/8 [00:00<00:00, 433.15trial/s, best loss=?]                                                              Model used for fitting:
 88%|████████▊ | 7/8 [01:31<00:13, 13.06s/trial, best loss=?]                                                             XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 88%|████████▊ | 7/8 [01:31<00:13, 13.06s/trial, best loss=?]                                                             [23:30:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 7/8 [01:34<00:13, 13.44s/trial, best loss=?]                                                             predict called
 88%|████████▊ | 7/8 [01:35<00:13, 13.66s/trial, best loss=?]                                                             Type of X:
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             Shape of X:
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             (259530, 139)
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             Type of y:
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             model fitted ?
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             True
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             y is not None
 88%|████████▊ | 7/8 [01:36<00:13, 13.75s/trial, best loss=?]                                                             Model used for fitting:
 88%|████████▊ | 7/8 [01:40<00:14, 14.40s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 7/8 [01:40<00:14, 14.40s/trial, best loss=?]                                                             [23:31:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 7/8 [01:43<00:14, 14.72s/trial, best loss=?]                                                             predict called
 88%|████████▊ | 7/8 [01:44<00:14, 14.91s/trial, best loss=?]                                                             Type of X:
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             Shape of X:
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             (271631, 139)
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             Type of y:
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             model fitted ?
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             True
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             y is not None
 88%|████████▊ | 7/8 [01:45<00:15, 15.01s/trial, best loss=?]                                                             Model used for fitting:
 88%|████████▊ | 7/8 [01:49<00:15, 15.66s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 7/8 [01:49<00:15, 15.66s/trial, best loss=?]                                                             [23:31:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 7/8 [01:51<00:15, 15.98s/trial, best loss=?]                                                             predict called
 88%|████████▊ | 7/8 [01:53<00:16, 16.18s/trial, best loss=?]                                                             Type of X:
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             Shape of X:
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             (286101, 139)
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             Type of y:
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             model fitted ?
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             True
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             y is not None
 88%|████████▊ | 7/8 [01:53<00:16, 16.28s/trial, best loss=?]                                                             Model used for fitting:
 88%|████████▊ | 7/8 [01:58<00:16, 16.98s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 7/8 [01:58<00:16, 16.98s/trial, best loss=?]                                                             [23:31:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 7/8 [02:01<00:17, 17.32s/trial, best loss=?]                                                             predict called
 88%|████████▊ | 7/8 [02:02<00:17, 17.53s/trial, best loss=?]                                                             Type of X:
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             Shape of X:
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             (289002, 139)
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             Type of y:
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             model fitted ?
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             True
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             y is not None
 88%|████████▊ | 7/8 [02:03<00:17, 17.63s/trial, best loss=?]                                                             Model used for fitting:
 88%|████████▊ | 7/8 [02:08<00:18, 18.35s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 7/8 [02:08<00:18, 18.35s/trial, best loss=?]                                                             [23:31:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 7/8 [02:10<00:18, 18.71s/trial, best loss=?]                                                             predict called
 88%|████████▊ | 7/8 [02:12<00:18, 18.92s/trial, best loss=?]                                                             Type of X:
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             Shape of X:
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             (323071, 139)
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             Type of y:
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             model fitted ?
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             True
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]                                                             y is not None
 88%|████████▊ | 7/8 [02:13<00:19, 19.08s/trial, best loss=?]100%|██████████| 8/8 [02:14<00:00, 134.63s/trial, best loss: -2752.0309103460922]100%|██████████| 8/8 [02:14<00:00, 16.83s/trial, best loss: -2752.0309103460922] 
Best model so far :
{'colsample_bytree': 2, 'features': 0, 'learning_rate': 2, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 2, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 4 since program start
2020-12-19 23:31:40.037843
Found saved Trials! Loading...
Rerunning from 8 trials to add another one.
 89%|████████▉ | 8/9 [00:00<?, ?trial/s, best loss=?]                                                     New call of f
 89%|████████▉ | 8/9 [00:00<00:00, 458.89trial/s, best loss=?]                                                              New call of hyperopt_train_test
 89%|████████▉ | 8/9 [00:00<00:00, 454.88trial/s, best loss=?]                                                              Model used for fitting:
 89%|████████▉ | 8/9 [01:31<00:11, 11.48s/trial, best loss=?]                                                             XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 89%|████████▉ | 8/9 [01:31<00:11, 11.48s/trial, best loss=?]                                                             [23:33:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 8/9 [01:34<00:11, 11.81s/trial, best loss=?]                                                             predict called
 89%|████████▉ | 8/9 [01:35<00:11, 11.98s/trial, best loss=?]                                                             Type of X:
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             Shape of X:
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             (259530, 139)
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             Type of y:
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             model fitted ?
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             True
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             y is not None
 89%|████████▉ | 8/9 [01:36<00:12, 12.06s/trial, best loss=?]                                                             Model used for fitting:
 89%|████████▉ | 8/9 [01:41<00:12, 12.63s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 8/9 [01:41<00:12, 12.63s/trial, best loss=?]                                                             [23:33:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 8/9 [01:43<00:12, 12.91s/trial, best loss=?]                                                             predict called
 89%|████████▉ | 8/9 [01:44<00:13, 13.07s/trial, best loss=?]                                                             Type of X:
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             Shape of X:
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             (271631, 139)
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             Type of y:
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             model fitted ?
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             True
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             y is not None
 89%|████████▉ | 8/9 [01:45<00:13, 13.16s/trial, best loss=?]                                                             Model used for fitting:
 89%|████████▉ | 8/9 [01:49<00:13, 13.72s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 8/9 [01:49<00:13, 13.72s/trial, best loss=?]                                                             [23:33:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 8/9 [01:52<00:14, 14.00s/trial, best loss=?]                                                             predict called
 89%|████████▉ | 8/9 [01:53<00:14, 14.17s/trial, best loss=?]                                                             Type of X:
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             Shape of X:
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             (286101, 139)
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             Type of y:
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             model fitted ?
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             True
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             y is not None
 89%|████████▉ | 8/9 [01:54<00:14, 14.26s/trial, best loss=?]                                                             Model used for fitting:
 89%|████████▉ | 8/9 [01:58<00:14, 14.87s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 8/9 [01:58<00:14, 14.87s/trial, best loss=?]                                                             [23:33:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 8/9 [02:01<00:15, 15.17s/trial, best loss=?]                                                             predict called
 89%|████████▉ | 8/9 [02:02<00:15, 15.34s/trial, best loss=?]                                                             Type of X:
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             Shape of X:
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             (289002, 139)
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             Type of y:
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             model fitted ?
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             True
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             y is not None
 89%|████████▉ | 8/9 [02:03<00:15, 15.43s/trial, best loss=?]                                                             Model used for fitting:
 89%|████████▉ | 8/9 [02:08<00:16, 16.06s/trial, best loss=?]                                                             XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 8/9 [02:08<00:16, 16.06s/trial, best loss=?]                                                             [23:33:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 8/9 [02:10<00:16, 16.37s/trial, best loss=?]                                                             predict called
 89%|████████▉ | 8/9 [02:12<00:16, 16.54s/trial, best loss=?]                                                             Type of X:
 89%|████████▉ | 8/9 [02:13<00:16, 16.68s/trial, best loss=?]                                                             <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]                                                             Shape of X:
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]                                                             (323071, 139)
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]                                                             Type of y:
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]                                                             <class 'numpy.ndarray'>
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]                                                             model fitted ?
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]                                                             True
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]                                                             y is not None
 89%|████████▉ | 8/9 [02:13<00:16, 16.69s/trial, best loss=?]100%|██████████| 9/9 [02:14<00:00, 134.53s/trial, best loss: -3056.1962269866444]100%|██████████| 9/9 [02:14<00:00, 14.95s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 5 since program start
2020-12-19 23:33:54.612780
Found saved Trials! Loading...
Rerunning from 9 trials to add another one.
 90%|█████████ | 9/10 [00:00<?, ?trial/s, best loss=?]                                                      New call of f
 90%|█████████ | 9/10 [00:00<00:00, 562.66trial/s, best loss=?]                                                               New call of hyperopt_train_test
 90%|█████████ | 9/10 [00:00<00:00, 557.37trial/s, best loss=?]                                                               Model used for fitting:
 90%|█████████ | 9/10 [01:32<00:10, 10.25s/trial, best loss=?]                                                              XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 90%|█████████ | 9/10 [01:32<00:10, 10.25s/trial, best loss=?]                                                              [23:35:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|█████████ | 9/10 [01:34<00:10, 10.55s/trial, best loss=?]                                                              predict called
 90%|█████████ | 9/10 [03:09<00:21, 21.06s/trial, best loss=?]                                                              Type of X:
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              Shape of X:
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              (259530, 139)
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              Type of y:
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              model fitted ?
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              True
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              y is not None
 90%|█████████ | 9/10 [03:14<00:21, 21.61s/trial, best loss=?]                                                              Model used for fitting:
 90%|█████████ | 9/10 [03:18<00:22, 22.10s/trial, best loss=?]                                                              XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|█████████ | 9/10 [03:18<00:22, 22.10s/trial, best loss=?]                                                              [23:37:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|█████████ | 9/10 [03:21<00:22, 22.36s/trial, best loss=?]                                                              predict called
 90%|█████████ | 9/10 [04:47<00:31, 31.96s/trial, best loss=?]                                                              Type of X:
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              Shape of X:
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              (271631, 139)
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              Type of y:
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              model fitted ?
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              True
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              y is not None
 90%|█████████ | 9/10 [04:52<00:32, 32.54s/trial, best loss=?]                                                              Model used for fitting:
 90%|█████████ | 9/10 [04:57<00:33, 33.03s/trial, best loss=?]                                                              XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|█████████ | 9/10 [04:57<00:33, 33.03s/trial, best loss=?]                                                              [23:38:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|█████████ | 9/10 [04:59<00:33, 33.28s/trial, best loss=?]                                                              predict called
 90%|█████████ | 9/10 [06:28<00:43, 43.11s/trial, best loss=?]                                                              Type of X:
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              Shape of X:
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              (286101, 139)
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              Type of y:
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              model fitted ?
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              True
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              y is not None
 90%|█████████ | 9/10 [06:33<00:43, 43.73s/trial, best loss=?]                                                              Model used for fitting:
 90%|█████████ | 9/10 [06:38<00:44, 44.26s/trial, best loss=?]                                                              XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|█████████ | 9/10 [06:38<00:44, 44.26s/trial, best loss=?]                                                              [23:40:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|█████████ | 9/10 [06:40<00:44, 44.54s/trial, best loss=?]                                                              predict called
 90%|█████████ | 9/10 [08:11<00:54, 54.62s/trial, best loss=?]                                                              Type of X:
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              Shape of X:
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              (289002, 139)
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              Type of y:
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              model fitted ?
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              True
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              y is not None
 90%|█████████ | 9/10 [08:17<00:55, 55.25s/trial, best loss=?]                                                              Model used for fitting:
 90%|█████████ | 9/10 [08:22<00:55, 55.81s/trial, best loss=?]                                                              XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|█████████ | 9/10 [08:22<00:55, 55.81s/trial, best loss=?]                                                              [23:42:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|█████████ | 9/10 [08:24<00:56, 56.09s/trial, best loss=?]                                                              predict called
 90%|█████████ | 9/10 [09:58<01:06, 66.45s/trial, best loss=?]                                                              Type of X:
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              <class 'pandas.core.frame.DataFrame'>
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              Shape of X:
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              (323071, 139)
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              Type of y:
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              <class 'numpy.ndarray'>
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              model fitted ?
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              True
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]                                                              y is not None
 90%|█████████ | 9/10 [10:04<01:07, 67.19s/trial, best loss=?]100%|██████████| 10/10 [10:05<00:00, 605.73s/trial, best loss: -3056.1962269866444]100%|██████████| 10/10 [10:05<00:00, 60.57s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 6 since program start
2020-12-19 23:44:00.445352
Found saved Trials! Loading...
Rerunning from 10 trials to add another one.
 91%|█████████ | 10/11 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 91%|█████████ | 10/11 [00:00<00:00, 620.52trial/s, best loss=?]                                                                New call of hyperopt_train_test
 91%|█████████ | 10/11 [00:00<00:00, 615.00trial/s, best loss=?]                                                                Model used for fitting:
 91%|█████████ | 10/11 [01:32<00:09,  9.22s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 91%|█████████ | 10/11 [01:32<00:09,  9.22s/trial, best loss=?]                                                               [23:45:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 10/11 [01:34<00:09,  9.48s/trial, best loss=?]                                                               predict called
 91%|█████████ | 10/11 [02:33<00:15, 15.39s/trial, best loss=?]                                                               Type of X:
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               Shape of X:
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               (259530, 139)
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               Type of y:
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               model fitted ?
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               True
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               y is not None
 91%|█████████ | 10/11 [02:37<00:15, 15.78s/trial, best loss=?]                                                               Model used for fitting:
 91%|█████████ | 10/11 [02:42<00:16, 16.22s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 10/11 [02:42<00:16, 16.22s/trial, best loss=?]                                                               [23:46:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 10/11 [02:44<00:16, 16.45s/trial, best loss=?]                                                               predict called
 91%|█████████ | 10/11 [03:39<00:21, 21.94s/trial, best loss=?]                                                               Type of X:
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               Shape of X:
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               (271631, 139)
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               Type of y:
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               model fitted ?
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               True
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               y is not None
 91%|█████████ | 10/11 [03:43<00:22, 22.35s/trial, best loss=?]                                                               Model used for fitting:
 91%|█████████ | 10/11 [03:47<00:22, 22.80s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 10/11 [03:47<00:22, 22.80s/trial, best loss=?]                                                               [23:47:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 10/11 [03:50<00:23, 23.03s/trial, best loss=?]                                                               predict called
 91%|█████████ | 10/11 [04:46<00:28, 28.61s/trial, best loss=?]                                                               Type of X:
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               Shape of X:
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               (286101, 139)
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               Type of y:
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               model fitted ?
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               True
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               y is not None
 91%|█████████ | 10/11 [04:50<00:29, 29.07s/trial, best loss=?]                                                               Model used for fitting:
 91%|█████████ | 10/11 [04:55<00:29, 29.56s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 10/11 [04:55<00:29, 29.56s/trial, best loss=?]                                                               [23:48:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 10/11 [04:58<00:29, 29.81s/trial, best loss=?]                                                               predict called
 91%|█████████ | 10/11 [05:55<00:35, 35.55s/trial, best loss=?]                                                               Type of X:
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               Shape of X:
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               (289002, 139)
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               Type of y:
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               model fitted ?
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               True
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               y is not None
 91%|█████████ | 10/11 [06:00<00:36, 36.01s/trial, best loss=?]                                                               Model used for fitting:
 91%|█████████ | 10/11 [06:05<00:36, 36.52s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 10/11 [06:05<00:36, 36.52s/trial, best loss=?]                                                               [23:50:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 10/11 [06:07<00:36, 36.77s/trial, best loss=?]                                                               predict called
 91%|█████████ | 10/11 [07:05<00:42, 42.53s/trial, best loss=?]                                                               Type of X:
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               Shape of X:
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               (323071, 139)
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               Type of y:
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               model fitted ?
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               True
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]                                                               y is not None
 91%|█████████ | 10/11 [07:10<00:43, 43.05s/trial, best loss=?]100%|██████████| 11/11 [07:11<00:00, 431.53s/trial, best loss: -3056.1962269866444]100%|██████████| 11/11 [07:11<00:00, 39.23s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 7 since program start
2020-12-19 23:51:12.027399
Found saved Trials! Loading...
Rerunning from 11 trials to add another one.
 92%|█████████▏| 11/12 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 92%|█████████▏| 11/12 [00:00<00:00, 701.06trial/s, best loss=?]                                                                New call of hyperopt_train_test
 92%|█████████▏| 11/12 [00:00<00:00, 693.71trial/s, best loss=?]                                                                Model used for fitting:
 92%|█████████▏| 11/12 [01:31<00:08,  8.35s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 92%|█████████▏| 11/12 [01:31<00:08,  8.35s/trial, best loss=?]                                                               [23:52:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 11/12 [01:34<00:08,  8.59s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 11/12 [01:59<00:10, 10.85s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               (259530, 139)
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               True
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 11/12 [02:01<00:11, 11.04s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 11/12 [02:05<00:11, 11.45s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 11/12 [02:05<00:11, 11.45s/trial, best loss=?]                                                               [23:53:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 11/12 [02:08<00:11, 11.66s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 11/12 [02:31<00:13, 13.79s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               (271631, 139)
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               True
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 11/12 [02:33<00:13, 13.99s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 11/12 [02:38<00:14, 14.40s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 11/12 [02:38<00:14, 14.40s/trial, best loss=?]                                                               [23:53:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 11/12 [02:40<00:14, 14.61s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 11/12 [03:04<00:16, 16.76s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               (286101, 139)
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               True
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 11/12 [03:06<00:16, 16.98s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 11/12 [03:11<00:17, 17.41s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 11/12 [03:11<00:17, 17.41s/trial, best loss=?]                                                               [23:54:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 11/12 [03:13<00:17, 17.64s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 11/12 [03:37<00:19, 19.82s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               (289002, 139)
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               True
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 11/12 [03:40<00:20, 20.03s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 11/12 [03:45<00:20, 20.49s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 11/12 [03:45<00:20, 20.49s/trial, best loss=?]                                                               [23:54:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 11/12 [03:47<00:20, 20.71s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 11/12 [04:12<00:22, 22.93s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               (323071, 139)
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               True
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 11/12 [04:15<00:23, 23.20s/trial, best loss=?]100%|██████████| 12/12 [04:16<00:00, 256.28s/trial, best loss: -3056.1962269866444]100%|██████████| 12/12 [04:16<00:00, 21.36s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 8 since program start
2020-12-19 23:55:28.358796
Found saved Trials! Loading...
Rerunning from 12 trials to add another one.
 92%|█████████▏| 12/13 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 92%|█████████▏| 12/13 [00:00<00:00, 743.26trial/s, best loss=?]                                                                New call of hyperopt_train_test
 92%|█████████▏| 12/13 [00:00<00:00, 736.65trial/s, best loss=?]                                                                Model used for fitting:
 92%|█████████▏| 12/13 [01:31<00:07,  7.63s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 92%|█████████▏| 12/13 [01:31<00:07,  7.63s/trial, best loss=?]                                                               [23:57:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 12/13 [01:35<00:07,  7.93s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 12/13 [02:27<00:12, 12.30s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               (259530, 139)
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               True
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 12/13 [02:28<00:12, 12.38s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 12/13 [02:33<00:12, 12.75s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 12/13 [02:33<00:12, 12.75s/trial, best loss=?]                                                               [23:58:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 12/13 [02:35<00:12, 12.99s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 12/13 [03:07<00:15, 15.64s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               (271631, 139)
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               True
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 12/13 [03:08<00:15, 15.73s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 12/13 [03:13<00:16, 16.10s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 12/13 [03:13<00:16, 16.10s/trial, best loss=?]                                                               [23:58:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 12/13 [03:16<00:16, 16.35s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 12/13 [03:54<00:19, 19.51s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               (286101, 139)
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               True
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 12/13 [03:55<00:19, 19.60s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 12/13 [03:59<00:19, 20.00s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 12/13 [03:59<00:19, 20.00s/trial, best loss=?]                                                               [23:59:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 12/13 [04:03<00:20, 20.25s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 12/13 [04:43<00:23, 23.59s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               (289002, 139)
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               True
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 12/13 [04:44<00:23, 23.69s/trial, best loss=?]                                                               Model used for fitting:
 92%|█████████▏| 12/13 [04:49<00:24, 24.10s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 12/13 [04:49<00:24, 24.10s/trial, best loss=?]                                                               [00:00:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 12/13 [04:52<00:24, 24.36s/trial, best loss=?]                                                               predict called
 92%|█████████▏| 12/13 [05:26<00:27, 27.22s/trial, best loss=?]                                                               Type of X:
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               Shape of X:
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               (323071, 139)
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               Type of y:
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               model fitted ?
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               True
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]                                                               y is not None
 92%|█████████▏| 12/13 [05:28<00:27, 27.35s/trial, best loss=?]100%|██████████| 13/13 [05:29<00:00, 329.28s/trial, best loss: -3056.1962269866444]100%|██████████| 13/13 [05:29<00:00, 25.33s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 9 since program start
2020-12-20 00:00:57.736463
Found saved Trials! Loading...
Rerunning from 13 trials to add another one.
 93%|█████████▎| 13/14 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 93%|█████████▎| 13/14 [00:00<00:00, 820.78trial/s, best loss=?]                                                                New call of hyperopt_train_test
 93%|█████████▎| 13/14 [00:00<00:00, 812.84trial/s, best loss=?]                                                                Model used for fitting:
 93%|█████████▎| 13/14 [01:31<00:07,  7.02s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 93%|█████████▎| 13/14 [01:31<00:07,  7.02s/trial, best loss=?]                                                               [00:02:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 13/14 [01:34<00:07,  7.24s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 13/14 [03:57<00:18, 18.31s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               (259530, 139)
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               True
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 13/14 [04:05<00:18, 18.87s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 13/14 [04:09<00:19, 19.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 13/14 [04:09<00:19, 19.21s/trial, best loss=?]                                                               [00:05:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 13/14 [04:12<00:19, 19.40s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 13/14 [06:20<00:29, 29.26s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               (271631, 139)
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               True
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 13/14 [06:27<00:29, 29.82s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 13/14 [06:32<00:30, 30.17s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 13/14 [06:32<00:30, 30.17s/trial, best loss=?]                                                               [00:07:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 13/14 [06:34<00:30, 30.36s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 13/14 [08:45<00:40, 40.41s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               (286101, 139)
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               True
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 13/14 [08:53<00:41, 41.02s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 13/14 [08:58<00:41, 41.40s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 13/14 [08:58<00:41, 41.40s/trial, best loss=?]                                                               [00:09:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 13/14 [09:00<00:41, 41.60s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 13/14 [11:18<00:52, 52.21s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               (289002, 139)
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               True
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 13/14 [11:26<00:52, 52.83s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 13/14 [11:31<00:53, 53.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 13/14 [11:31<00:53, 53.21s/trial, best loss=?]                                                               [00:12:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 13/14 [11:34<00:53, 53.42s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 13/14 [13:55<01:04, 64.29s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               (323071, 139)
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               True
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 13/14 [14:05<01:05, 65.03s/trial, best loss=?]100%|██████████| 14/14 [14:06<00:00, 846.51s/trial, best loss: -3056.1962269866444]100%|██████████| 14/14 [14:06<00:00, 60.47s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 10 since program start
2020-12-20 00:15:04.299095
Found saved Trials! Loading...
Rerunning from 14 trials to add another one.
 93%|█████████▎| 14/15 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 93%|█████████▎| 14/15 [00:00<00:00, 877.64trial/s, best loss=?]                                                                New call of hyperopt_train_test
 93%|█████████▎| 14/15 [00:00<00:00, 869.81trial/s, best loss=?]                                                                Model used for fitting:
 93%|█████████▎| 14/15 [01:31<00:06,  6.54s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 93%|█████████▎| 14/15 [01:31<00:06,  6.54s/trial, best loss=?]                                                               [00:16:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 14/15 [01:34<00:06,  6.73s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 14/15 [01:35<00:06,  6.83s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               (259530, 139)
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               True
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 14/15 [01:36<00:06,  6.89s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 14/15 [01:41<00:07,  7.22s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 14/15 [01:41<00:07,  7.22s/trial, best loss=?]                                                               [00:16:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 14/15 [01:43<00:07,  7.39s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 14/15 [01:44<00:07,  7.48s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               (271631, 139)
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               True
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 14/15 [01:45<00:07,  7.55s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 14/15 [01:50<00:07,  7.88s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 14/15 [01:50<00:07,  7.88s/trial, best loss=?]                                                               [00:16:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 14/15 [01:52<00:08,  8.06s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 14/15 [01:54<00:08,  8.15s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               (286101, 139)
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               True
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 14/15 [01:55<00:08,  8.22s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 14/15 [02:00<00:08,  8.58s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 14/15 [02:00<00:08,  8.58s/trial, best loss=?]                                                               [00:17:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 14/15 [02:02<00:08,  8.76s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 14/15 [02:04<00:08,  8.86s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               (289002, 139)
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               True
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 14/15 [02:04<00:08,  8.93s/trial, best loss=?]                                                               Model used for fitting:
 93%|█████████▎| 14/15 [02:10<00:09,  9.31s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 14/15 [02:10<00:09,  9.31s/trial, best loss=?]                                                               [00:17:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 14/15 [02:12<00:09,  9.50s/trial, best loss=?]                                                               predict called
 93%|█████████▎| 14/15 [02:14<00:09,  9.60s/trial, best loss=?]                                                               Type of X:
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               Shape of X:
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               (323071, 139)
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               Type of y:
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               model fitted ?
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               True
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]                                                               y is not None
 93%|█████████▎| 14/15 [02:15<00:09,  9.69s/trial, best loss=?]100%|██████████| 15/15 [02:16<00:00, 136.87s/trial, best loss: -3056.1962269866444]100%|██████████| 15/15 [02:16<00:00,  9.12s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 11 since program start
2020-12-20 00:17:21.215916
Found saved Trials! Loading...
Rerunning from 15 trials to add another one.
 94%|█████████▍| 15/16 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 94%|█████████▍| 15/16 [00:00<00:00, 193.31trial/s, best loss=?]                                                                New call of hyperopt_train_test
 94%|█████████▍| 15/16 [00:00<00:00, 192.85trial/s, best loss=?]                                                                Model used for fitting:
 94%|█████████▍| 15/16 [01:30<00:06,  6.04s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 94%|█████████▍| 15/16 [01:30<00:06,  6.04s/trial, best loss=?]                                                               [00:18:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 15/16 [01:33<00:06,  6.23s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 15/16 [01:36<00:06,  6.44s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               (259530, 139)
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               True
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 15/16 [01:37<00:06,  6.48s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 15/16 [01:41<00:06,  6.79s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 15/16 [01:41<00:06,  6.79s/trial, best loss=?]                                                               [00:19:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 15/16 [01:44<00:06,  6.95s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 15/16 [01:46<00:07,  7.12s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               (271631, 139)
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               True
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 15/16 [01:47<00:07,  7.18s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 15/16 [01:52<00:07,  7.49s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 15/16 [01:52<00:07,  7.49s/trial, best loss=?]                                                               [00:19:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 15/16 [01:54<00:07,  7.65s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 15/16 [01:57<00:07,  7.83s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               (286101, 139)
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               True
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 15/16 [01:58<00:07,  7.90s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 15/16 [02:03<00:08,  8.23s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 15/16 [02:03<00:08,  8.23s/trial, best loss=?]                                                               [00:19:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 15/16 [02:06<00:08,  8.41s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 15/16 [02:08<00:08,  8.59s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               (289002, 139)
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               True
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 15/16 [02:09<00:08,  8.66s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 15/16 [02:15<00:09,  9.02s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 15/16 [02:15<00:09,  9.02s/trial, best loss=?]                                                               [00:19:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 15/16 [02:17<00:09,  9.19s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 15/16 [02:20<00:09,  9.38s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               (323071, 139)
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               True
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 15/16 [02:22<00:09,  9.47s/trial, best loss=?]100%|██████████| 16/16 [02:23<00:00, 143.15s/trial, best loss: -3056.1962269866444]100%|██████████| 16/16 [02:23<00:00,  8.95s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 12 since program start
2020-12-20 00:19:44.416620
Found saved Trials! Loading...
Rerunning from 16 trials to add another one.
 94%|█████████▍| 16/17 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 94%|█████████▍| 16/17 [00:00<00:00, 990.61trial/s, best loss=?]                                                                New call of hyperopt_train_test
 94%|█████████▍| 16/17 [00:00<00:00, 981.87trial/s, best loss=?]START OF OPTIMIZATION:
Run 0 since program start
2020-12-20 00:32:52.544606
Found saved Trials! Loading...
Rerunning from 16 trials to add another one.
 94%|█████████▍| 16/17 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 94%|█████████▍| 16/17 [00:00<00:00, 981.74trial/s, best loss=?]                                                                New call of hyperopt_train_test
 94%|█████████▍| 16/17 [00:00<00:00, 972.45trial/s, best loss=?]                                                                Model used for fitting:
 94%|█████████▍| 16/17 [01:27<00:05,  5.46s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 94%|█████████▍| 16/17 [01:27<00:05,  5.46s/trial, best loss=?]                                                               [00:34:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 16/17 [01:31<00:05,  5.69s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 16/17 [09:26<00:35, 35.39s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               (259530, 139)
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               True
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 16/17 [09:41<00:36, 36.35s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 16/17 [09:45<00:36, 36.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 16/17 [09:45<00:36, 36.61s/trial, best loss=?]                                                               [00:42:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 16/17 [09:47<00:36, 36.73s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 16/17 [16:51<01:03, 63.19s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               (271631, 139)
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               True
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 16/17 [17:06<01:04, 64.14s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 16/17 [17:10<01:04, 64.39s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 16/17 [17:10<01:04, 64.39s/trial, best loss=?]                                                               [00:50:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 16/17 [17:12<01:04, 64.50s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 16/17 [24:20<01:31, 91.27s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               (286101, 139)
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               True
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 16/17 [24:36<01:32, 92.27s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 16/17 [24:40<01:32, 92.54s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 16/17 [24:40<01:32, 92.54s/trial, best loss=?]                                                               [00:57:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 16/17 [24:42<01:32, 92.67s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 16/17 [32:13<02:00, 120.82s/trial, best loss=?]                                                                Type of X:
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                Shape of X:
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                (289002, 139)
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                Type of y:
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                <class 'numpy.ndarray'>
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                model fitted ?
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                True
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                y is not None
 94%|█████████▍| 16/17 [32:29<02:01, 121.85s/trial, best loss=?]                                                                Model used for fitting:
 94%|█████████▍| 16/17 [32:33<02:02, 122.12s/trial, best loss=?]                                                                XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 16/17 [32:33<02:02, 122.12s/trial, best loss=?]                                                                [01:05:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 16/17 [32:36<02:02, 122.26s/trial, best loss=?]                                                                predict called
 94%|█████████▍| 16/17 [40:21<02:31, 151.32s/trial, best loss=?]                                                                Type of X:
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                Shape of X:
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                (323071, 139)
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                Type of y:
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                <class 'numpy.ndarray'>
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                model fitted ?
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                True
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]                                                                y is not None
 94%|█████████▍| 16/17 [40:39<02:32, 152.48s/trial, best loss=?]100%|██████████| 17/17 [40:40<00:00, 2440.30s/trial, best loss: -3056.1962269866444]100%|██████████| 17/17 [40:40<00:00, 143.55s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 1 since program start
2020-12-20 01:13:33.081754
Found saved Trials! Loading...
Rerunning from 17 trials to add another one.
 94%|█████████▍| 17/18 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 94%|█████████▍| 17/18 [00:00<00:00, 1043.68trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 94%|█████████▍| 17/18 [00:00<00:00, 1033.66trial/s, best loss=?]                                                                 Model used for fitting:
 94%|█████████▍| 17/18 [01:25<00:05,  5.06s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 94%|█████████▍| 17/18 [01:25<00:05,  5.06s/trial, best loss=?]                                                               [01:15:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 17/18 [01:27<00:05,  5.16s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 17/18 [02:13<00:07,  7.87s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               (259530, 139)
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               True
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 17/18 [02:16<00:08,  8.03s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 17/18 [02:20<00:08,  8.26s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 17/18 [02:20<00:08,  8.26s/trial, best loss=?]                                                               [01:15:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 17/18 [02:21<00:08,  8.35s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 17/18 [03:05<00:10, 10.88s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               (271631, 139)
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               True
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 17/18 [03:07<00:11, 11.05s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 17/18 [03:11<00:11, 11.28s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 17/18 [03:11<00:11, 11.28s/trial, best loss=?]                                                               [01:16:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 17/18 [03:13<00:11, 11.37s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 17/18 [03:56<00:13, 13.93s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               (286101, 139)
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               True
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 17/18 [03:59<00:14, 14.10s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 17/18 [04:04<00:14, 14.36s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 17/18 [04:04<00:14, 14.36s/trial, best loss=?]                                                               [01:17:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 17/18 [04:05<00:14, 14.45s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 17/18 [04:51<00:17, 17.12s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               (289002, 139)
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               True
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 17/18 [04:54<00:17, 17.30s/trial, best loss=?]                                                               Model used for fitting:
 94%|█████████▍| 17/18 [04:58<00:17, 17.57s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 17/18 [04:58<00:17, 17.57s/trial, best loss=?]                                                               [01:18:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 17/18 [05:00<00:17, 17.67s/trial, best loss=?]                                                               predict called
 94%|█████████▍| 17/18 [05:45<00:20, 20.34s/trial, best loss=?]                                                               Type of X:
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               Shape of X:
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               (323071, 139)
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               Type of y:
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               model fitted ?
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               True
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]                                                               y is not None
 94%|█████████▍| 17/18 [05:49<00:20, 20.54s/trial, best loss=?]100%|██████████| 18/18 [05:49<00:00, 349.88s/trial, best loss: -3056.1962269866444]100%|██████████| 18/18 [05:49<00:00, 19.44s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 2 since program start
2020-12-20 01:19:23.015767
Found saved Trials! Loading...
Rerunning from 18 trials to add another one.
 95%|█████████▍| 18/19 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 95%|█████████▍| 18/19 [00:00<00:00, 1104.05trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 95%|█████████▍| 18/19 [00:00<00:00, 1093.34trial/s, best loss=?]                                                                 Model used for fitting:
 95%|█████████▍| 18/19 [01:25<00:04,  4.73s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 95%|█████████▍| 18/19 [01:25<00:04,  4.73s/trial, best loss=?]                                                               [01:20:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 18/19 [01:27<00:04,  4.84s/trial, best loss=?]                                                               predict called
 95%|█████████▍| 18/19 [03:55<00:13, 13.08s/trial, best loss=?]                                                               Type of X:
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               (259530, 139)
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               Type of y:
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               True
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               y is not None
 95%|█████████▍| 18/19 [04:02<00:13, 13.48s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▍| 18/19 [04:06<00:13, 13.70s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 18/19 [04:06<00:13, 13.70s/trial, best loss=?]                                                               [01:23:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 18/19 [04:08<00:13, 13.79s/trial, best loss=?]                                                               predict called
 95%|█████████▍| 18/19 [06:20<00:21, 21.15s/trial, best loss=?]                                                               Type of X:
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               (271631, 139)
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               Type of y:
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               True
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               y is not None
 95%|█████████▍| 18/19 [06:28<00:21, 21.57s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▍| 18/19 [06:32<00:21, 21.79s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 18/19 [06:32<00:21, 21.79s/trial, best loss=?]                                                               [01:25:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 18/19 [06:33<00:21, 21.88s/trial, best loss=?]                                                               predict called
 95%|█████████▍| 18/19 [08:47<00:29, 29.33s/trial, best loss=?]                                                               Type of X:
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               (286101, 139)
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               Type of y:
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               True
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               y is not None
 95%|█████████▍| 18/19 [08:55<00:29, 29.77s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▍| 18/19 [09:00<00:30, 30.00s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 18/19 [09:00<00:30, 30.00s/trial, best loss=?]                                                               [01:28:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 18/19 [09:01<00:30, 30.10s/trial, best loss=?]                                                               predict called
 95%|█████████▍| 18/19 [11:23<00:37, 37.98s/trial, best loss=?]                                                               Type of X:
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               (289002, 139)
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               Type of y:
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               True
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               y is not None
 95%|█████████▍| 18/19 [11:31<00:38, 38.42s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▍| 18/19 [11:36<00:38, 38.67s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 18/19 [11:36<00:38, 38.67s/trial, best loss=?]                                                               [01:31:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 18/19 [11:37<00:38, 38.77s/trial, best loss=?]                                                               predict called
 95%|█████████▍| 18/19 [14:03<00:46, 46.85s/trial, best loss=?]                                                               Type of X:
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               (323071, 139)
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               Type of y:
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               True
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]                                                               y is not None
 95%|█████████▍| 18/19 [14:12<00:47, 47.35s/trial, best loss=?]100%|██████████| 19/19 [14:12<00:00, 852.89s/trial, best loss: -3056.1962269866444]100%|██████████| 19/19 [14:12<00:00, 44.89s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 3 since program start
2020-12-20 01:33:36.015904
Found saved Trials! Loading...
Rerunning from 19 trials to add another one.
 95%|█████████▌| 19/20 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 95%|█████████▌| 19/20 [00:00<00:00, 1205.90trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 95%|█████████▌| 19/20 [00:00<00:00, 1193.92trial/s, best loss=?]                                                                 Model used for fitting:
 95%|█████████▌| 19/20 [01:26<00:04,  4.55s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 95%|█████████▌| 19/20 [01:26<00:04,  4.55s/trial, best loss=?]                                                               [01:35:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 19/20 [01:28<00:04,  4.66s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 19/20 [04:18<00:13, 13.62s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               (259530, 139)
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               True
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 19/20 [04:26<00:14, 14.03s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 19/20 [04:30<00:14, 14.24s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 19/20 [04:30<00:14, 14.24s/trial, best loss=?]                                                               [01:38:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 19/20 [04:32<00:14, 14.32s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 19/20 [07:04<00:22, 22.35s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               (271631, 139)
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               True
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 19/20 [07:12<00:22, 22.76s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 19/20 [07:16<00:22, 22.97s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 19/20 [07:16<00:22, 22.97s/trial, best loss=?]                                                               [01:40:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 19/20 [07:18<00:23, 23.06s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 19/20 [09:52<00:31, 31.20s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               (286101, 139)
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               True
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 19/20 [10:01<00:31, 31.63s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 19/20 [10:05<00:31, 31.86s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 19/20 [10:05<00:31, 31.86s/trial, best loss=?]                                                               [01:43:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 19/20 [10:07<00:31, 31.95s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 19/20 [12:50<00:40, 40.54s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               (289002, 139)
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               True
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 19/20 [12:58<00:40, 40.99s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 19/20 [13:03<00:41, 41.22s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 19/20 [13:03<00:41, 41.22s/trial, best loss=?]                                                               [01:46:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 19/20 [13:05<00:41, 41.32s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 19/20 [15:52<00:50, 50.12s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               (323071, 139)
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               True
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 19/20 [16:01<00:50, 50.62s/trial, best loss=?]100%|██████████| 20/20 [16:02<00:00, 962.47s/trial, best loss: -3056.1962269866444]100%|██████████| 20/20 [16:02<00:00, 48.12s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 4 since program start
2020-12-20 01:49:38.539570
Found saved Trials! Loading...
Rerunning from 20 trials to add another one.
 95%|█████████▌| 20/21 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 95%|█████████▌| 20/21 [00:00<00:00, 981.58trial/s, best loss=?]                                                                New call of hyperopt_train_test
 95%|█████████▌| 20/21 [00:00<00:00, 974.25trial/s, best loss=?]                                                                Model used for fitting:
 95%|█████████▌| 20/21 [01:25<00:04,  4.26s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 95%|█████████▌| 20/21 [01:25<00:04,  4.26s/trial, best loss=?]                                                               [01:51:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 20/21 [01:26<00:04,  4.35s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 20/21 [01:32<00:04,  4.64s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               (259530, 139)
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               True
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 20/21 [01:33<00:04,  4.70s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 20/21 [01:37<00:04,  4.89s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 20/21 [01:37<00:04,  4.89s/trial, best loss=?]                                                               [01:51:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 20/21 [01:39<00:04,  4.97s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 20/21 [01:44<00:05,  5.23s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               (271631, 139)
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               True
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 20/21 [01:45<00:05,  5.29s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 20/21 [01:49<00:05,  5.49s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 20/21 [01:49<00:05,  5.49s/trial, best loss=?]                                                               [01:51:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 20/21 [01:51<00:05,  5.56s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 20/21 [01:56<00:05,  5.83s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               (286101, 139)
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               True
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 20/21 [01:57<00:05,  5.89s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 20/21 [02:02<00:06,  6.10s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 20/21 [02:02<00:06,  6.10s/trial, best loss=?]                                                               [01:51:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 20/21 [02:03<00:06,  6.18s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 20/21 [02:09<00:06,  6.46s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               (289002, 139)
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               True
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 20/21 [02:10<00:06,  6.53s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 20/21 [02:14<00:06,  6.75s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 20/21 [02:14<00:06,  6.75s/trial, best loss=?]                                                               [01:51:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 20/21 [02:16<00:06,  6.83s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 20/21 [02:22<00:07,  7.12s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               (323071, 139)
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               True
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 20/21 [02:23<00:07,  7.19s/trial, best loss=?]100%|██████████| 21/21 [02:24<00:00, 144.40s/trial, best loss: -3056.1962269866444]100%|██████████| 21/21 [02:24<00:00,  6.88s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 5 since program start
2020-12-20 01:52:02.989037
Found saved Trials! Loading...
Rerunning from 21 trials to add another one.
 95%|█████████▌| 21/22 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 95%|█████████▌| 21/22 [00:00<00:00, 1025.53trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 95%|█████████▌| 21/22 [00:00<00:00, 1017.86trial/s, best loss=?]                                                                 Model used for fitting:
 95%|█████████▌| 21/22 [01:25<00:04,  4.06s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 95%|█████████▌| 21/22 [01:25<00:04,  4.06s/trial, best loss=?]                                                               [01:53:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 21/22 [01:26<00:04,  4.14s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 21/22 [01:33<00:04,  4.46s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               (259530, 139)
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               True
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 21/22 [01:34<00:04,  4.51s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 21/22 [01:38<00:04,  4.70s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 21/22 [01:38<00:04,  4.70s/trial, best loss=?]                                                               [01:53:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 21/22 [01:40<00:04,  4.77s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 21/22 [01:46<00:05,  5.05s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               (271631, 139)
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               True
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 21/22 [01:47<00:05,  5.11s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 21/22 [01:51<00:05,  5.30s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 21/22 [01:51<00:05,  5.30s/trial, best loss=?]                                                               [01:53:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 21/22 [01:52<00:05,  5.36s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 21/22 [01:58<00:05,  5.65s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               (286101, 139)
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               True
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 21/22 [02:00<00:05,  5.72s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 21/22 [02:04<00:05,  5.92s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 21/22 [02:04<00:05,  5.92s/trial, best loss=?]                                                               [01:54:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 21/22 [02:05<00:05,  5.99s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 21/22 [02:12<00:06,  6.29s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               (289002, 139)
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               True
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 21/22 [02:13<00:06,  6.35s/trial, best loss=?]                                                               Model used for fitting:
 95%|█████████▌| 21/22 [02:17<00:06,  6.56s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▌| 21/22 [02:17<00:06,  6.56s/trial, best loss=?]                                                               [01:54:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▌| 21/22 [02:19<00:06,  6.64s/trial, best loss=?]                                                               predict called
 95%|█████████▌| 21/22 [02:25<00:06,  6.95s/trial, best loss=?]                                                               Type of X:
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               Shape of X:
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               (323071, 139)
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               Type of y:
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               model fitted ?
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               True
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]                                                               y is not None
 95%|█████████▌| 21/22 [02:27<00:07,  7.02s/trial, best loss=?]100%|██████████| 22/22 [02:27<00:00, 147.94s/trial, best loss: -3056.1962269866444]100%|██████████| 22/22 [02:27<00:00,  6.72s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 6 since program start
2020-12-20 01:54:31.033069
Found saved Trials! Loading...
Rerunning from 22 trials to add another one.
 96%|█████████▌| 22/23 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 96%|█████████▌| 22/23 [00:00<00:00, 1065.65trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 96%|█████████▌| 22/23 [00:00<00:00, 1057.65trial/s, best loss=?]                                                                 Model used for fitting:
 96%|█████████▌| 22/23 [01:27<00:03,  3.96s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▌| 22/23 [01:27<00:03,  3.96s/trial, best loss=?]                                                               [01:55:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 22/23 [01:28<00:04,  4.04s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 22/23 [01:35<00:04,  4.33s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               (259530, 139)
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               True
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 22/23 [01:36<00:04,  4.38s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 22/23 [01:40<00:04,  4.56s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 22/23 [01:40<00:04,  4.56s/trial, best loss=?]                                                               [01:56:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 22/23 [01:41<00:04,  4.62s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 22/23 [01:47<00:04,  4.89s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               (271631, 139)
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               True
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 22/23 [01:48<00:04,  4.94s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 22/23 [01:52<00:05,  5.12s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 22/23 [01:52<00:05,  5.12s/trial, best loss=?]                                                               [01:56:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 22/23 [01:54<00:05,  5.19s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 22/23 [02:00<00:05,  5.45s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               (286101, 139)
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               True
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 22/23 [02:01<00:05,  5.51s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 22/23 [02:05<00:05,  5.70s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 22/23 [02:05<00:05,  5.70s/trial, best loss=?]                                                               [01:56:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 22/23 [02:07<00:05,  5.77s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 22/23 [02:13<00:06,  6.05s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               (289002, 139)
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               True
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 22/23 [02:14<00:06,  6.11s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 22/23 [02:18<00:06,  6.31s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 22/23 [02:18<00:06,  6.31s/trial, best loss=?]                                                               [01:56:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 22/23 [02:20<00:06,  6.39s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 22/23 [02:26<00:06,  6.67s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               (323071, 139)
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               True
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 22/23 [02:28<00:06,  6.74s/trial, best loss=?]100%|██████████| 23/23 [02:28<00:00, 148.84s/trial, best loss: -3056.1962269866444]100%|██████████| 23/23 [02:28<00:00,  6.47s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 7 since program start
2020-12-20 01:56:59.922613
Found saved Trials! Loading...
Rerunning from 23 trials to add another one.
 96%|█████████▌| 23/24 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 96%|█████████▌| 23/24 [00:00<00:00, 1136.94trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 96%|█████████▌| 23/24 [00:00<00:00, 1128.38trial/s, best loss=?]                                                                 Model used for fitting:
 96%|█████████▌| 23/24 [01:25<00:03,  3.73s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▌| 23/24 [01:25<00:03,  3.73s/trial, best loss=?]                                                               [01:58:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 23/24 [01:27<00:03,  3.81s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 23/24 [01:33<00:04,  4.07s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               (259530, 139)
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               True
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 23/24 [01:34<00:04,  4.11s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 23/24 [01:38<00:04,  4.29s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 23/24 [01:38<00:04,  4.29s/trial, best loss=?]                                                               [01:58:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 23/24 [01:40<00:04,  4.35s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 23/24 [01:45<00:04,  4.58s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               (271631, 139)
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               True
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 23/24 [01:46<00:04,  4.64s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 23/24 [01:50<00:04,  4.81s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 23/24 [01:50<00:04,  4.81s/trial, best loss=?]                                                               [01:58:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 23/24 [01:52<00:04,  4.87s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 23/24 [01:57<00:05,  5.11s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               (286101, 139)
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               True
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 23/24 [01:58<00:05,  5.17s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 23/24 [02:03<00:05,  5.35s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 23/24 [02:03<00:05,  5.35s/trial, best loss=?]                                                               [01:59:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 23/24 [02:04<00:05,  5.42s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 23/24 [02:10<00:05,  5.67s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               (289002, 139)
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               True
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 23/24 [02:11<00:05,  5.72s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 23/24 [02:16<00:05,  5.91s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 23/24 [02:16<00:05,  5.91s/trial, best loss=?]                                                               [01:59:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 23/24 [02:17<00:05,  5.98s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 23/24 [02:23<00:06,  6.24s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               (323071, 139)
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               True
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 23/24 [02:24<00:06,  6.30s/trial, best loss=?]100%|██████████| 24/24 [02:25<00:00, 145.50s/trial, best loss: -3056.1962269866444]100%|██████████| 24/24 [02:25<00:00,  6.06s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 8 since program start
2020-12-20 01:59:25.474099
Found saved Trials! Loading...
Rerunning from 24 trials to add another one.
 96%|█████████▌| 24/25 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 96%|█████████▌| 24/25 [00:00<00:00, 292.87trial/s, best loss=?]                                                                New call of hyperopt_train_test
 96%|█████████▌| 24/25 [00:00<00:00, 292.21trial/s, best loss=?]                                                                Model used for fitting:
 96%|█████████▌| 24/25 [01:25<00:03,  3.56s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▌| 24/25 [01:25<00:03,  3.56s/trial, best loss=?]                                                               [02:00:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 24/25 [01:27<00:03,  3.64s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 24/25 [01:54<00:04,  4.78s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               (259530, 139)
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               True
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 24/25 [01:56<00:04,  4.84s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 24/25 [02:00<00:05,  5.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 24/25 [02:00<00:05,  5.01s/trial, best loss=?]                                                               [02:01:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 24/25 [02:01<00:05,  5.07s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 24/25 [02:27<00:06,  6.14s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               (271631, 139)
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               True
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 24/25 [02:29<00:06,  6.21s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 24/25 [02:33<00:06,  6.38s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 24/25 [02:33<00:06,  6.38s/trial, best loss=?]                                                               [02:02:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 24/25 [02:34<00:06,  6.44s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 24/25 [03:00<00:07,  7.51s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               (286101, 139)
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               True
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 24/25 [03:02<00:07,  7.60s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 24/25 [03:06<00:07,  7.77s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 24/25 [03:06<00:07,  7.77s/trial, best loss=?]                                                               [02:02:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 24/25 [03:08<00:07,  7.84s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 24/25 [03:34<00:08,  8.95s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               (289002, 139)
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               True
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 24/25 [03:36<00:09,  9.02s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 24/25 [03:41<00:09,  9.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 24/25 [03:41<00:09,  9.21s/trial, best loss=?]                                                               [02:03:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 24/25 [03:42<00:09,  9.28s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 24/25 [04:09<00:10, 10.38s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               (323071, 139)
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               True
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 24/25 [04:11<00:10, 10.46s/trial, best loss=?]100%|██████████| 25/25 [04:11<00:00, 251.76s/trial, best loss: -3056.1962269866444]100%|██████████| 25/25 [04:11<00:00, 10.07s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 9 since program start
2020-12-20 02:03:37.284931
Found saved Trials! Loading...
Rerunning from 25 trials to add another one.
 96%|█████████▌| 25/26 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 96%|█████████▌| 25/26 [00:00<00:00, 1234.46trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 96%|█████████▌| 25/26 [00:00<00:00, 1224.89trial/s, best loss=?]                                                                 Model used for fitting:
 96%|█████████▌| 25/26 [01:25<00:03,  3.41s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▌| 25/26 [01:25<00:03,  3.41s/trial, best loss=?]                                                               [02:05:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 25/26 [01:27<00:03,  3.48s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 25/26 [01:28<00:03,  3.54s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               (259530, 139)
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               True
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 25/26 [01:29<00:03,  3.57s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 25/26 [01:33<00:03,  3.73s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 25/26 [01:33<00:03,  3.73s/trial, best loss=?]                                                               [02:05:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 25/26 [01:34<00:03,  3.79s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 25/26 [01:35<00:03,  3.84s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               (271631, 139)
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               True
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 25/26 [01:36<00:03,  3.87s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 25/26 [01:40<00:04,  4.03s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 25/26 [01:40<00:04,  4.03s/trial, best loss=?]                                                               [02:05:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 25/26 [01:42<00:04,  4.08s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 25/26 [01:43<00:04,  4.14s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               (286101, 139)
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               True
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 25/26 [01:44<00:04,  4.17s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 25/26 [01:48<00:04,  4.34s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 25/26 [01:48<00:04,  4.34s/trial, best loss=?]                                                               [02:05:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 25/26 [01:50<00:04,  4.40s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 25/26 [01:51<00:04,  4.46s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               (289002, 139)
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               True
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 25/26 [01:52<00:04,  4.48s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▌| 25/26 [01:56<00:04,  4.66s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 25/26 [01:56<00:04,  4.66s/trial, best loss=?]                                                               [02:05:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 25/26 [01:58<00:04,  4.73s/trial, best loss=?]                                                               predict called
 96%|█████████▌| 25/26 [01:59<00:04,  4.78s/trial, best loss=?]                                                               Type of X:
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               (323071, 139)
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               Type of y:
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               True
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]                                                               y is not None
 96%|█████████▌| 25/26 [02:00<00:04,  4.82s/trial, best loss=?]100%|██████████| 26/26 [02:01<00:00, 121.02s/trial, best loss: -3056.1962269866444]100%|██████████| 26/26 [02:01<00:00,  4.65s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 10 since program start
2020-12-20 02:05:38.359336
Found saved Trials! Loading...
Rerunning from 26 trials to add another one.
 96%|█████████▋| 26/27 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 96%|█████████▋| 26/27 [00:00<00:00, 1236.61trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 96%|█████████▋| 26/27 [00:00<00:00, 1226.43trial/s, best loss=?]                                                                 Model used for fitting:
 96%|█████████▋| 26/27 [01:26<00:03,  3.34s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▋| 26/27 [01:26<00:03,  3.34s/trial, best loss=?]                                                               [02:07:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 26/27 [01:28<00:03,  3.40s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               (259530, 139)
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               True
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 26/27 [01:35<00:03,  3.68s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 26/27 [01:39<00:03,  3.84s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 26/27 [01:39<00:03,  3.84s/trial, best loss=?]                                                               [02:07:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 26/27 [01:41<00:03,  3.89s/trial, best loss=?]START OF OPTIMIZATION:
Run 0 since program start
2020-12-20 02:22:28.345077
Found saved Trials! Loading...
Rerunning from 26 trials to add another one.
 96%|█████████▋| 26/27 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 96%|█████████▋| 26/27 [00:00<00:00, 1251.24trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 96%|█████████▋| 26/27 [00:00<00:00, 1238.14trial/s, best loss=?]                                                                 Model used for fitting:
 96%|█████████▋| 26/27 [01:24<00:03,  3.25s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▋| 26/27 [01:24<00:03,  3.25s/trial, best loss=?]                                                               [02:23:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 26/27 [01:27<00:03,  3.36s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 26/27 [01:33<00:03,  3.59s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               (259530, 139)
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               True
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 26/27 [01:34<00:03,  3.64s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 26/27 [01:39<00:03,  3.81s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 26/27 [01:39<00:03,  3.81s/trial, best loss=?]                                                               [02:24:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 26/27 [01:41<00:03,  3.90s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 26/27 [01:46<00:04,  4.10s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               (271631, 139)
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               True
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 26/27 [01:48<00:04,  4.16s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 26/27 [01:52<00:04,  4.33s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 26/27 [01:52<00:04,  4.33s/trial, best loss=?]                                                               [02:24:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 26/27 [01:54<00:04,  4.42s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 26/27 [02:00<00:04,  4.63s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               (286101, 139)
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               True
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 26/27 [02:01<00:04,  4.68s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 26/27 [02:06<00:04,  4.87s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 26/27 [02:06<00:04,  4.87s/trial, best loss=?]                                                               [02:24:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 26/27 [02:09<00:04,  4.96s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 26/27 [02:14<00:05,  5.18s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               (289002, 139)
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               True
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 26/27 [02:16<00:05,  5.24s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 26/27 [02:21<00:05,  5.44s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 26/27 [02:21<00:05,  5.44s/trial, best loss=?]                                                               [02:24:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 26/27 [02:23<00:05,  5.53s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 26/27 [02:29<00:05,  5.75s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               (323071, 139)
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               True
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 26/27 [02:31<00:05,  5.82s/trial, best loss=?]100%|██████████| 27/27 [02:32<00:00, 152.27s/trial, best loss: -3056.1962269866444]100%|██████████| 27/27 [02:32<00:00,  5.64s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 1 since program start
2020-12-20 02:25:00.834765
Found saved Trials! Loading...
Rerunning from 27 trials to add another one.
 96%|█████████▋| 27/28 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 96%|█████████▋| 27/28 [00:00<00:00, 1271.59trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 96%|█████████▋| 27/28 [00:00<00:00, 1260.78trial/s, best loss=?]                                                                 Model used for fitting:
 96%|█████████▋| 27/28 [01:24<00:03,  3.13s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▋| 27/28 [01:24<00:03,  3.13s/trial, best loss=?]                                                               [02:26:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 27/28 [01:27<00:03,  3.23s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 27/28 [01:33<00:03,  3.44s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               (259530, 139)
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               True
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 27/28 [01:34<00:03,  3.49s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 27/28 [01:38<00:03,  3.66s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 27/28 [01:38<00:03,  3.66s/trial, best loss=?]                                                               [02:26:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 27/28 [01:41<00:03,  3.75s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 27/28 [01:46<00:03,  3.94s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               (271631, 139)
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               True
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 27/28 [01:47<00:03,  4.00s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 27/28 [01:52<00:04,  4.16s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 27/28 [01:52<00:04,  4.16s/trial, best loss=?]                                                               [02:26:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 27/28 [01:54<00:04,  4.25s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 27/28 [02:00<00:04,  4.45s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               (286101, 139)
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               True
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 27/28 [02:01<00:04,  4.50s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 27/28 [02:06<00:04,  4.68s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 27/28 [02:06<00:04,  4.68s/trial, best loss=?]                                                               [02:27:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 27/28 [02:08<00:04,  4.77s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 27/28 [02:14<00:04,  4.98s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               (289002, 139)
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               True
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 27/28 [02:15<00:05,  5.03s/trial, best loss=?]                                                               Model used for fitting:
 96%|█████████▋| 27/28 [02:21<00:05,  5.22s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▋| 27/28 [02:21<00:05,  5.22s/trial, best loss=?]                                                               [02:27:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▋| 27/28 [02:23<00:05,  5.32s/trial, best loss=?]                                                               predict called
 96%|█████████▋| 27/28 [02:29<00:05,  5.53s/trial, best loss=?]                                                               Type of X:
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               Shape of X:
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               (323071, 139)
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               Type of y:
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               model fitted ?
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               True
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]                                                               y is not None
 96%|█████████▋| 27/28 [02:31<00:05,  5.60s/trial, best loss=?]100%|██████████| 28/28 [02:32<00:00, 152.07s/trial, best loss: -3056.1962269866444]100%|██████████| 28/28 [02:32<00:00,  5.43s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 2 since program start
2020-12-20 02:27:32.962902
Found saved Trials! Loading...
Rerunning from 28 trials to add another one.
 97%|█████████▋| 28/29 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 28/29 [00:00<00:00, 1343.31trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 28/29 [00:00<00:00, 1331.56trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 28/29 [01:25<00:03,  3.04s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 28/29 [01:25<00:03,  3.04s/trial, best loss=?]                                                               [02:29:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 28/29 [01:27<00:03,  3.14s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 28/29 [01:32<00:03,  3.31s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               True
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 28/29 [01:33<00:03,  3.34s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 28/29 [01:38<00:03,  3.51s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 28/29 [01:38<00:03,  3.51s/trial, best loss=?]                                                               [02:29:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 28/29 [01:40<00:03,  3.59s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 28/29 [01:44<00:03,  3.74s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               True
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 28/29 [01:45<00:03,  3.78s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 28/29 [01:50<00:03,  3.95s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 28/29 [01:50<00:03,  3.95s/trial, best loss=?]                                                               [02:29:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 28/29 [01:52<00:04,  4.03s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 28/29 [01:57<00:04,  4.19s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               True
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 28/29 [01:58<00:04,  4.23s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 28/29 [02:03<00:04,  4.40s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 28/29 [02:03<00:04,  4.40s/trial, best loss=?]                                                               [02:29:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 28/29 [02:05<00:04,  4.49s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 28/29 [02:10<00:04,  4.65s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               True
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 28/29 [02:11<00:04,  4.69s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 28/29 [02:16<00:04,  4.87s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 28/29 [02:16<00:04,  4.87s/trial, best loss=?]                                                               [02:29:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 28/29 [02:18<00:04,  4.96s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 28/29 [02:23<00:05,  5.13s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               True
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 28/29 [02:24<00:05,  5.17s/trial, best loss=?]100%|██████████| 29/29 [02:25<00:00, 145.77s/trial, best loss: -3056.1962269866444]100%|██████████| 29/29 [02:25<00:00,  5.03s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 3 since program start
2020-12-20 02:29:58.834660
Found saved Trials! Loading...
Rerunning from 29 trials to add another one.
 97%|█████████▋| 29/30 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 29/30 [00:00<00:00, 1403.98trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 29/30 [00:00<00:00, 1389.57trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 29/30 [01:25<00:02,  2.93s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 29/30 [01:25<00:02,  2.93s/trial, best loss=?]                                                               [02:31:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 29/30 [01:29<00:03,  3.07s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 29/30 [06:10<00:12, 12.78s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               True
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 29/30 [06:14<00:12, 12.90s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 29/30 [06:18<00:13, 13.05s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 29/30 [06:18<00:13, 13.05s/trial, best loss=?]                                                               [02:36:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 29/30 [06:21<00:13, 13.15s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 29/30 [10:24<00:21, 21.54s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               True
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 29/30 [10:28<00:21, 21.67s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 29/30 [10:32<00:21, 21.81s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 29/30 [10:32<00:21, 21.81s/trial, best loss=?]                                                               [02:40:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 29/30 [10:35<00:21, 21.92s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 29/30 [15:01<00:31, 31.07s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               True
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 29/30 [15:05<00:31, 31.21s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 29/30 [15:09<00:31, 31.36s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 29/30 [15:09<00:31, 31.36s/trial, best loss=?]                                                               [02:45:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 29/30 [15:13<00:31, 31.49s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 29/30 [19:44<00:40, 40.83s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               True
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 29/30 [19:48<00:40, 40.98s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 29/30 [19:52<00:41, 41.14s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 29/30 [19:52<00:41, 41.14s/trial, best loss=?]                                                               [02:49:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 29/30 [19:56<00:41, 41.25s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 29/30 [24:21<00:50, 50.41s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               True
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 29/30 [24:26<00:50, 50.57s/trial, best loss=?]100%|██████████| 30/30 [24:27<00:00, 1467.15s/trial, best loss: -3056.1962269866444]100%|██████████| 30/30 [24:27<00:00, 48.90s/trial, best loss: -3056.1962269866444]  
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 4 since program start
2020-12-20 02:54:26.030617
Found saved Trials! Loading...
Rerunning from 30 trials to add another one.
 97%|█████████▋| 30/31 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 30/31 [00:00<00:00, 1429.94trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 30/31 [00:00<00:00, 1419.31trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 30/31 [01:24<00:02,  2.81s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 30/31 [01:24<00:02,  2.81s/trial, best loss=?]                                                               [02:55:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 30/31 [01:26<00:02,  2.90s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 30/31 [01:29<00:02,  2.98s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               True
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 30/31 [01:29<00:02,  3.00s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 30/31 [01:34<00:03,  3.14s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 30/31 [01:34<00:03,  3.14s/trial, best loss=?]                                                               [02:56:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 30/31 [01:36<00:03,  3.21s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 30/31 [01:38<00:03,  3.29s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               True
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 30/31 [01:39<00:03,  3.31s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 30/31 [01:43<00:03,  3.45s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 30/31 [01:43<00:03,  3.45s/trial, best loss=?]                                                               [02:56:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 30/31 [01:45<00:03,  3.51s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 30/31 [01:47<00:03,  3.59s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               True
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 30/31 [01:48<00:03,  3.62s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 30/31 [01:52<00:03,  3.77s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 30/31 [01:52<00:03,  3.77s/trial, best loss=?]                                                               [02:56:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 30/31 [01:55<00:03,  3.85s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 30/31 [01:57<00:03,  3.92s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               True
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 30/31 [01:58<00:03,  3.95s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 30/31 [02:03<00:04,  4.11s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 30/31 [02:03<00:04,  4.11s/trial, best loss=?]                                                               [02:56:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 30/31 [02:05<00:04,  4.19s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 30/31 [02:08<00:04,  4.27s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               True
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 30/31 [02:09<00:04,  4.30s/trial, best loss=?]100%|██████████| 31/31 [02:09<00:00, 129.69s/trial, best loss: -3056.1962269866444]100%|██████████| 31/31 [02:09<00:00,  4.18s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 5 since program start
2020-12-20 02:56:35.768162
Found saved Trials! Loading...
Rerunning from 31 trials to add another one.
 97%|█████████▋| 31/32 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 31/32 [00:00<00:00, 1463.27trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 31/32 [00:00<00:00, 1452.03trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 31/32 [01:24<00:02,  2.72s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 31/32 [01:24<00:02,  2.72s/trial, best loss=?]                                                               [02:58:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 31/32 [01:26<00:02,  2.80s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 31/32 [01:28<00:02,  2.85s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               True
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 31/32 [01:28<00:02,  2.87s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 31/32 [01:32<00:02,  2.99s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 31/32 [01:32<00:02,  2.99s/trial, best loss=?]                                                               [02:58:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 31/32 [01:34<00:03,  3.04s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 31/32 [01:35<00:03,  3.08s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               True
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 31/32 [01:36<00:03,  3.11s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 31/32 [01:40<00:03,  3.23s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 31/32 [01:40<00:03,  3.23s/trial, best loss=?]                                                               [02:58:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 31/32 [01:41<00:03,  3.29s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 31/32 [01:43<00:03,  3.33s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               True
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 31/32 [01:44<00:03,  3.36s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 31/32 [01:48<00:03,  3.49s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 31/32 [01:48<00:03,  3.49s/trial, best loss=?]                                                               [02:58:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 31/32 [01:50<00:03,  3.57s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 31/32 [01:52<00:03,  3.61s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               True
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 31/32 [01:52<00:03,  3.64s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 31/32 [01:57<00:03,  3.78s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 31/32 [01:57<00:03,  3.78s/trial, best loss=?]                                                               [02:58:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 31/32 [01:59<00:03,  3.85s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 31/32 [02:00<00:03,  3.90s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               True
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 31/32 [02:01<00:03,  3.93s/trial, best loss=?]100%|██████████| 32/32 [02:02<00:00, 122.34s/trial, best loss: -3056.1962269866444]100%|██████████| 32/32 [02:02<00:00,  3.82s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 6 since program start
2020-12-20 02:58:38.219464
Found saved Trials! Loading...
Rerunning from 32 trials to add another one.
 97%|█████████▋| 32/33 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 32/33 [00:00<00:00, 1525.98trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 32/33 [00:00<00:00, 1514.67trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 32/33 [01:24<00:02,  2.63s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 32/33 [01:24<00:02,  2.63s/trial, best loss=?]                                                               [03:00:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 32/33 [01:26<00:02,  2.71s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 32/33 [02:43<00:05,  5.11s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               True
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 32/33 [02:45<00:05,  5.18s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 32/33 [02:49<00:05,  5.30s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 32/33 [02:49<00:05,  5.30s/trial, best loss=?]                                                               [03:01:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 32/33 [02:51<00:05,  5.36s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 32/33 [03:57<00:07,  7.42s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               True
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 32/33 [03:59<00:07,  7.49s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 32/33 [04:03<00:07,  7.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 32/33 [04:03<00:07,  7.61s/trial, best loss=?]                                                               [03:02:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 32/33 [04:05<00:07,  7.66s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 32/33 [05:15<00:09,  9.85s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               True
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 32/33 [05:17<00:09,  9.93s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 32/33 [05:21<00:10, 10.06s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 32/33 [05:21<00:10, 10.06s/trial, best loss=?]                                                               [03:04:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 32/33 [05:24<00:10, 10.14s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 32/33 [06:34<00:12, 12.32s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               True
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 32/33 [06:36<00:12, 12.39s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 32/33 [06:40<00:12, 12.53s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 32/33 [06:40<00:12, 12.53s/trial, best loss=?]                                                               [03:05:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 32/33 [06:43<00:12, 12.61s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 32/33 [07:51<00:14, 14.73s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               True
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 32/33 [07:53<00:14, 14.81s/trial, best loss=?]100%|██████████| 33/33 [07:54<00:00, 474.48s/trial, best loss: -3056.1962269866444]100%|██████████| 33/33 [07:54<00:00, 14.38s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 7 since program start
2020-12-20 03:06:32.749607
Found saved Trials! Loading...
Rerunning from 33 trials to add another one.
 97%|█████████▋| 33/34 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 33/34 [00:00<00:00, 1496.39trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 33/34 [00:00<00:00, 1485.62trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 33/34 [01:24<00:02,  2.55s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.3, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 33/34 [01:24<00:02,  2.55s/trial, best loss=?]                                                               [03:07:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 33/34 [01:26<00:02,  2.63s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 33/34 [01:28<00:02,  2.67s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               True
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 33/34 [01:28<00:02,  2.69s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 33/34 [01:32<00:02,  2.81s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 33/34 [01:32<00:02,  2.81s/trial, best loss=?]                                                               [03:08:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 33/34 [01:34<00:02,  2.85s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 33/34 [01:35<00:02,  2.89s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               True
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 33/34 [01:36<00:02,  2.91s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 33/34 [01:39<00:03,  3.03s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 33/34 [01:39<00:03,  3.03s/trial, best loss=?]                                                               [03:08:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 33/34 [01:41<00:03,  3.08s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 33/34 [01:42<00:03,  3.12s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               True
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 33/34 [01:43<00:03,  3.14s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 33/34 [01:47<00:03,  3.27s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 33/34 [01:47<00:03,  3.27s/trial, best loss=?]                                                               [03:08:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 33/34 [01:50<00:03,  3.34s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 33/34 [01:51<00:03,  3.38s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               True
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 33/34 [01:52<00:03,  3.40s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 33/34 [01:56<00:03,  3.53s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 33/34 [01:56<00:03,  3.53s/trial, best loss=?]                                                               [03:08:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 33/34 [01:58<00:03,  3.60s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 33/34 [02:00<00:03,  3.65s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               True
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 33/34 [02:01<00:03,  3.67s/trial, best loss=?]100%|██████████| 34/34 [02:01<00:00, 121.80s/trial, best loss: -3056.1962269866444]100%|██████████| 34/34 [02:01<00:00,  3.58s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 8 since program start
2020-12-20 03:08:34.599321
Found saved Trials! Loading...
Rerunning from 34 trials to add another one.
 97%|█████████▋| 34/35 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 34/35 [00:00<00:00, 432.48trial/s, best loss=?]                                                                New call of hyperopt_train_test
 97%|█████████▋| 34/35 [00:00<00:00, 431.46trial/s, best loss=?]                                                                Model used for fitting:
 97%|█████████▋| 34/35 [01:24<00:02,  2.49s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 34/35 [01:24<00:02,  2.49s/trial, best loss=?]                                                               [03:10:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 34/35 [01:27<00:02,  2.57s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 34/35 [01:33<00:02,  2.75s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               True
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 34/35 [01:34<00:02,  2.79s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 34/35 [01:38<00:02,  2.90s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 34/35 [01:38<00:02,  2.90s/trial, best loss=?]                                                               [03:10:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 34/35 [01:40<00:02,  2.95s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 34/35 [01:45<00:03,  3.11s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               True
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 34/35 [01:46<00:03,  3.15s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 34/35 [01:50<00:03,  3.26s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 34/35 [01:50<00:03,  3.26s/trial, best loss=?]                                                               [03:10:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 34/35 [01:52<00:03,  3.30s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 34/35 [01:58<00:03,  3.47s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               True
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 34/35 [01:59<00:03,  3.51s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 34/35 [02:03<00:03,  3.63s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 34/35 [02:03<00:03,  3.63s/trial, best loss=?]                                                               [03:10:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 34/35 [02:05<00:03,  3.70s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 34/35 [02:11<00:03,  3.87s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               True
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 34/35 [02:12<00:03,  3.91s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 34/35 [02:17<00:04,  4.04s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 34/35 [02:17<00:04,  4.04s/trial, best loss=?]                                                               [03:10:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 34/35 [02:19<00:04,  4.11s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 34/35 [02:25<00:04,  4.29s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               True
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 34/35 [02:27<00:04,  4.33s/trial, best loss=?]100%|██████████| 35/35 [02:27<00:00, 147.89s/trial, best loss: -3056.1962269866444]100%|██████████| 35/35 [02:27<00:00,  4.23s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 9 since program start
2020-12-20 03:11:02.539298
Found saved Trials! Loading...
Rerunning from 35 trials to add another one.
 97%|█████████▋| 35/36 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 35/36 [00:00<00:00, 1649.15trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 35/36 [00:00<00:00, 1636.88trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 35/36 [01:23<00:02,  2.40s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 35/36 [01:23<00:02,  2.40s/trial, best loss=?]                                                               [03:12:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 35/36 [01:27<00:02,  2.51s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 35/36 [09:10<00:15, 15.72s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               True
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 35/36 [09:18<00:15, 15.95s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 35/36 [09:22<00:16, 16.07s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 35/36 [09:22<00:16, 16.07s/trial, best loss=?]                                                               [03:20:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 35/36 [09:24<00:16, 16.14s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 35/36 [16:13<00:27, 27.81s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               True
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 35/36 [16:21<00:28, 28.03s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 35/36 [16:25<00:28, 28.15s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 35/36 [16:25<00:28, 28.15s/trial, best loss=?]                                                               [03:27:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 35/36 [16:27<00:28, 28.21s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 35/36 [23:30<00:40, 40.29s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               True
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 35/36 [23:38<00:40, 40.54s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 35/36 [23:43<00:40, 40.66s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 35/36 [23:43<00:40, 40.66s/trial, best loss=?]                                                               [03:34:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 35/36 [23:45<00:40, 40.74s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 35/36 [31:08<00:53, 53.39s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               True
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 35/36 [31:17<00:53, 53.64s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 35/36 [31:21<00:53, 53.77s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 35/36 [31:21<00:53, 53.77s/trial, best loss=?]                                                               [03:42:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 35/36 [31:24<00:53, 53.84s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 35/36 [38:50<01:06, 66.59s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               True
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 35/36 [39:00<01:06, 66.87s/trial, best loss=?]100%|██████████| 36/36 [39:01<00:00, 2341.11s/trial, best loss: -3056.1962269866444]100%|██████████| 36/36 [39:01<00:00, 65.03s/trial, best loss: -3056.1962269866444]  
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 10 since program start
2020-12-20 03:50:03.701525
Found saved Trials! Loading...
Rerunning from 36 trials to add another one.
 97%|█████████▋| 36/37 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 36/37 [00:00<00:00, 1720.33trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 36/37 [00:00<00:00, 1707.16trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 36/37 [01:24<00:02,  2.34s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 36/37 [01:24<00:02,  2.34s/trial, best loss=?]                                                               [03:51:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 36/37 [01:26<00:02,  2.39s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 36/37 [01:27<00:02,  2.43s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               True
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 36/37 [01:28<00:02,  2.45s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 36/37 [01:32<00:02,  2.56s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 36/37 [01:32<00:02,  2.56s/trial, best loss=?]                                                               [03:51:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 36/37 [01:33<00:02,  2.60s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 36/37 [01:34<00:02,  2.64s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               True
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 36/37 [01:35<00:02,  2.66s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 36/37 [01:39<00:02,  2.76s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 36/37 [01:39<00:02,  2.76s/trial, best loss=?]                                                               [03:51:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 36/37 [01:40<00:02,  2.80s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 36/37 [01:42<00:02,  2.84s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               True
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 36/37 [01:43<00:02,  2.86s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 36/37 [01:47<00:02,  2.98s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 36/37 [01:47<00:02,  2.98s/trial, best loss=?]                                                               [03:51:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 36/37 [01:48<00:03,  3.02s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 36/37 [01:50<00:03,  3.06s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               True
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 36/37 [01:50<00:03,  3.08s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 36/37 [01:55<00:03,  3.20s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 36/37 [01:55<00:03,  3.20s/trial, best loss=?]                                                               [03:52:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 36/37 [01:56<00:03,  3.25s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 36/37 [01:58<00:03,  3.28s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               True
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 36/37 [01:59<00:03,  3.31s/trial, best loss=?]100%|██████████| 37/37 [01:59<00:00, 119.67s/trial, best loss: -3056.1962269866444]100%|██████████| 37/37 [01:59<00:00,  3.23s/trial, best loss: -3056.1962269866444] 
Best model so far :
{'colsample_bytree': 2, 'features': 1, 'learning_rate': 4, 'max_depth': 0, 'n_estimators': 0, 'random_state': 0, 'subsample': 0, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 11 since program start
2020-12-20 03:52:03.426841
Found saved Trials! Loading...
Rerunning from 37 trials to add another one.
 97%|█████████▋| 37/38 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 37/38 [00:00<00:00, 477.99trial/s, best loss=?]                                                                New call of hyperopt_train_test
 97%|█████████▋| 37/38 [00:00<00:00, 476.86trial/s, best loss=?]                                                                Model used for fitting:
 97%|█████████▋| 37/38 [01:23<00:02,  2.27s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 37/38 [01:23<00:02,  2.27s/trial, best loss=?]                                                               [03:53:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 37/38 [01:25<00:02,  2.31s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 37/38 [01:31<00:02,  2.48s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               True
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 37/38 [01:32<00:02,  2.51s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 37/38 [01:36<00:02,  2.62s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 37/38 [01:36<00:02,  2.62s/trial, best loss=?]                                                               [03:53:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 37/38 [01:38<00:02,  2.66s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 37/38 [01:43<00:02,  2.81s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               True
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 37/38 [01:45<00:02,  2.84s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 37/38 [01:49<00:02,  2.95s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 37/38 [01:49<00:02,  2.95s/trial, best loss=?]                                                               [03:53:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 37/38 [01:50<00:02,  2.99s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 37/38 [01:56<00:03,  3.14s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               True
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 37/38 [01:57<00:03,  3.18s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 37/38 [02:01<00:03,  3.29s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 37/38 [02:01<00:03,  3.29s/trial, best loss=?]                                                               [03:54:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 37/38 [02:03<00:03,  3.33s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 37/38 [02:09<00:03,  3.50s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               True
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 37/38 [02:10<00:03,  3.53s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 37/38 [02:14<00:03,  3.65s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 37/38 [02:14<00:03,  3.65s/trial, best loss=?]                                                               [03:54:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 37/38 [02:16<00:03,  3.69s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 37/38 [02:22<00:03,  3.85s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               True
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 37/38 [02:24<00:03,  3.89s/trial, best loss=?]100%|██████████| 38/38 [02:24<00:00, 144.63s/trial, best loss: -3143.0184291784662]100%|██████████| 38/38 [02:24<00:00,  3.81s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 12 since program start
2020-12-20 03:54:28.114433
Found saved Trials! Loading...
Rerunning from 38 trials to add another one.
 97%|█████████▋| 38/39 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 97%|█████████▋| 38/39 [00:00<00:00, 1821.42trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 97%|█████████▋| 38/39 [00:00<00:00, 1807.50trial/s, best loss=?]                                                                 Model used for fitting:
 97%|█████████▋| 38/39 [01:24<00:02,  2.23s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 38/39 [01:24<00:02,  2.23s/trial, best loss=?]                                                               [03:55:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 38/39 [01:26<00:02,  2.28s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 38/39 [01:32<00:02,  2.44s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               (259530, 139)
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               True
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 38/39 [01:33<00:02,  2.46s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 38/39 [01:37<00:02,  2.57s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 38/39 [01:37<00:02,  2.57s/trial, best loss=?]                                                               [03:56:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 38/39 [01:39<00:02,  2.61s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 38/39 [01:44<00:02,  2.74s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               (271631, 139)
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               True
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 38/39 [01:44<00:02,  2.76s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 38/39 [01:48<00:02,  2.86s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 38/39 [01:48<00:02,  2.86s/trial, best loss=?]                                                               [03:56:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 38/39 [01:50<00:02,  2.90s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 38/39 [01:55<00:03,  3.05s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               (286101, 139)
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               True
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 38/39 [01:56<00:03,  3.07s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 38/39 [02:00<00:03,  3.18s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 38/39 [02:00<00:03,  3.18s/trial, best loss=?]                                                               [03:56:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 38/39 [02:02<00:03,  3.22s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 38/39 [02:07<00:03,  3.36s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               (289002, 139)
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               True
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 38/39 [02:08<00:03,  3.38s/trial, best loss=?]                                                               Model used for fitting:
 97%|█████████▋| 38/39 [02:13<00:03,  3.50s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 38/39 [02:13<00:03,  3.50s/trial, best loss=?]                                                               [03:56:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 38/39 [02:14<00:03,  3.54s/trial, best loss=?]                                                               predict called
 97%|█████████▋| 38/39 [02:19<00:03,  3.67s/trial, best loss=?]                                                               Type of X:
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               Shape of X:
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               (323071, 139)
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               Type of y:
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               model fitted ?
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               True
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]                                                               y is not None
 97%|█████████▋| 38/39 [02:20<00:03,  3.70s/trial, best loss=?]100%|██████████| 39/39 [02:21<00:00, 141.16s/trial, best loss: -3143.0184291784662]100%|██████████| 39/39 [02:21<00:00,  3.62s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 13 since program start
2020-12-20 03:56:49.324228
Found saved Trials! Loading...
Rerunning from 39 trials to add another one.
 98%|█████████▊| 39/40 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 39/40 [00:00<00:00, 1844.69trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 39/40 [00:00<00:00, 1830.94trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 39/40 [01:25<00:02,  2.18s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 39/40 [01:25<00:02,  2.18s/trial, best loss=?]                                                               [03:58:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 39/40 [01:26<00:02,  2.23s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 39/40 [01:53<00:02,  2.92s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               True
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 39/40 [01:56<00:02,  2.98s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 39/40 [02:00<00:03,  3.08s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 39/40 [02:00<00:03,  3.08s/trial, best loss=?]                                                               [03:58:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 39/40 [02:01<00:03,  3.12s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 39/40 [02:26<00:03,  3.77s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               True
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 39/40 [02:29<00:03,  3.83s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 39/40 [02:33<00:03,  3.93s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 39/40 [02:33<00:03,  3.93s/trial, best loss=?]                                                               [03:59:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 39/40 [02:34<00:03,  3.97s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 39/40 [03:00<00:04,  4.62s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               True
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 39/40 [03:02<00:04,  4.69s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 39/40 [03:07<00:04,  4.80s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 39/40 [03:07<00:04,  4.80s/trial, best loss=?]                                                               [03:59:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 39/40 [03:08<00:04,  4.84s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 39/40 [03:35<00:05,  5.52s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               True
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 39/40 [03:37<00:05,  5.58s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 39/40 [03:42<00:05,  5.70s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 39/40 [03:42<00:05,  5.70s/trial, best loss=?]                                                               [04:00:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 39/40 [03:43<00:05,  5.74s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 39/40 [04:10<00:06,  6.41s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               True
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 39/40 [04:12<00:06,  6.48s/trial, best loss=?]100%|██████████| 40/40 [04:13<00:00, 253.39s/trial, best loss: -3143.0184291784662]100%|██████████| 40/40 [04:13<00:00,  6.33s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 14 since program start
2020-12-20 04:01:02.766040
Found saved Trials! Loading...
Rerunning from 40 trials to add another one.
 98%|█████████▊| 40/41 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 40/41 [00:00<00:00, 1853.39trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 40/41 [00:00<00:00, 1838.94trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 40/41 [01:24<00:02,  2.11s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 40/41 [01:24<00:02,  2.11s/trial, best loss=?]                                                               [04:02:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 40/41 [01:26<00:02,  2.15s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 40/41 [01:32<00:02,  2.32s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               True
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 40/41 [01:33<00:02,  2.35s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 40/41 [01:37<00:02,  2.45s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 40/41 [01:37<00:02,  2.45s/trial, best loss=?]                                                               [04:02:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 40/41 [01:39<00:02,  2.48s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 40/41 [01:45<00:02,  2.64s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               True
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 40/41 [01:46<00:02,  2.67s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 40/41 [01:50<00:02,  2.77s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 40/41 [01:50<00:02,  2.77s/trial, best loss=?]                                                               [04:02:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 40/41 [01:52<00:02,  2.80s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 40/41 [01:58<00:02,  2.96s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               True
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 40/41 [01:59<00:02,  2.99s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 40/41 [02:04<00:03,  3.10s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 40/41 [02:04<00:03,  3.10s/trial, best loss=?]                                                               [04:03:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 40/41 [02:05<00:03,  3.14s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 40/41 [02:12<00:03,  3.30s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               True
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 40/41 [02:13<00:03,  3.33s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 40/41 [02:17<00:03,  3.45s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 40/41 [02:17<00:03,  3.45s/trial, best loss=?]                                                               [04:03:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 40/41 [02:19<00:03,  3.49s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 40/41 [02:26<00:03,  3.65s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               True
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 40/41 [02:27<00:03,  3.69s/trial, best loss=?]100%|██████████| 41/41 [02:28<00:00, 148.11s/trial, best loss: -3143.0184291784662]100%|██████████| 41/41 [02:28<00:00,  3.61s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 15 since program start
2020-12-20 04:03:30.931198
Found saved Trials! Loading...
Rerunning from 41 trials to add another one.
 98%|█████████▊| 41/42 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 41/42 [00:00<00:00, 1916.45trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 41/42 [00:00<00:00, 1899.76trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 41/42 [01:26<00:02,  2.11s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 41/42 [01:26<00:02,  2.11s/trial, best loss=?]                                                               [04:04:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 41/42 [01:28<00:02,  2.16s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 41/42 [01:39<00:02,  2.43s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               True
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 41/42 [01:40<00:02,  2.45s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 41/42 [01:44<00:02,  2.55s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 41/42 [01:44<00:02,  2.55s/trial, best loss=?]                                                               [04:05:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 41/42 [01:46<00:02,  2.59s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 41/42 [01:56<00:02,  2.84s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               True
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 41/42 [01:57<00:02,  2.86s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 41/42 [02:01<00:02,  2.96s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 41/42 [02:01<00:02,  2.96s/trial, best loss=?]                                                               [04:05:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 41/42 [02:02<00:02,  3.00s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 41/42 [02:13<00:03,  3.25s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               True
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 41/42 [02:14<00:03,  3.27s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 41/42 [02:18<00:03,  3.38s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 41/42 [02:18<00:03,  3.38s/trial, best loss=?]                                                               [04:05:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 41/42 [02:20<00:03,  3.42s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 41/42 [02:31<00:03,  3.69s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               True
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 41/42 [02:32<00:03,  3.71s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 41/42 [02:36<00:03,  3.82s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 41/42 [02:36<00:03,  3.82s/trial, best loss=?]                                                               [04:06:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 41/42 [02:38<00:03,  3.87s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 41/42 [02:48<00:04,  4.12s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               True
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 41/42 [02:49<00:04,  4.15s/trial, best loss=?]100%|██████████| 42/42 [02:50<00:00, 170.61s/trial, best loss: -3143.0184291784662]100%|██████████| 42/42 [02:50<00:00,  4.06s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 16 since program start
2020-12-20 04:06:21.592893
Found saved Trials! Loading...
Rerunning from 42 trials to add another one.
 98%|█████████▊| 42/43 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 42/43 [00:00<00:00, 1970.81trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 42/43 [00:00<00:00, 1954.19trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 42/43 [01:24<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 42/43 [01:24<00:02,  2.01s/trial, best loss=?]                                                               [04:07:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 42/43 [01:27<00:02,  2.08s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 42/43 [09:33<00:13, 13.66s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               True
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 42/43 [09:41<00:13, 13.85s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 42/43 [09:45<00:13, 13.94s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 42/43 [09:45<00:13, 13.94s/trial, best loss=?]                                                               [04:16:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 42/43 [09:47<00:13, 13.99s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 42/43 [16:57<00:24, 24.23s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               True
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 42/43 [17:05<00:24, 24.42s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 42/43 [17:09<00:24, 24.52s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 42/43 [17:09<00:24, 24.52s/trial, best loss=?]                                                               [04:23:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 42/43 [17:12<00:24, 24.57s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 42/43 [24:39<00:35, 35.23s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               True
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 42/43 [24:48<00:35, 35.44s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 42/43 [24:52<00:35, 35.54s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 42/43 [24:52<00:35, 35.55s/trial, best loss=?]                                                               [04:31:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 42/43 [24:55<00:35, 35.61s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 42/43 [32:42<00:46, 46.72s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               True
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 42/43 [32:51<00:46, 46.94s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 42/43 [32:55<00:47, 47.04s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 42/43 [32:55<00:47, 47.04s/trial, best loss=?]                                                               [04:39:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 42/43 [32:58<00:47, 47.10s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 42/43 [40:45<00:58, 58.23s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               True
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 42/43 [40:55<00:58, 58.46s/trial, best loss=?]100%|██████████| 43/43 [40:56<00:00, 2456.04s/trial, best loss: -3143.0184291784662]100%|██████████| 43/43 [40:56<00:00, 57.12s/trial, best loss: -3143.0184291784662]  
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 17 since program start
2020-12-20 04:47:17.686074
Found saved Trials! Loading...
Rerunning from 43 trials to add another one.
 98%|█████████▊| 43/44 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 43/44 [00:00<00:00, 1902.94trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 43/44 [00:00<00:00, 1889.11trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 43/44 [01:26<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.3, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 43/44 [01:26<00:02,  2.01s/trial, best loss=?]                                                               [04:48:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 43/44 [01:28<00:02,  2.05s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 43/44 [01:29<00:02,  2.09s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               True
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 43/44 [01:30<00:02,  2.10s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 43/44 [01:34<00:02,  2.19s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 43/44 [01:34<00:02,  2.19s/trial, best loss=?]                                                               [04:48:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 43/44 [01:35<00:02,  2.23s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 43/44 [01:37<00:02,  2.26s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               True
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 43/44 [01:37<00:02,  2.28s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 43/44 [01:41<00:02,  2.37s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 43/44 [01:41<00:02,  2.37s/trial, best loss=?]                                                               [04:49:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 43/44 [01:43<00:02,  2.40s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 43/44 [01:44<00:02,  2.44s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               True
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 43/44 [01:45<00:02,  2.45s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 43/44 [01:49<00:02,  2.55s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 43/44 [01:49<00:02,  2.55s/trial, best loss=?]                                                               [04:49:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 43/44 [01:51<00:02,  2.59s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 43/44 [01:52<00:02,  2.62s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               True
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 43/44 [01:53<00:02,  2.64s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 43/44 [01:57<00:02,  2.74s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 43/44 [01:57<00:02,  2.74s/trial, best loss=?]                                                               [04:49:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 43/44 [01:59<00:02,  2.78s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 43/44 [02:01<00:02,  2.82s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               True
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 43/44 [02:01<00:02,  2.84s/trial, best loss=?]100%|██████████| 44/44 [02:02<00:00, 122.56s/trial, best loss: -3143.0184291784662]100%|██████████| 44/44 [02:02<00:00,  2.79s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 18 since program start
2020-12-20 04:49:20.303186
Found saved Trials! Loading...
Rerunning from 44 trials to add another one.
 98%|█████████▊| 44/45 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 44/45 [00:00<00:00, 2051.60trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 44/45 [00:00<00:00, 2036.38trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 44/45 [01:25<00:01,  1.93s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 44/45 [01:25<00:01,  1.93s/trial, best loss=?]                                                               [04:50:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 44/45 [01:26<00:01,  1.97s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 44/45 [02:10<00:02,  2.97s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               True
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 44/45 [02:14<00:03,  3.06s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 44/45 [02:18<00:03,  3.15s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 44/45 [02:18<00:03,  3.15s/trial, best loss=?]                                                               [04:51:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 44/45 [02:19<00:03,  3.18s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 44/45 [03:00<00:04,  4.10s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               True
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 44/45 [03:04<00:04,  4.19s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 44/45 [03:08<00:04,  4.28s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 44/45 [03:08<00:04,  4.28s/trial, best loss=?]                                                               [04:52:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 44/45 [03:09<00:04,  4.32s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 44/45 [03:50<00:05,  5.25s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               True
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 44/45 [03:55<00:05,  5.35s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 44/45 [03:59<00:05,  5.44s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 44/45 [03:59<00:05,  5.44s/trial, best loss=?]                                                               [04:53:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 44/45 [04:01<00:05,  5.48s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 44/45 [04:43<00:06,  6.44s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               True
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 44/45 [04:47<00:06,  6.54s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 44/45 [04:52<00:06,  6.64s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 44/45 [04:52<00:06,  6.64s/trial, best loss=?]                                                               [04:54:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 44/45 [04:53<00:06,  6.68s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 44/45 [05:36<00:07,  7.65s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               True
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 44/45 [05:41<00:07,  7.76s/trial, best loss=?]100%|██████████| 45/45 [05:42<00:00, 342.13s/trial, best loss: -3143.0184291784662]100%|██████████| 45/45 [05:42<00:00,  7.60s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 19 since program start
2020-12-20 04:55:02.548773
Found saved Trials! Loading...
Rerunning from 45 trials to add another one.
 98%|█████████▊| 45/46 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 45/46 [00:00<00:00, 2124.99trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 45/46 [00:00<00:00, 2108.99trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 45/46 [01:24<00:01,  1.88s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 45/46 [01:24<00:01,  1.88s/trial, best loss=?]                                                               [04:56:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 45/46 [01:26<00:01,  1.92s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 45/46 [01:32<00:02,  2.06s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               True
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 45/46 [01:33<00:02,  2.09s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 45/46 [01:37<00:02,  2.18s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 45/46 [01:37<00:02,  2.18s/trial, best loss=?]                                                               [04:56:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 45/46 [01:39<00:02,  2.21s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 45/46 [01:45<00:02,  2.34s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               True
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 45/46 [01:46<00:02,  2.37s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 45/46 [01:50<00:02,  2.46s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 45/46 [01:50<00:02,  2.46s/trial, best loss=?]                                                               [04:56:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 45/46 [01:51<00:02,  2.49s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 45/46 [01:58<00:02,  2.62s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               True
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 45/46 [01:59<00:02,  2.65s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 45/46 [02:03<00:02,  2.75s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 45/46 [02:03<00:02,  2.75s/trial, best loss=?]                                                               [04:57:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 45/46 [02:05<00:02,  2.78s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 45/46 [02:11<00:02,  2.92s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               True
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 45/46 [02:12<00:02,  2.95s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 45/46 [02:17<00:03,  3.05s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 45/46 [02:17<00:03,  3.05s/trial, best loss=?]                                                               [04:57:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 45/46 [02:18<00:03,  3.09s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 45/46 [02:25<00:03,  3.23s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               True
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 45/46 [02:26<00:03,  3.26s/trial, best loss=?]100%|██████████| 46/46 [02:27<00:00, 147.42s/trial, best loss: -3143.0184291784662]100%|██████████| 46/46 [02:27<00:00,  3.20s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 20 since program start
2020-12-20 04:57:30.021022
Found saved Trials! Loading...
Rerunning from 46 trials to add another one.
 98%|█████████▊| 46/47 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 46/47 [00:00<00:00, 2143.04trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 46/47 [00:00<00:00, 2127.19trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 46/47 [01:25<00:01,  1.86s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 46/47 [01:25<00:01,  1.86s/trial, best loss=?]                                                               [04:58:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 46/47 [01:27<00:01,  1.90s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 46/47 [01:33<00:02,  2.03s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               True
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 46/47 [01:34<00:02,  2.05s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 46/47 [01:38<00:02,  2.13s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 46/47 [01:38<00:02,  2.13s/trial, best loss=?]                                                               [04:59:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 46/47 [01:39<00:02,  2.17s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 46/47 [01:43<00:02,  2.26s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               True
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 46/47 [01:44<00:02,  2.28s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 46/47 [01:48<00:02,  2.36s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 46/47 [01:48<00:02,  2.36s/trial, best loss=?]                                                               [04:59:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 46/47 [01:50<00:02,  2.40s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 46/47 [01:55<00:02,  2.50s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               True
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 46/47 [01:55<00:02,  2.52s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 46/47 [02:00<00:02,  2.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 46/47 [02:00<00:02,  2.61s/trial, best loss=?]                                                               [04:59:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 46/47 [02:01<00:02,  2.65s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 46/47 [02:06<00:02,  2.75s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               True
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 46/47 [02:07<00:02,  2.77s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 46/47 [02:11<00:02,  2.87s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 46/47 [02:11<00:02,  2.87s/trial, best loss=?]                                                               [04:59:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 46/47 [02:13<00:02,  2.90s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 46/47 [02:18<00:03,  3.01s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               True
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 46/47 [02:19<00:03,  3.03s/trial, best loss=?]100%|██████████| 47/47 [02:19<00:00, 139.81s/trial, best loss: -3143.0184291784662]100%|██████████| 47/47 [02:19<00:00,  2.97s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 21 since program start
2020-12-20 04:59:49.887738
Found saved Trials! Loading...
Rerunning from 47 trials to add another one.
 98%|█████████▊| 47/48 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 47/48 [00:00<00:00, 2183.44trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 47/48 [00:00<00:00, 2167.69trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 47/48 [01:23<00:01,  1.78s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 47/48 [01:23<00:01,  1.78s/trial, best loss=?]                                                               [05:01:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 47/48 [01:25<00:01,  1.83s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 47/48 [03:30<00:04,  4.49s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               True
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 47/48 [03:35<00:04,  4.58s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 47/48 [03:38<00:04,  4.66s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 47/48 [03:39<00:04,  4.66s/trial, best loss=?]                                                               [05:03:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 47/48 [03:40<00:04,  4.70s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 47/48 [05:33<00:07,  7.09s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               True
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 47/48 [05:37<00:07,  7.18s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 47/48 [05:41<00:07,  7.27s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 47/48 [05:41<00:07,  7.27s/trial, best loss=?]                                                               [05:05:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 47/48 [05:43<00:07,  7.30s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 47/48 [07:38<00:09,  9.76s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               True
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 47/48 [07:43<00:09,  9.85s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 47/48 [07:47<00:09,  9.94s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 47/48 [07:47<00:09,  9.94s/trial, best loss=?]                                                               [05:07:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 47/48 [07:49<00:09,  9.98s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 47/48 [09:49<00:12, 12.55s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               True
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 47/48 [09:54<00:12, 12.65s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 47/48 [09:58<00:12, 12.74s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 47/48 [09:58<00:12, 12.74s/trial, best loss=?]                                                               [05:09:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 47/48 [10:00<00:12, 12.78s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 47/48 [12:03<00:15, 15.39s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               True
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 47/48 [12:08<00:15, 15.50s/trial, best loss=?]100%|██████████| 48/48 [12:09<00:00, 729.04s/trial, best loss: -3143.0184291784662]100%|██████████| 48/48 [12:09<00:00, 15.19s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 22 since program start
2020-12-20 05:11:59.039808
Found saved Trials! Loading...
Rerunning from 48 trials to add another one.
 98%|█████████▊| 48/49 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 48/49 [00:00<00:00, 2240.97trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 48/49 [00:00<00:00, 2224.41trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 48/49 [01:24<00:01,  1.75s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 48/49 [01:24<00:01,  1.75s/trial, best loss=?]                                                               [05:13:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 48/49 [01:26<00:01,  1.80s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 48/49 [11:41<00:14, 14.62s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               True
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 48/49 [11:56<00:14, 14.93s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 48/49 [12:00<00:15, 15.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 48/49 [12:00<00:15, 15.01s/trial, best loss=?]                                                               [05:24:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 48/49 [12:02<00:15, 15.05s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 48/49 [21:03<00:26, 26.32s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               True
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 48/49 [21:18<00:26, 26.64s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 48/49 [21:22<00:26, 26.72s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 48/49 [21:22<00:26, 26.72s/trial, best loss=?]                                                               [05:33:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 48/49 [21:24<00:26, 26.76s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 48/49 [30:37<00:38, 38.28s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               True
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 48/49 [30:52<00:38, 38.60s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 48/49 [30:57<00:38, 38.69s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 48/49 [30:57<00:38, 38.69s/trial, best loss=?]                                                               [05:42:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 48/49 [30:59<00:38, 38.73s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 48/49 [40:41<00:50, 50.86s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               True
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 48/49 [40:56<00:51, 51.18s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 48/49 [41:01<00:51, 51.28s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 48/49 [41:01<00:51, 51.28s/trial, best loss=?]                                                               [05:53:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 48/49 [41:03<00:51, 51.32s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 48/49 [50:58<01:03, 63.71s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               True
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 48/49 [51:16<01:04, 64.09s/trial, best loss=?]100%|██████████| 49/49 [51:16<00:00, 3076.86s/trial, best loss: -3143.0184291784662]100%|██████████| 49/49 [51:16<00:00, 62.79s/trial, best loss: -3143.0184291784662]  
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 23 since program start
2020-12-20 06:03:15.956689
Found saved Trials! Loading...
Rerunning from 49 trials to add another one.
 98%|█████████▊| 49/50 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 49/50 [00:00<00:00, 2281.59trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 49/50 [00:00<00:00, 2265.34trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 49/50 [01:25<00:01,  1.74s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 49/50 [01:25<00:01,  1.74s/trial, best loss=?]                                                               [06:04:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 49/50 [01:26<00:01,  1.77s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 49/50 [01:33<00:01,  1.92s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               True
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 49/50 [01:35<00:01,  1.94s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 49/50 [01:39<00:02,  2.02s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 49/50 [01:39<00:02,  2.02s/trial, best loss=?]                                                               [06:04:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 49/50 [01:40<00:02,  2.05s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 49/50 [01:46<00:02,  2.18s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               True
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 49/50 [01:48<00:02,  2.21s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 49/50 [01:52<00:02,  2.29s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 49/50 [01:52<00:02,  2.29s/trial, best loss=?]                                                               [06:05:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 49/50 [01:53<00:02,  2.32s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 49/50 [01:59<00:02,  2.45s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               True
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 49/50 [02:01<00:02,  2.47s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 49/50 [02:05<00:02,  2.56s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 49/50 [02:05<00:02,  2.56s/trial, best loss=?]                                                               [06:05:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 49/50 [02:07<00:02,  2.59s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 49/50 [02:13<00:02,  2.73s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               True
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 49/50 [02:15<00:02,  2.76s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 49/50 [02:19<00:02,  2.85s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 49/50 [02:19<00:02,  2.85s/trial, best loss=?]                                                               [06:05:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 49/50 [02:21<00:02,  2.88s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 49/50 [02:27<00:03,  3.02s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               True
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 49/50 [02:29<00:03,  3.05s/trial, best loss=?]100%|██████████| 50/50 [02:29<00:00, 149.89s/trial, best loss: -3143.0184291784662]100%|██████████| 50/50 [02:29<00:00,  3.00s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 24 since program start
2020-12-20 06:05:45.903729
Found saved Trials! Loading...
Rerunning from 50 trials to add another one.
 98%|█████████▊| 50/51 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 50/51 [00:00<00:00, 2357.83trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 50/51 [00:00<00:00, 2340.39trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 50/51 [01:23<00:01,  1.67s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 50/51 [01:23<00:01,  1.67s/trial, best loss=?]                                                               [06:07:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 50/51 [01:25<00:01,  1.71s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 50/51 [01:27<00:01,  1.75s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               True
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 50/51 [01:28<00:01,  1.77s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 50/51 [01:32<00:01,  1.85s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 50/51 [01:32<00:01,  1.85s/trial, best loss=?]                                                               [06:07:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 50/51 [01:33<00:01,  1.88s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 50/51 [01:35<00:01,  1.92s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               True
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 50/51 [01:36<00:01,  1.93s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 50/51 [01:40<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 50/51 [01:40<00:02,  2.01s/trial, best loss=?]                                                               [06:07:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 50/51 [01:41<00:02,  2.04s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 50/51 [01:44<00:02,  2.09s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               True
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 50/51 [01:45<00:02,  2.10s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 50/51 [01:49<00:02,  2.19s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 50/51 [01:49<00:02,  2.19s/trial, best loss=?]                                                               [06:07:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 50/51 [01:50<00:02,  2.22s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 50/51 [01:53<00:02,  2.26s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               True
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 50/51 [01:53<00:02,  2.28s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 50/51 [01:58<00:02,  2.37s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 50/51 [01:58<00:02,  2.37s/trial, best loss=?]                                                               [06:07:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 50/51 [01:59<00:02,  2.40s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 50/51 [02:02<00:02,  2.44s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               True
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 50/51 [02:03<00:02,  2.46s/trial, best loss=?]100%|██████████| 51/51 [02:03<00:00, 123.73s/trial, best loss: -3143.0184291784662]100%|██████████| 51/51 [02:03<00:00,  2.43s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 25 since program start
2020-12-20 06:07:49.739637
Found saved Trials! Loading...
Rerunning from 51 trials to add another one.
 98%|█████████▊| 51/52 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 51/52 [00:00<00:00, 2394.81trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 51/52 [00:00<00:00, 2377.64trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 51/52 [01:23<00:01,  1.64s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 51/52 [01:23<00:01,  1.64s/trial, best loss=?]                                                               [06:09:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 51/52 [01:25<00:01,  1.68s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 51/52 [01:48<00:02,  2.13s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               True
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 51/52 [01:51<00:02,  2.19s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 51/52 [01:55<00:02,  2.27s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 51/52 [01:55<00:02,  2.27s/trial, best loss=?]                                                               [06:09:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 51/52 [01:56<00:02,  2.29s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 51/52 [02:17<00:02,  2.71s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               True
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 51/52 [02:21<00:02,  2.77s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 51/52 [02:25<00:02,  2.84s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 51/52 [02:25<00:02,  2.84s/trial, best loss=?]                                                               [06:10:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 51/52 [02:26<00:02,  2.87s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 51/52 [02:47<00:03,  3.29s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               True
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 51/52 [02:51<00:03,  3.36s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 51/52 [02:55<00:03,  3.44s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 51/52 [02:55<00:03,  3.44s/trial, best loss=?]                                                               [06:10:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 51/52 [02:56<00:03,  3.47s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 51/52 [03:19<00:03,  3.91s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               True
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 51/52 [03:22<00:03,  3.97s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 51/52 [03:26<00:04,  4.06s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 51/52 [03:26<00:04,  4.06s/trial, best loss=?]                                                               [06:11:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 51/52 [03:28<00:04,  4.09s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 51/52 [03:50<00:04,  4.53s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               True
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 51/52 [03:54<00:04,  4.60s/trial, best loss=?]100%|██████████| 52/52 [03:55<00:00, 235.09s/trial, best loss: -3143.0184291784662]100%|██████████| 52/52 [03:55<00:00,  4.52s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 26 since program start
2020-12-20 06:11:44.884517
Found saved Trials! Loading...
Rerunning from 52 trials to add another one.
 98%|█████████▊| 52/53 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 52/53 [00:00<00:00, 2409.59trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 52/53 [00:00<00:00, 2390.10trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 52/53 [01:24<00:01,  1.62s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 52/53 [01:24<00:01,  1.62s/trial, best loss=?]                                                               [06:13:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 52/53 [01:25<00:01,  1.65s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 52/53 [01:52<00:02,  2.16s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               True
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 52/53 [01:53<00:02,  2.19s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 52/53 [01:57<00:02,  2.26s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 52/53 [01:57<00:02,  2.26s/trial, best loss=?]                                                               [06:13:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 52/53 [01:59<00:02,  2.29s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 52/53 [02:23<00:02,  2.76s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               True
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 52/53 [02:25<00:02,  2.79s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 52/53 [02:29<00:02,  2.87s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 52/53 [02:29<00:02,  2.87s/trial, best loss=?]                                                               [06:14:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 52/53 [02:30<00:02,  2.90s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 52/53 [02:55<00:03,  3.37s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               True
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 52/53 [02:57<00:03,  3.41s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 52/53 [03:01<00:03,  3.49s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 52/53 [03:01<00:03,  3.49s/trial, best loss=?]                                                               [06:14:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 52/53 [03:02<00:03,  3.52s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 52/53 [03:28<00:04,  4.01s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               True
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 52/53 [03:30<00:04,  4.04s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 52/53 [03:34<00:04,  4.13s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 52/53 [03:34<00:04,  4.13s/trial, best loss=?]                                                               [06:15:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 52/53 [03:36<00:04,  4.16s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 52/53 [04:01<00:04,  4.65s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               True
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 52/53 [04:03<00:04,  4.69s/trial, best loss=?]100%|██████████| 53/53 [04:04<00:00, 244.26s/trial, best loss: -3143.0184291784662]100%|██████████| 53/53 [04:04<00:00,  4.61s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 27 since program start
2020-12-20 06:15:49.194457
Found saved Trials! Loading...
Rerunning from 53 trials to add another one.
 98%|█████████▊| 53/54 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 53/54 [00:00<00:00, 2468.25trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 53/54 [00:00<00:00, 2450.24trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 53/54 [01:23<00:01,  1.58s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 53/54 [01:23<00:01,  1.58s/trial, best loss=?]                                                               [06:17:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 53/54 [01:25<00:01,  1.61s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 53/54 [01:37<00:01,  1.84s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               True
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 53/54 [01:39<00:01,  1.87s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 53/54 [01:43<00:01,  1.94s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 53/54 [01:43<00:01,  1.94s/trial, best loss=?]                                                               [06:17:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 53/54 [01:44<00:01,  1.97s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 53/54 [01:55<00:02,  2.17s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               True
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 53/54 [01:57<00:02,  2.21s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 53/54 [02:01<00:02,  2.29s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 53/54 [02:01<00:02,  2.29s/trial, best loss=?]                                                               [06:17:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 53/54 [02:02<00:02,  2.31s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 53/54 [02:13<00:02,  2.52s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               True
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 53/54 [02:15<00:02,  2.56s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 53/54 [02:19<00:02,  2.64s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 53/54 [02:19<00:02,  2.64s/trial, best loss=?]                                                               [06:18:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 53/54 [02:21<00:02,  2.67s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 53/54 [02:32<00:02,  2.88s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               True
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 53/54 [02:34<00:02,  2.92s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 53/54 [02:39<00:03,  3.00s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 53/54 [02:39<00:03,  3.00s/trial, best loss=?]                                                               [06:18:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 53/54 [02:40<00:03,  3.03s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 53/54 [02:52<00:03,  3.25s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               True
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 53/54 [02:54<00:03,  3.29s/trial, best loss=?]100%|██████████| 54/54 [02:54<00:00, 174.96s/trial, best loss: -3143.0184291784662]100%|██████████| 54/54 [02:54<00:00,  3.24s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 28 since program start
2020-12-20 06:18:44.257145
Found saved Trials! Loading...
Rerunning from 54 trials to add another one.
 98%|█████████▊| 54/55 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 54/55 [00:00<00:00, 2518.32trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 54/55 [00:00<00:00, 2499.42trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 54/55 [01:23<00:01,  1.55s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 54/55 [01:23<00:01,  1.55s/trial, best loss=?]                                                               [06:20:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 54/55 [01:25<00:01,  1.58s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 54/55 [01:43<00:01,  1.92s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               True
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 54/55 [01:44<00:01,  1.93s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 54/55 [01:48<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 54/55 [01:48<00:02,  2.01s/trial, best loss=?]                                                               [06:20:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 54/55 [01:50<00:02,  2.04s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 54/55 [02:03<00:02,  2.28s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               True
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 54/55 [02:03<00:02,  2.30s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 54/55 [02:07<00:02,  2.37s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 54/55 [02:07<00:02,  2.37s/trial, best loss=?]                                                               [06:20:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 54/55 [02:09<00:02,  2.40s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 54/55 [02:24<00:02,  2.68s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               True
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 54/55 [02:25<00:02,  2.69s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 54/55 [02:29<00:02,  2.77s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 54/55 [02:29<00:02,  2.77s/trial, best loss=?]                                                               [06:21:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 54/55 [02:31<00:02,  2.81s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 54/55 [02:46<00:03,  3.07s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               True
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 54/55 [02:46<00:03,  3.09s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 54/55 [02:51<00:03,  3.17s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 54/55 [02:51<00:03,  3.17s/trial, best loss=?]                                                               [06:21:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 54/55 [02:53<00:03,  3.21s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 54/55 [03:05<00:03,  3.44s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               True
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 54/55 [03:06<00:03,  3.46s/trial, best loss=?]100%|██████████| 55/55 [03:07<00:00, 187.54s/trial, best loss: -3143.0184291784662]100%|██████████| 55/55 [03:07<00:00,  3.41s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 29 since program start
2020-12-20 06:21:51.853435
Found saved Trials! Loading...
Rerunning from 55 trials to add another one.
 98%|█████████▊| 55/56 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 55/56 [00:00<00:00, 2591.43trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 55/56 [00:00<00:00, 2572.65trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 55/56 [01:24<00:01,  1.54s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 55/56 [01:24<00:01,  1.54s/trial, best loss=?]                                                               [06:23:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 55/56 [01:26<00:01,  1.57s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 55/56 [01:32<00:01,  1.69s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               True
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 55/56 [01:34<00:01,  1.71s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 55/56 [01:38<00:01,  1.78s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 55/56 [01:38<00:01,  1.78s/trial, best loss=?]                                                               [06:23:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 55/56 [01:39<00:01,  1.81s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 55/56 [01:45<00:01,  1.92s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               True
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 55/56 [01:46<00:01,  1.94s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 55/56 [01:50<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 55/56 [01:50<00:02,  2.01s/trial, best loss=?]                                                               [06:23:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 55/56 [01:52<00:02,  2.04s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 55/56 [01:58<00:02,  2.15s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               True
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 55/56 [01:59<00:02,  2.17s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 55/56 [02:03<00:02,  2.25s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 55/56 [02:03<00:02,  2.25s/trial, best loss=?]                                                               [06:23:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 55/56 [02:05<00:02,  2.28s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 55/56 [02:11<00:02,  2.39s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               True
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 55/56 [02:12<00:02,  2.41s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 55/56 [02:17<00:02,  2.49s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 55/56 [02:17<00:02,  2.49s/trial, best loss=?]                                                               [06:24:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 55/56 [02:18<00:02,  2.52s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 55/56 [02:25<00:02,  2.64s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               True
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 55/56 [02:26<00:02,  2.67s/trial, best loss=?]100%|██████████| 56/56 [02:27<00:00, 147.26s/trial, best loss: -3143.0184291784662]100%|██████████| 56/56 [02:27<00:00,  2.63s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 30 since program start
2020-12-20 06:24:19.168757
Found saved Trials! Loading...
Rerunning from 56 trials to add another one.
 98%|█████████▊| 56/57 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 56/57 [00:00<00:00, 2576.52trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 56/57 [00:00<00:00, 2557.95trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 56/57 [01:25<00:01,  1.53s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 56/57 [01:25<00:01,  1.53s/trial, best loss=?]                                                               [06:25:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 56/57 [01:28<00:01,  1.58s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 56/57 [07:21<00:07,  7.89s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               True
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 56/57 [07:32<00:08,  8.08s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 56/57 [07:36<00:08,  8.16s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 56/57 [07:36<00:08,  8.16s/trial, best loss=?]                                                               [06:31:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 56/57 [07:38<00:08,  8.19s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 56/57 [12:53<00:13, 13.80s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               True
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 56/57 [13:04<00:14, 14.00s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 56/57 [13:08<00:14, 14.08s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 56/57 [13:08<00:14, 14.08s/trial, best loss=?]                                                               [06:37:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 56/57 [13:10<00:14, 14.11s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 56/57 [18:29<00:19, 19.81s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               True
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 56/57 [18:41<00:20, 20.03s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 56/57 [18:45<00:20, 20.10s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 56/57 [18:45<00:20, 20.10s/trial, best loss=?]                                                               [06:43:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 56/57 [18:48<00:20, 20.14s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 56/57 [24:27<00:26, 26.21s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               True
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 56/57 [24:39<00:26, 26.43s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 56/57 [24:44<00:26, 26.51s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 56/57 [24:44<00:26, 26.51s/trial, best loss=?]                                                               [06:49:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 56/57 [24:46<00:26, 26.55s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 56/57 [30:33<00:32, 32.74s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               True
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 56/57 [30:47<00:32, 32.99s/trial, best loss=?]100%|██████████| 57/57 [30:48<00:00, 1848.19s/trial, best loss: -3143.0184291784662]100%|██████████| 57/57 [30:48<00:00, 32.42s/trial, best loss: -3143.0184291784662]  
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 31 since program start
2020-12-20 06:55:07.461592
Found saved Trials! Loading...
Rerunning from 57 trials to add another one.
 98%|█████████▊| 57/58 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 57/58 [00:00<00:00, 2706.16trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 57/58 [00:00<00:00, 2682.23trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 57/58 [01:23<00:01,  1.46s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 57/58 [01:23<00:01,  1.46s/trial, best loss=?]                                                               [06:56:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 57/58 [01:25<00:01,  1.49s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 57/58 [01:27<00:01,  1.54s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               True
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 57/58 [01:28<00:01,  1.55s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 57/58 [01:32<00:01,  1.62s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 57/58 [01:32<00:01,  1.62s/trial, best loss=?]                                                               [06:56:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 57/58 [01:33<00:01,  1.64s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 57/58 [01:35<00:01,  1.68s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               True
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 57/58 [01:36<00:01,  1.69s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 57/58 [01:40<00:01,  1.76s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 57/58 [01:40<00:01,  1.76s/trial, best loss=?]                                                               [06:56:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 57/58 [01:41<00:01,  1.79s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 57/58 [01:44<00:01,  1.83s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               True
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 57/58 [01:45<00:01,  1.84s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 57/58 [01:49<00:01,  1.92s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 57/58 [01:49<00:01,  1.92s/trial, best loss=?]                                                               [06:56:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 57/58 [01:50<00:01,  1.94s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 57/58 [01:53<00:01,  1.98s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               True
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 57/58 [01:53<00:01,  2.00s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 57/58 [01:58<00:02,  2.07s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 57/58 [01:58<00:02,  2.07s/trial, best loss=?]                                                               [06:57:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 57/58 [01:59<00:02,  2.10s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 57/58 [02:02<00:02,  2.14s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               True
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 57/58 [02:02<00:02,  2.16s/trial, best loss=?]100%|██████████| 58/58 [02:03<00:00, 123.55s/trial, best loss: -3143.0184291784662]100%|██████████| 58/58 [02:03<00:00,  2.13s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 32 since program start
2020-12-20 06:57:11.065675
Found saved Trials! Loading...
Rerunning from 58 trials to add another one.
 98%|█████████▊| 58/59 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 58/59 [00:00<00:00, 2714.15trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 58/59 [00:00<00:00, 2694.25trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 58/59 [01:24<00:01,  1.45s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 58/59 [01:24<00:01,  1.45s/trial, best loss=?]                                                               [06:58:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 58/59 [01:25<00:01,  1.48s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 58/59 [01:32<00:01,  1.59s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               True
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 58/59 [01:33<00:01,  1.61s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 58/59 [01:37<00:01,  1.68s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 58/59 [01:37<00:01,  1.68s/trial, best loss=?]                                                               [06:58:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 58/59 [01:38<00:01,  1.70s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 58/59 [01:44<00:01,  1.81s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               True
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 58/59 [01:46<00:01,  1.83s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 58/59 [01:50<00:01,  1.90s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 58/59 [01:50<00:01,  1.90s/trial, best loss=?]                                                               [06:59:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 58/59 [01:51<00:01,  1.92s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 58/59 [01:57<00:02,  2.03s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               True
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 58/59 [01:59<00:02,  2.05s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 58/59 [02:03<00:02,  2.12s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 58/59 [02:03<00:02,  2.12s/trial, best loss=?]                                                               [06:59:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 58/59 [02:04<00:02,  2.15s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 58/59 [02:11<00:02,  2.26s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               True
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 58/59 [02:12<00:02,  2.28s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 58/59 [02:16<00:02,  2.36s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 58/59 [02:16<00:02,  2.36s/trial, best loss=?]                                                               [06:59:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 58/59 [02:18<00:02,  2.39s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 58/59 [02:24<00:02,  2.50s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               True
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 58/59 [02:26<00:02,  2.52s/trial, best loss=?]100%|██████████| 59/59 [02:27<00:00, 147.07s/trial, best loss: -3143.0184291784662]100%|██████████| 59/59 [02:27<00:00,  2.49s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 33 since program start
2020-12-20 06:59:38.185874
Found saved Trials! Loading...
Rerunning from 59 trials to add another one.
 98%|█████████▊| 59/60 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 59/60 [00:00<00:00, 2678.67trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 59/60 [00:00<00:00, 2659.07trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 59/60 [01:24<00:01,  1.43s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 59/60 [01:24<00:01,  1.43s/trial, best loss=?]                                                               [07:01:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 59/60 [01:26<00:01,  1.46s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 59/60 [02:24<00:02,  2.45s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               True
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 59/60 [02:26<00:02,  2.49s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 59/60 [02:30<00:02,  2.56s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 59/60 [02:30<00:02,  2.56s/trial, best loss=?]                                                               [07:02:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 59/60 [02:32<00:02,  2.59s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 59/60 [03:26<00:03,  3.50s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               True
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 59/60 [03:29<00:03,  3.55s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 59/60 [03:33<00:03,  3.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 59/60 [03:33<00:03,  3.61s/trial, best loss=?]                                                               [07:03:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 59/60 [03:34<00:03,  3.64s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 59/60 [04:29<00:04,  4.57s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               True
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 59/60 [04:32<00:04,  4.63s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 59/60 [04:37<00:04,  4.70s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 59/60 [04:37<00:04,  4.70s/trial, best loss=?]                                                               [07:04:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 59/60 [04:38<00:04,  4.73s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 59/60 [05:35<00:05,  5.68s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               True
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 59/60 [05:38<00:05,  5.74s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 59/60 [05:42<00:05,  5.81s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 59/60 [05:42<00:05,  5.81s/trial, best loss=?]                                                               [07:05:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 59/60 [05:44<00:05,  5.84s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 59/60 [06:41<00:06,  6.80s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               True
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 59/60 [06:44<00:06,  6.86s/trial, best loss=?]100%|██████████| 60/60 [06:45<00:00, 405.58s/trial, best loss: -3143.0184291784662]100%|██████████| 60/60 [06:45<00:00,  6.76s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 34 since program start
2020-12-20 07:06:23.872530
Found saved Trials! Loading...
Rerunning from 60 trials to add another one.
 98%|█████████▊| 60/61 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 60/61 [00:00<00:00, 2814.91trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 60/61 [00:00<00:00, 2794.31trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 60/61 [01:24<00:01,  1.40s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 60/61 [01:24<00:01,  1.40s/trial, best loss=?]                                                               [07:07:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 60/61 [01:25<00:01,  1.43s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 60/61 [01:31<00:01,  1.53s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               True
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 60/61 [01:32<00:01,  1.55s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 60/61 [01:36<00:01,  1.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 60/61 [01:36<00:01,  1.61s/trial, best loss=?]                                                               [07:08:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 60/61 [01:38<00:01,  1.64s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 60/61 [01:43<00:01,  1.73s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               True
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 60/61 [01:45<00:01,  1.75s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 60/61 [01:48<00:01,  1.82s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 60/61 [01:48<00:01,  1.82s/trial, best loss=?]                                                               [07:08:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 60/61 [01:50<00:01,  1.84s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 60/61 [01:55<00:01,  1.93s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               True
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 60/61 [01:57<00:01,  1.95s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 60/61 [02:01<00:02,  2.03s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 60/61 [02:01<00:02,  2.03s/trial, best loss=?]                                                               [07:08:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 60/61 [02:03<00:02,  2.05s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 60/61 [02:08<00:02,  2.15s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               True
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 60/61 [02:10<00:02,  2.17s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 60/61 [02:14<00:02,  2.24s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 60/61 [02:14<00:02,  2.24s/trial, best loss=?]                                                               [07:08:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 60/61 [02:16<00:02,  2.27s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 60/61 [02:21<00:02,  2.37s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               True
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 60/61 [02:23<00:02,  2.39s/trial, best loss=?]100%|██████████| 61/61 [02:24<00:00, 144.06s/trial, best loss: -3143.0184291784662]100%|██████████| 61/61 [02:24<00:00,  2.36s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 35 since program start
2020-12-20 07:08:47.986593
Found saved Trials! Loading...
Rerunning from 61 trials to add another one.
 98%|█████████▊| 61/62 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 61/62 [00:00<00:00, 2768.91trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 61/62 [00:00<00:00, 2748.77trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 61/62 [01:24<00:01,  1.38s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 61/62 [01:24<00:01,  1.38s/trial, best loss=?]                                                               [07:10:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 61/62 [01:26<00:01,  1.42s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 61/62 [01:41<00:01,  1.67s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               True
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 61/62 [01:42<00:01,  1.68s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 61/62 [01:46<00:01,  1.74s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 61/62 [01:46<00:01,  1.74s/trial, best loss=?]                                                               [07:10:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 61/62 [01:48<00:01,  1.77s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 61/62 [02:01<00:01,  1.99s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               True
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 61/62 [02:02<00:02,  2.01s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 61/62 [02:06<00:02,  2.07s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 61/62 [02:06<00:02,  2.07s/trial, best loss=?]                                                               [07:10:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 61/62 [02:08<00:02,  2.10s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 61/62 [02:22<00:02,  2.33s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               True
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 61/62 [02:23<00:02,  2.35s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 61/62 [02:27<00:02,  2.42s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 61/62 [02:27<00:02,  2.42s/trial, best loss=?]                                                               [07:11:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 61/62 [02:29<00:02,  2.45s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 61/62 [02:44<00:02,  2.69s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               True
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 61/62 [02:45<00:02,  2.71s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 61/62 [02:49<00:02,  2.78s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 61/62 [02:49<00:02,  2.78s/trial, best loss=?]                                                               [07:11:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 61/62 [02:51<00:02,  2.81s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 61/62 [03:05<00:03,  3.04s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               True
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 61/62 [03:06<00:03,  3.05s/trial, best loss=?]100%|██████████| 62/62 [03:06<00:00, 186.82s/trial, best loss: -3143.0184291784662]100%|██████████| 62/62 [03:06<00:00,  3.01s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 36 since program start
2020-12-20 07:11:54.862682
Found saved Trials! Loading...
Rerunning from 62 trials to add another one.
 98%|█████████▊| 62/63 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 62/63 [00:00<00:00, 2838.60trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 62/63 [00:00<00:00, 2817.56trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 62/63 [01:24<00:01,  1.36s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 62/63 [01:24<00:01,  1.36s/trial, best loss=?]                                                               [07:13:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 62/63 [01:26<00:01,  1.39s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 62/63 [01:48<00:01,  1.75s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               True
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 62/63 [01:51<00:01,  1.80s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 62/63 [01:55<00:01,  1.87s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 62/63 [01:55<00:01,  1.87s/trial, best loss=?]                                                               [07:13:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 62/63 [01:57<00:01,  1.89s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 62/63 [02:17<00:02,  2.22s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               True
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 62/63 [02:20<00:02,  2.27s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 62/63 [02:24<00:02,  2.34s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 62/63 [02:24<00:02,  2.34s/trial, best loss=?]                                                               [07:14:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 62/63 [02:26<00:02,  2.36s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 62/63 [02:47<00:02,  2.70s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               True
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 62/63 [02:50<00:02,  2.75s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 62/63 [02:54<00:02,  2.82s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 62/63 [02:54<00:02,  2.82s/trial, best loss=?]                                                               [07:14:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 62/63 [02:56<00:02,  2.85s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 62/63 [03:18<00:03,  3.20s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               True
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 62/63 [03:21<00:03,  3.25s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 62/63 [03:26<00:03,  3.32s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 62/63 [03:26<00:03,  3.32s/trial, best loss=?]                                                               [07:15:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 62/63 [03:27<00:03,  3.35s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 62/63 [03:49<00:03,  3.71s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               True
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 62/63 [03:53<00:03,  3.77s/trial, best loss=?]100%|██████████| 63/63 [03:54<00:00, 234.13s/trial, best loss: -3143.0184291784662]100%|██████████| 63/63 [03:54<00:00,  3.72s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 37 since program start
2020-12-20 07:15:49.106291
Found saved Trials! Loading...
Rerunning from 63 trials to add another one.
 98%|█████████▊| 63/64 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 63/64 [00:00<00:00, 2870.94trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 63/64 [00:00<00:00, 2850.01trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 63/64 [01:24<00:01,  1.35s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.3, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 63/64 [01:24<00:01,  1.35s/trial, best loss=?]                                                               [07:17:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 63/64 [01:28<00:01,  1.40s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 63/64 [03:54<00:03,  3.72s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               True
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 63/64 [03:57<00:03,  3.76s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 63/64 [04:01<00:03,  3.83s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 63/64 [04:01<00:03,  3.83s/trial, best loss=?]                                                               [07:19:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 63/64 [04:03<00:03,  3.87s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 63/64 [06:09<00:05,  5.87s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               True
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 63/64 [06:12<00:05,  5.92s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 63/64 [06:16<00:05,  5.98s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 63/64 [06:16<00:05,  5.98s/trial, best loss=?]                                                               [07:22:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 63/64 [06:19<00:06,  6.02s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 63/64 [08:29<00:08,  8.09s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               True
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 63/64 [08:32<00:08,  8.14s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 63/64 [08:36<00:08,  8.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 63/64 [08:36<00:08,  8.21s/trial, best loss=?]                                                               [07:24:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 63/64 [08:39<00:08,  8.25s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 63/64 [10:57<00:10, 10.44s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               True
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 63/64 [11:01<00:10, 10.50s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 63/64 [11:05<00:10, 10.57s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 63/64 [11:05<00:10, 10.57s/trial, best loss=?]                                                               [07:26:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 63/64 [11:08<00:10, 10.61s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 63/64 [13:28<00:12, 12.84s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               True
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 63/64 [13:32<00:12, 12.90s/trial, best loss=?]100%|██████████| 64/64 [13:33<00:00, 813.49s/trial, best loss: -3143.0184291784662]100%|██████████| 64/64 [13:33<00:00, 12.71s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 38 since program start
2020-12-20 07:29:22.656556
Found saved Trials! Loading...
Rerunning from 64 trials to add another one.
 98%|█████████▊| 64/65 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 64/65 [00:00<00:00, 2945.41trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 64/65 [00:00<00:00, 2923.85trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 64/65 [01:24<00:01,  1.31s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 64/65 [01:24<00:01,  1.31s/trial, best loss=?]                                                               [07:30:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 64/65 [01:25<00:01,  1.34s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 64/65 [01:27<00:01,  1.36s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               True
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 64/65 [01:27<00:01,  1.37s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 64/65 [01:31<00:01,  1.44s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 64/65 [01:31<00:01,  1.44s/trial, best loss=?]                                                               [07:30:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 64/65 [01:33<00:01,  1.46s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 64/65 [01:34<00:01,  1.48s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               True
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 64/65 [01:35<00:01,  1.49s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 64/65 [01:39<00:01,  1.55s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 64/65 [01:39<00:01,  1.55s/trial, best loss=?]                                                               [07:31:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 64/65 [01:40<00:01,  1.58s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 64/65 [01:42<00:01,  1.60s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               True
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 64/65 [01:42<00:01,  1.61s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 64/65 [01:47<00:01,  1.67s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 64/65 [01:47<00:01,  1.67s/trial, best loss=?]                                                               [07:31:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 64/65 [01:48<00:01,  1.70s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 64/65 [01:50<00:01,  1.72s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               True
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 64/65 [01:50<00:01,  1.73s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 64/65 [01:55<00:01,  1.80s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 64/65 [01:55<00:01,  1.80s/trial, best loss=?]                                                               [07:31:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 64/65 [01:56<00:01,  1.83s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 64/65 [01:58<00:01,  1.85s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               True
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 64/65 [01:59<00:01,  1.86s/trial, best loss=?]100%|██████████| 65/65 [01:59<00:00, 119.75s/trial, best loss: -3143.0184291784662]100%|██████████| 65/65 [01:59<00:00,  1.84s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 39 since program start
2020-12-20 07:31:22.462098
Found saved Trials! Loading...
Rerunning from 65 trials to add another one.
 98%|█████████▊| 65/66 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 98%|█████████▊| 65/66 [00:00<00:00, 2997.71trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 98%|█████████▊| 65/66 [00:00<00:00, 2975.82trial/s, best loss=?]                                                                 Model used for fitting:
 98%|█████████▊| 65/66 [01:25<00:01,  1.31s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 65/66 [01:25<00:01,  1.31s/trial, best loss=?]                                                               [07:32:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 65/66 [01:26<00:01,  1.34s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 65/66 [01:33<00:01,  1.43s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               (259530, 139)
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               True
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 65/66 [01:34<00:01,  1.45s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 65/66 [01:38<00:01,  1.51s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 65/66 [01:38<00:01,  1.51s/trial, best loss=?]                                                               [07:33:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 65/66 [01:39<00:01,  1.54s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 65/66 [01:45<00:01,  1.62s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               (271631, 139)
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               True
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 65/66 [01:46<00:01,  1.64s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 65/66 [01:50<00:01,  1.70s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 65/66 [01:50<00:01,  1.70s/trial, best loss=?]                                                               [07:33:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 65/66 [01:52<00:01,  1.72s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 65/66 [01:57<00:01,  1.81s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               (286101, 139)
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               True
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 65/66 [01:59<00:01,  1.83s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 65/66 [02:03<00:01,  1.90s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 65/66 [02:03<00:01,  1.90s/trial, best loss=?]                                                               [07:33:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 65/66 [02:04<00:01,  1.92s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 65/66 [02:10<00:02,  2.01s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               (289002, 139)
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               True
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 65/66 [02:12<00:02,  2.03s/trial, best loss=?]                                                               Model used for fitting:
 98%|█████████▊| 65/66 [02:16<00:02,  2.10s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 65/66 [02:16<00:02,  2.10s/trial, best loss=?]                                                               [07:33:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 65/66 [02:18<00:02,  2.13s/trial, best loss=?]                                                               predict called
 98%|█████████▊| 65/66 [02:24<00:02,  2.22s/trial, best loss=?]                                                               Type of X:
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               Shape of X:
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               (323071, 139)
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               Type of y:
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               model fitted ?
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               True
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]                                                               y is not None
 98%|█████████▊| 65/66 [02:25<00:02,  2.24s/trial, best loss=?]100%|██████████| 66/66 [02:26<00:00, 146.44s/trial, best loss: -3143.0184291784662]100%|██████████| 66/66 [02:26<00:00,  2.22s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 40 since program start
2020-12-20 07:33:49.017633
Found saved Trials! Loading...
Rerunning from 66 trials to add another one.
 99%|█████████▊| 66/67 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 66/67 [00:00<00:00, 3057.58trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 66/67 [00:00<00:00, 3034.99trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 66/67 [01:23<00:01,  1.26s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 66/67 [01:23<00:01,  1.26s/trial, best loss=?]                                                               [07:35:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 66/67 [01:24<00:01,  1.29s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 66/67 [01:31<00:01,  1.38s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               True
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 66/67 [01:32<00:01,  1.40s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 66/67 [01:36<00:01,  1.46s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 66/67 [01:36<00:01,  1.46s/trial, best loss=?]                                                               [07:35:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 66/67 [01:37<00:01,  1.48s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 66/67 [01:43<00:01,  1.57s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               True
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 66/67 [01:44<00:01,  1.59s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 66/67 [01:48<00:01,  1.65s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 66/67 [01:48<00:01,  1.65s/trial, best loss=?]                                                               [07:35:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 66/67 [01:50<00:01,  1.67s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 66/67 [01:55<00:01,  1.75s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               True
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 66/67 [01:57<00:01,  1.77s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 66/67 [02:01<00:01,  1.84s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 66/67 [02:01<00:01,  1.84s/trial, best loss=?]                                                               [07:35:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 66/67 [02:02<00:01,  1.86s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 66/67 [02:08<00:01,  1.95s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               True
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 66/67 [02:10<00:01,  1.97s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 66/67 [02:14<00:02,  2.04s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 66/67 [02:14<00:02,  2.04s/trial, best loss=?]                                                               [07:36:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 66/67 [02:16<00:02,  2.07s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 66/67 [02:22<00:02,  2.16s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               True
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 66/67 [02:23<00:02,  2.18s/trial, best loss=?]100%|██████████| 67/67 [02:24<00:00, 144.47s/trial, best loss: -3143.0184291784662]100%|██████████| 67/67 [02:24<00:00,  2.16s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 41 since program start
2020-12-20 07:36:13.546684
Found saved Trials! Loading...
Rerunning from 67 trials to add another one.
 99%|█████████▊| 67/68 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 67/68 [00:00<00:00, 3084.55trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 67/68 [00:00<00:00, 3062.44trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 67/68 [01:24<00:01,  1.25s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 67/68 [01:24<00:01,  1.25s/trial, best loss=?]                                                               [07:37:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 67/68 [01:25<00:01,  1.28s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 67/68 [01:32<00:01,  1.37s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               True
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 67/68 [01:33<00:01,  1.39s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 67/68 [01:37<00:01,  1.45s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 67/68 [01:37<00:01,  1.45s/trial, best loss=?]                                                               [07:37:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 67/68 [01:38<00:01,  1.47s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 67/68 [01:44<00:01,  1.55s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               True
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 67/68 [01:45<00:01,  1.57s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 67/68 [01:49<00:01,  1.63s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 67/68 [01:49<00:01,  1.63s/trial, best loss=?]                                                               [07:38:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 67/68 [01:50<00:01,  1.65s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 67/68 [01:56<00:01,  1.74s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               True
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 67/68 [01:57<00:01,  1.76s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 67/68 [02:01<00:01,  1.82s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 67/68 [02:01<00:01,  1.82s/trial, best loss=?]                                                               [07:38:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 67/68 [02:03<00:01,  1.84s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 67/68 [02:09<00:01,  1.93s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               True
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 67/68 [02:10<00:01,  1.95s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 67/68 [02:14<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 67/68 [02:14<00:02,  2.01s/trial, best loss=?]                                                               [07:38:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 67/68 [02:16<00:02,  2.04s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 67/68 [02:22<00:02,  2.13s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               True
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 67/68 [02:24<00:02,  2.15s/trial, best loss=?]100%|██████████| 68/68 [02:24<00:00, 144.70s/trial, best loss: -3143.0184291784662]100%|██████████| 68/68 [02:24<00:00,  2.13s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 42 since program start
2020-12-20 07:38:38.306946
Found saved Trials! Loading...
Rerunning from 68 trials to add another one.
 99%|█████████▊| 68/69 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 68/69 [00:00<00:00, 3017.17trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 68/69 [00:00<00:00, 2992.60trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 68/69 [01:24<00:01,  1.24s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.04, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 68/69 [01:24<00:01,  1.24s/trial, best loss=?]                                                               [07:40:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 68/69 [01:25<00:01,  1.26s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 68/69 [01:32<00:01,  1.36s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               True
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 68/69 [01:33<00:01,  1.37s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 68/69 [01:37<00:01,  1.43s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 68/69 [01:37<00:01,  1.43s/trial, best loss=?]                                                               [07:40:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 68/69 [01:38<00:01,  1.45s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 68/69 [01:44<00:01,  1.54s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               True
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 68/69 [01:45<00:01,  1.55s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 68/69 [01:49<00:01,  1.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 68/69 [01:49<00:01,  1.61s/trial, best loss=?]                                                               [07:40:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 68/69 [01:50<00:01,  1.63s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 68/69 [01:56<00:01,  1.72s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               True
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 68/69 [01:57<00:01,  1.73s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 68/69 [02:02<00:01,  1.80s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 68/69 [02:02<00:01,  1.80s/trial, best loss=?]                                                               [07:40:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 68/69 [02:03<00:01,  1.82s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 68/69 [02:09<00:01,  1.91s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               True
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 68/69 [02:10<00:01,  1.93s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 68/69 [02:15<00:01,  1.99s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.04, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 68/69 [02:15<00:01,  1.99s/trial, best loss=?]                                                               [07:40:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 68/69 [02:16<00:02,  2.01s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 68/69 [02:23<00:02,  2.10s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               True
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 68/69 [02:24<00:02,  2.13s/trial, best loss=?]100%|██████████| 69/69 [02:25<00:00, 145.16s/trial, best loss: -3143.0184291784662]100%|██████████| 69/69 [02:25<00:00,  2.10s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 43 since program start
2020-12-20 07:41:03.576864
Found saved Trials! Loading...
Rerunning from 69 trials to add another one.
 99%|█████████▊| 69/70 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 69/70 [00:00<00:00, 3199.92trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 69/70 [00:00<00:00, 3177.26trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 69/70 [01:23<00:01,  1.21s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 69/70 [01:23<00:01,  1.21s/trial, best loss=?]                                                               [07:42:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 69/70 [01:25<00:01,  1.24s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 69/70 [01:32<00:01,  1.33s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               True
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 69/70 [01:33<00:01,  1.35s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 69/70 [01:37<00:01,  1.41s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 69/70 [01:37<00:01,  1.41s/trial, best loss=?]                                                               [07:42:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 69/70 [01:38<00:01,  1.43s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 69/70 [01:44<00:01,  1.51s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               True
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 69/70 [01:45<00:01,  1.53s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 69/70 [01:49<00:01,  1.59s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 69/70 [01:49<00:01,  1.59s/trial, best loss=?]                                                               [07:42:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 69/70 [01:50<00:01,  1.61s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 69/70 [01:56<00:01,  1.69s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               True
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 69/70 [01:58<00:01,  1.71s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 69/70 [02:02<00:01,  1.77s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 69/70 [02:02<00:01,  1.77s/trial, best loss=?]                                                               [07:43:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 69/70 [02:03<00:01,  1.79s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 69/70 [02:09<00:01,  1.88s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               True
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 69/70 [02:11<00:01,  1.90s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 69/70 [02:15<00:01,  1.96s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 69/70 [02:15<00:01,  1.96s/trial, best loss=?]                                                               [07:43:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 69/70 [02:17<00:01,  1.99s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 69/70 [02:23<00:02,  2.07s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               True
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 69/70 [02:24<00:02,  2.10s/trial, best loss=?]100%|██████████| 70/70 [02:25<00:00, 145.19s/trial, best loss: -3143.0184291784662]100%|██████████| 70/70 [02:25<00:00,  2.07s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 44 since program start
2020-12-20 07:43:28.824925
Found saved Trials! Loading...
Rerunning from 70 trials to add another one.
 99%|█████████▊| 70/71 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 70/71 [00:00<00:00, 3200.08trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 70/71 [00:00<00:00, 3176.82trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 70/71 [01:23<00:01,  1.20s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 70/71 [01:23<00:01,  1.20s/trial, best loss=?]                                                               [07:44:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 70/71 [01:25<00:01,  1.22s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 70/71 [01:31<00:01,  1.31s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               True
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 70/71 [01:32<00:01,  1.32s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 70/71 [01:36<00:01,  1.38s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 70/71 [01:36<00:01,  1.38s/trial, best loss=?]                                                               [07:45:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 70/71 [01:38<00:01,  1.40s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 70/71 [01:43<00:01,  1.48s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               True
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 70/71 [01:44<00:01,  1.50s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 70/71 [01:48<00:01,  1.55s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 70/71 [01:48<00:01,  1.55s/trial, best loss=?]                                                               [07:45:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 70/71 [01:50<00:01,  1.57s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 70/71 [01:55<00:01,  1.65s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               True
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 70/71 [01:56<00:01,  1.67s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 70/71 [02:01<00:01,  1.73s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 70/71 [02:01<00:01,  1.73s/trial, best loss=?]                                                               [07:45:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 70/71 [02:02<00:01,  1.75s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 70/71 [02:08<00:01,  1.83s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               True
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 70/71 [02:09<00:01,  1.85s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 70/71 [02:14<00:01,  1.92s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 70/71 [02:14<00:01,  1.92s/trial, best loss=?]                                                               [07:45:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 70/71 [02:15<00:01,  1.94s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 70/71 [02:21<00:02,  2.02s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               True
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 70/71 [02:22<00:02,  2.04s/trial, best loss=?]100%|██████████| 71/71 [02:23<00:00, 143.54s/trial, best loss: -3143.0184291784662]100%|██████████| 71/71 [02:23<00:00,  2.02s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 45 since program start
2020-12-20 07:45:52.420820
Found saved Trials! Loading...
Rerunning from 71 trials to add another one.
 99%|█████████▊| 71/72 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 71/72 [00:00<00:00, 3271.47trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 71/72 [00:00<00:00, 3247.21trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 71/72 [01:23<00:01,  1.18s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=8,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 71/72 [01:23<00:01,  1.18s/trial, best loss=?]                                                               [07:47:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 71/72 [01:25<00:01,  1.20s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 71/72 [01:31<00:01,  1.29s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               True
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 71/72 [01:32<00:01,  1.30s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 71/72 [01:36<00:01,  1.36s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 71/72 [01:36<00:01,  1.36s/trial, best loss=?]                                                               [07:47:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 71/72 [01:37<00:01,  1.38s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 71/72 [01:43<00:01,  1.45s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               True
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 71/72 [01:44<00:01,  1.47s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 71/72 [01:48<00:01,  1.52s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 71/72 [01:48<00:01,  1.52s/trial, best loss=?]                                                               [07:47:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 71/72 [01:49<00:01,  1.54s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 71/72 [01:55<00:01,  1.62s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               True
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 71/72 [01:56<00:01,  1.64s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 71/72 [02:00<00:01,  1.70s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 71/72 [02:00<00:01,  1.70s/trial, best loss=?]                                                               [07:47:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 71/72 [02:02<00:01,  1.72s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 71/72 [02:07<00:01,  1.80s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               True
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 71/72 [02:08<00:01,  1.82s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 71/72 [02:13<00:01,  1.88s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=8,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 71/72 [02:13<00:01,  1.88s/trial, best loss=?]                                                               [07:48:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 71/72 [02:14<00:01,  1.90s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 71/72 [02:20<00:01,  1.98s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               True
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 71/72 [02:22<00:02,  2.00s/trial, best loss=?]100%|██████████| 72/72 [02:22<00:00, 142.82s/trial, best loss: -3143.0184291784662]100%|██████████| 72/72 [02:22<00:00,  1.98s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 46 since program start
2020-12-20 07:48:15.348465
Found saved Trials! Loading...
Rerunning from 72 trials to add another one.
 99%|█████████▊| 72/73 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 72/73 [00:00<00:00, 3277.62trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 72/73 [00:00<00:00, 3254.17trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 72/73 [01:24<00:01,  1.17s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 72/73 [01:24<00:01,  1.17s/trial, best loss=?]                                                               [07:49:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 72/73 [01:25<00:01,  1.19s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 72/73 [01:38<00:01,  1.36s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               True
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 72/73 [01:39<00:01,  1.38s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 72/73 [01:43<00:01,  1.44s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 72/73 [01:43<00:01,  1.44s/trial, best loss=?]                                                               [07:50:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 72/73 [01:44<00:01,  1.46s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 72/73 [01:55<00:01,  1.61s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               True
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 72/73 [01:57<00:01,  1.63s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 72/73 [02:01<00:01,  1.68s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 72/73 [02:01<00:01,  1.68s/trial, best loss=?]                                                               [07:50:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 72/73 [02:02<00:01,  1.70s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 72/73 [02:13<00:01,  1.86s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               True
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 72/73 [02:15<00:01,  1.88s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 72/73 [02:19<00:01,  1.94s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 72/73 [02:19<00:01,  1.94s/trial, best loss=?]                                                               [07:50:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 72/73 [02:21<00:01,  1.96s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 72/73 [02:32<00:02,  2.12s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               True
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 72/73 [02:33<00:02,  2.14s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 72/73 [02:38<00:02,  2.20s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 72/73 [02:38<00:02,  2.20s/trial, best loss=?]                                                               [07:50:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 72/73 [02:40<00:02,  2.22s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 72/73 [02:51<00:02,  2.38s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               True
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 72/73 [02:53<00:02,  2.40s/trial, best loss=?]100%|██████████| 73/73 [02:53<00:00, 173.75s/trial, best loss: -3143.0184291784662]100%|██████████| 73/73 [02:53<00:00,  2.38s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 47 since program start
2020-12-20 07:51:09.153806
Found saved Trials! Loading...
Rerunning from 73 trials to add another one.
 99%|█████████▊| 73/74 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 73/74 [00:00<00:00, 3283.09trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 73/74 [00:00<00:00, 3259.91trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 73/74 [01:24<00:01,  1.15s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 73/74 [01:24<00:01,  1.15s/trial, best loss=?]                                                               [07:52:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 73/74 [01:25<00:01,  1.17s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 73/74 [01:37<00:01,  1.34s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               True
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 73/74 [01:39<00:01,  1.36s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 73/74 [01:43<00:01,  1.41s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 73/74 [01:43<00:01,  1.41s/trial, best loss=?]                                                               [07:52:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 73/74 [01:44<00:01,  1.43s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 73/74 [01:55<00:01,  1.58s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               True
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 73/74 [01:56<00:01,  1.60s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 73/74 [02:00<00:01,  1.66s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 73/74 [02:00<00:01,  1.66s/trial, best loss=?]                                                               [07:53:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 73/74 [02:02<00:01,  1.68s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 73/74 [02:13<00:01,  1.83s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               True
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 73/74 [02:15<00:01,  1.85s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 73/74 [02:19<00:01,  1.91s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 73/74 [02:19<00:01,  1.91s/trial, best loss=?]                                                               [07:53:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 73/74 [02:21<00:01,  1.93s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 73/74 [02:32<00:02,  2.09s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               True
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 73/74 [02:33<00:02,  2.11s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 73/74 [02:38<00:02,  2.17s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 73/74 [02:38<00:02,  2.17s/trial, best loss=?]                                                               [07:53:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 73/74 [02:39<00:02,  2.19s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 73/74 [02:51<00:02,  2.35s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               True
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 73/74 [02:53<00:02,  2.37s/trial, best loss=?]100%|██████████| 74/74 [02:53<00:00, 173.70s/trial, best loss: -3143.0184291784662]100%|██████████| 74/74 [02:53<00:00,  2.35s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 48 since program start
2020-12-20 07:54:02.909187
Found saved Trials! Loading...
Rerunning from 74 trials to add another one.
 99%|█████████▊| 74/75 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 74/75 [00:00<00:00, 3375.70trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 74/75 [00:00<00:00, 3351.82trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 74/75 [01:23<00:01,  1.13s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 74/75 [01:23<00:01,  1.13s/trial, best loss=?]                                                               [07:55:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 74/75 [01:25<00:01,  1.16s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 74/75 [01:37<00:01,  1.32s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               True
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 74/75 [01:39<00:01,  1.34s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 74/75 [01:42<00:01,  1.39s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 74/75 [01:42<00:01,  1.39s/trial, best loss=?]                                                               [07:55:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 74/75 [01:44<00:01,  1.41s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 74/75 [01:55<00:01,  1.56s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               True
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 74/75 [02:00<00:01,  1.63s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 74/75 [02:00<00:01,  1.63s/trial, best loss=?]                                                               [07:56:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 74/75 [02:02<00:01,  1.65s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 74/75 [02:13<00:01,  1.80s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               True
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 74/75 [02:14<00:01,  1.82s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 74/75 [02:19<00:01,  1.88s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 74/75 [02:19<00:01,  1.88s/trial, best loss=?]                                                               [07:56:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 74/75 [02:20<00:01,  1.90s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 74/75 [02:31<00:02,  2.05s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               True
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 74/75 [02:33<00:02,  2.07s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 74/75 [02:37<00:02,  2.13s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 74/75 [02:37<00:02,  2.13s/trial, best loss=?]                                                               [07:56:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 74/75 [02:39<00:02,  2.15s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 74/75 [02:50<00:02,  2.31s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               True
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 74/75 [02:52<00:02,  2.33s/trial, best loss=?]100%|██████████| 75/75 [02:53<00:00, 173.15s/trial, best loss: -3143.0184291784662]100%|██████████| 75/75 [02:53<00:00,  2.31s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 49 since program start
2020-12-20 07:56:56.174012
Found saved Trials! Loading...
Rerunning from 75 trials to add another one.
 99%|█████████▊| 75/76 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 75/76 [00:00<00:00, 3464.23trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 75/76 [00:00<00:00, 3439.65trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 75/76 [01:24<00:01,  1.12s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 75/76 [01:24<00:01,  1.12s/trial, best loss=?]                                                               [07:58:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 75/76 [01:25<00:01,  1.14s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 75/76 [01:38<00:01,  1.31s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               True
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 75/76 [01:39<00:01,  1.32s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 75/76 [01:43<00:01,  1.38s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 75/76 [01:43<00:01,  1.38s/trial, best loss=?]                                                               [07:58:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 75/76 [01:44<00:01,  1.40s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 75/76 [01:55<00:01,  1.54s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               True
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 75/76 [01:57<00:01,  1.56s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 75/76 [02:01<00:01,  1.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 75/76 [02:01<00:01,  1.61s/trial, best loss=?]                                                               [07:58:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 75/76 [02:02<00:01,  1.63s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 75/76 [02:13<00:01,  1.78s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               True
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 75/76 [02:15<00:01,  1.80s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 75/76 [02:19<00:01,  1.86s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 75/76 [02:19<00:01,  1.86s/trial, best loss=?]                                                               [07:59:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 75/76 [02:21<00:01,  1.88s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 75/76 [02:32<00:02,  2.03s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               True
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 75/76 [02:33<00:02,  2.05s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 75/76 [02:38<00:02,  2.11s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 75/76 [02:38<00:02,  2.11s/trial, best loss=?]                                                               [07:59:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 75/76 [02:40<00:02,  2.14s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 75/76 [02:51<00:02,  2.29s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               True
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 75/76 [02:53<00:02,  2.31s/trial, best loss=?]100%|██████████| 76/76 [02:53<00:00, 173.84s/trial, best loss: -3143.0184291784662]100%|██████████| 76/76 [02:53<00:00,  2.29s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 50 since program start
2020-12-20 07:59:50.075156
Found saved Trials! Loading...
Rerunning from 76 trials to add another one.
 99%|█████████▊| 76/77 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 76/77 [00:00<00:00, 3474.34trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 76/77 [00:00<00:00, 3447.62trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 76/77 [01:23<00:01,  1.10s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 76/77 [01:23<00:01,  1.10s/trial, best loss=?]                                                               [08:01:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 76/77 [01:25<00:01,  1.12s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 76/77 [01:39<00:01,  1.31s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               True
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 76/77 [01:41<00:01,  1.33s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 76/77 [01:45<00:01,  1.38s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 76/77 [01:45<00:01,  1.38s/trial, best loss=?]                                                               [08:01:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 76/77 [01:46<00:01,  1.40s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 76/77 [01:59<00:01,  1.57s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               True
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 76/77 [02:00<00:01,  1.59s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 76/77 [02:04<00:01,  1.64s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 76/77 [02:04<00:01,  1.64s/trial, best loss=?]                                                               [08:01:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 76/77 [02:06<00:01,  1.66s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 76/77 [02:19<00:01,  1.84s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               True
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 76/77 [02:21<00:01,  1.86s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 76/77 [02:25<00:01,  1.91s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 76/77 [02:25<00:01,  1.91s/trial, best loss=?]                                                               [08:02:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 76/77 [02:26<00:01,  1.93s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 76/77 [02:40<00:02,  2.11s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               True
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 76/77 [02:41<00:02,  2.13s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 76/77 [02:46<00:02,  2.19s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 76/77 [02:46<00:02,  2.19s/trial, best loss=?]                                                               [08:02:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 76/77 [02:47<00:02,  2.21s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 76/77 [03:01<00:02,  2.39s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               True
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 76/77 [03:02<00:02,  2.41s/trial, best loss=?]100%|██████████| 77/77 [03:03<00:00, 183.60s/trial, best loss: -3143.0184291784662]100%|██████████| 77/77 [03:03<00:00,  2.38s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 51 since program start
2020-12-20 08:02:53.739045
Found saved Trials! Loading...
Rerunning from 77 trials to add another one.
 99%|█████████▊| 77/78 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 77/78 [00:00<00:00, 3505.04trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 77/78 [00:00<00:00, 3479.81trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 77/78 [01:23<00:01,  1.09s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.3, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 77/78 [01:23<00:01,  1.09s/trial, best loss=?]                                                               [08:04:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 77/78 [01:25<00:01,  1.11s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 77/78 [01:37<00:01,  1.27s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               True
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 77/78 [01:39<00:01,  1.29s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 77/78 [01:43<00:01,  1.34s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 77/78 [01:43<00:01,  1.34s/trial, best loss=?]                                                               [08:04:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 77/78 [01:44<00:01,  1.36s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 77/78 [01:55<00:01,  1.50s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               True
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 77/78 [01:56<00:01,  1.52s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 77/78 [02:00<00:01,  1.57s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 77/78 [02:00<00:01,  1.57s/trial, best loss=?]                                                               [08:04:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 77/78 [02:02<00:01,  1.59s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 77/78 [02:13<00:01,  1.74s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               True
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 77/78 [02:15<00:01,  1.75s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 77/78 [02:19<00:01,  1.81s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 77/78 [02:19<00:01,  1.81s/trial, best loss=?]                                                               [08:05:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 77/78 [02:21<00:01,  1.83s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 77/78 [02:32<00:01,  1.98s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               True
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 77/78 [02:33<00:01,  2.00s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 77/78 [02:38<00:02,  2.06s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 77/78 [02:38<00:02,  2.06s/trial, best loss=?]                                                               [08:05:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 77/78 [02:40<00:02,  2.08s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 77/78 [02:51<00:02,  2.23s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               True
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 77/78 [02:53<00:02,  2.25s/trial, best loss=?]100%|██████████| 78/78 [02:53<00:00, 173.84s/trial, best loss: -3143.0184291784662]100%|██████████| 78/78 [02:53<00:00,  2.23s/trial, best loss: -3143.0184291784662] 
Best model so far :
{'colsample_bytree': 3, 'features': 1, 'learning_rate': 3, 'max_depth': 0, 'n_estimators': 1, 'random_state': 0, 'subsample': 5, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 52 since program start
2020-12-20 08:05:47.699707
Found saved Trials! Loading...
Rerunning from 78 trials to add another one.
 99%|█████████▊| 78/79 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▊| 78/79 [00:00<00:00, 3600.54trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▊| 78/79 [00:00<00:00, 3574.42trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▊| 78/79 [01:24<00:01,  1.08s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 78/79 [01:24<00:01,  1.08s/trial, best loss=?]                                                               [08:07:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 78/79 [01:25<00:01,  1.10s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 78/79 [01:39<00:01,  1.27s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               True
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 78/79 [01:40<00:01,  1.29s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 78/79 [01:44<00:01,  1.34s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 78/79 [01:44<00:01,  1.34s/trial, best loss=?]                                                               [08:07:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 78/79 [01:45<00:01,  1.36s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 78/79 [01:57<00:01,  1.51s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               True
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 78/79 [01:59<00:01,  1.53s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 78/79 [02:03<00:01,  1.58s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 78/79 [02:03<00:01,  1.58s/trial, best loss=?]                                                               [08:07:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 78/79 [02:04<00:01,  1.60s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 78/79 [02:16<00:01,  1.75s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               True
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 78/79 [02:18<00:01,  1.77s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 78/79 [02:22<00:01,  1.83s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 78/79 [02:22<00:01,  1.83s/trial, best loss=?]                                                               [08:08:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 78/79 [02:24<00:01,  1.85s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 78/79 [02:36<00:02,  2.01s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               True
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 78/79 [02:38<00:02,  2.03s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▊| 78/79 [02:42<00:02,  2.08s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 78/79 [02:42<00:02,  2.08s/trial, best loss=?]                                                               [08:08:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 78/79 [02:44<00:02,  2.10s/trial, best loss=?]                                                               predict called
 99%|█████████▊| 78/79 [02:56<00:02,  2.26s/trial, best loss=?]                                                               Type of X:
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               Type of y:
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               True
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]                                                               y is not None
 99%|█████████▊| 78/79 [02:58<00:02,  2.28s/trial, best loss=?]100%|██████████| 79/79 [02:58<00:00, 178.67s/trial, best loss: -3265.602358202397]100%|██████████| 79/79 [02:58<00:00,  2.26s/trial, best loss: -3265.602358202397] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 1, 'random_state': 0, 'subsample': 6, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 53 since program start
2020-12-20 08:08:46.430334
Found saved Trials! Loading...
Rerunning from 79 trials to add another one.
 99%|█████████▉| 79/80 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 79/80 [00:00<00:00, 3583.83trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 79/80 [00:00<00:00, 3558.16trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 79/80 [01:25<00:01,  1.08s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 79/80 [01:25<00:01,  1.08s/trial, best loss=?]                                                               [08:10:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 79/80 [01:26<00:01,  1.10s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 79/80 [01:40<00:01,  1.27s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               True
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 79/80 [01:41<00:01,  1.29s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 79/80 [01:45<00:01,  1.34s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 79/80 [01:45<00:01,  1.34s/trial, best loss=?]                                                               [08:10:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 79/80 [01:47<00:01,  1.36s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 79/80 [01:58<00:01,  1.51s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               True
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 79/80 [02:00<00:01,  1.52s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 79/80 [02:04<00:01,  1.57s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 79/80 [02:04<00:01,  1.57s/trial, best loss=?]                                                               [08:10:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 79/80 [02:05<00:01,  1.59s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 79/80 [02:18<00:01,  1.75s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               True
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 79/80 [02:19<00:01,  1.77s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 79/80 [02:23<00:01,  1.82s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 79/80 [02:23<00:01,  1.82s/trial, best loss=?]                                                               [08:11:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 79/80 [02:25<00:01,  1.84s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 79/80 [02:37<00:01,  2.00s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               True
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 79/80 [02:39<00:02,  2.02s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 79/80 [02:43<00:02,  2.07s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 79/80 [02:43<00:02,  2.07s/trial, best loss=?]                                                               [08:11:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 79/80 [02:45<00:02,  2.09s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 79/80 [02:57<00:02,  2.25s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               True
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 79/80 [02:59<00:02,  2.27s/trial, best loss=?]100%|██████████| 80/80 [03:00<00:00, 180.04s/trial, best loss: -3265.602358202397]100%|██████████| 80/80 [03:00<00:00,  2.25s/trial, best loss: -3265.602358202397] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 1, 'random_state': 0, 'subsample': 6, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 54 since program start
2020-12-20 08:11:46.524552
Found saved Trials! Loading...
Rerunning from 80 trials to add another one.
 99%|█████████▉| 80/81 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 80/81 [00:00<00:00, 1061.74trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 80/81 [00:00<00:00, 1059.10trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 80/81 [01:24<00:01,  1.05s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 80/81 [01:24<00:01,  1.05s/trial, best loss=?]                                                               [08:13:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 80/81 [01:26<00:01,  1.08s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 80/81 [01:39<00:01,  1.24s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               True
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 80/81 [01:40<00:01,  1.26s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 80/81 [01:44<00:01,  1.30s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 80/81 [01:44<00:01,  1.31s/trial, best loss=?]                                                               [08:13:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 80/81 [01:45<00:01,  1.32s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 80/81 [01:57<00:01,  1.47s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               True
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 80/81 [01:59<00:01,  1.49s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 80/81 [02:03<00:01,  1.54s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 80/81 [02:03<00:01,  1.54s/trial, best loss=?]                                                               [08:13:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 80/81 [02:04<00:01,  1.56s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 80/81 [02:16<00:01,  1.71s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               True
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 80/81 [02:18<00:01,  1.73s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 80/81 [02:22<00:01,  1.78s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 80/81 [02:22<00:01,  1.78s/trial, best loss=?]                                                               [08:14:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 80/81 [02:24<00:01,  1.80s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 80/81 [02:36<00:01,  1.96s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               True
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 80/81 [02:37<00:01,  1.97s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 80/81 [02:42<00:02,  2.03s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 80/81 [02:42<00:02,  2.03s/trial, best loss=?]                                                               [08:14:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 80/81 [02:44<00:02,  2.05s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 80/81 [02:56<00:02,  2.21s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               True
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 80/81 [02:58<00:02,  2.23s/trial, best loss=?]100%|██████████| 81/81 [02:58<00:00, 178.76s/trial, best loss: -3265.602358202397]100%|██████████| 81/81 [02:58<00:00,  2.21s/trial, best loss: -3265.602358202397] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 1, 'random_state': 0, 'subsample': 6, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 55 since program start
2020-12-20 08:14:45.339754
Found saved Trials! Loading...
Rerunning from 81 trials to add another one.
 99%|█████████▉| 81/82 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 81/82 [00:00<00:00, 3715.14trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 81/82 [00:00<00:00, 3688.32trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 81/82 [01:23<00:01,  1.04s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=250, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 81/82 [01:23<00:01,  1.04s/trial, best loss=?]                                                               [08:16:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 81/82 [01:25<00:01,  1.06s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 81/82 [01:38<00:01,  1.22s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               True
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 81/82 [01:40<00:01,  1.24s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 81/82 [01:44<00:01,  1.29s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 81/82 [01:44<00:01,  1.29s/trial, best loss=?]                                                               [08:16:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 81/82 [01:45<00:01,  1.30s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 81/82 [01:57<00:01,  1.45s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               True
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 81/82 [01:59<00:01,  1.47s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 81/82 [02:03<00:01,  1.52s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 81/82 [02:03<00:01,  1.52s/trial, best loss=?]                                                               [08:16:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 81/82 [02:04<00:01,  1.54s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 81/82 [02:16<00:01,  1.69s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               True
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 81/82 [02:18<00:01,  1.71s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 81/82 [02:22<00:01,  1.76s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 81/82 [02:22<00:01,  1.76s/trial, best loss=?]                                                               [08:17:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 81/82 [02:24<00:01,  1.78s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 81/82 [02:36<00:01,  1.94s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               True
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 81/82 [02:38<00:01,  1.96s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 81/82 [02:42<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=250, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 81/82 [02:42<00:02,  2.01s/trial, best loss=?]                                                               [08:17:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 81/82 [02:44<00:02,  2.03s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 81/82 [02:57<00:02,  2.19s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               True
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 81/82 [02:59<00:02,  2.21s/trial, best loss=?]100%|██████████| 82/82 [02:59<00:00, 179.80s/trial, best loss: -3265.602358202397]100%|██████████| 82/82 [02:59<00:00,  2.19s/trial, best loss: -3265.602358202397] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 1, 'random_state': 0, 'subsample': 6, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 56 since program start
2020-12-20 08:17:45.200959
Found saved Trials! Loading...
Rerunning from 82 trials to add another one.
 99%|█████████▉| 82/83 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 82/83 [00:00<00:00, 3491.24trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 82/83 [00:00<00:00, 3467.49trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 82/83 [01:24<00:01,  1.03s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 82/83 [01:24<00:01,  1.03s/trial, best loss=?]                                                               [08:19:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 82/83 [01:26<00:01,  1.05s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 82/83 [01:51<00:01,  1.36s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               True
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 82/83 [01:53<00:01,  1.39s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 82/83 [01:57<00:01,  1.43s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 82/83 [01:57<00:01,  1.43s/trial, best loss=?]                                                               [08:19:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 82/83 [01:59<00:01,  1.45s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 82/83 [02:22<00:01,  1.74s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               True
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 82/83 [02:24<00:01,  1.77s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 82/83 [02:28<00:01,  1.82s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 82/83 [02:28<00:01,  1.82s/trial, best loss=?]                                                               [08:20:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 82/83 [02:30<00:01,  1.83s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 82/83 [02:54<00:02,  2.12s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               True
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 82/83 [02:56<00:02,  2.15s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 82/83 [03:00<00:02,  2.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 82/83 [03:00<00:02,  2.21s/trial, best loss=?]                                                               [08:20:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 82/83 [03:02<00:02,  2.23s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 82/83 [03:26<00:02,  2.52s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               True
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 82/83 [03:29<00:02,  2.55s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 82/83 [03:33<00:02,  2.61s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 82/83 [03:33<00:02,  2.61s/trial, best loss=?]                                                               [08:21:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 82/83 [03:35<00:02,  2.63s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 82/83 [04:00<00:02,  2.93s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               True
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 82/83 [04:02<00:02,  2.96s/trial, best loss=?]100%|██████████| 83/83 [04:03<00:00, 243.45s/trial, best loss: -3449.9525508693705]100%|██████████| 83/83 [04:03<00:00,  2.93s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 57 since program start
2020-12-20 08:21:48.713427
Found saved Trials! Loading...
Rerunning from 83 trials to add another one.
 99%|█████████▉| 83/84 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 83/84 [00:00<00:00, 951.06trial/s, best loss=?]                                                                New call of hyperopt_train_test
 99%|█████████▉| 83/84 [00:00<00:00, 948.91trial/s, best loss=?]                                                                Model used for fitting:
 99%|█████████▉| 83/84 [01:24<00:01,  1.02s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 83/84 [01:24<00:01,  1.02s/trial, best loss=?]                                                               [08:23:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 83/84 [01:26<00:01,  1.04s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 83/84 [01:51<00:01,  1.35s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               True
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 83/84 [01:53<00:01,  1.37s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 83/84 [01:57<00:01,  1.42s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 83/84 [01:57<00:01,  1.42s/trial, best loss=?]                                                               [08:23:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 83/84 [01:59<00:01,  1.44s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 83/84 [02:22<00:01,  1.72s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               True
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 83/84 [02:25<00:01,  1.75s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 83/84 [02:29<00:01,  1.80s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 83/84 [02:29<00:01,  1.80s/trial, best loss=?]                                                               [08:24:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 83/84 [02:30<00:01,  1.81s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 83/84 [02:54<00:02,  2.10s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               True
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 83/84 [02:56<00:02,  2.13s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 83/84 [03:01<00:02,  2.18s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 83/84 [03:01<00:02,  2.18s/trial, best loss=?]                                                               [08:24:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 83/84 [03:02<00:02,  2.20s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 83/84 [03:27<00:02,  2.50s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               True
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 83/84 [03:29<00:02,  2.52s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 83/84 [03:34<00:02,  2.58s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 83/84 [03:34<00:02,  2.58s/trial, best loss=?]                                                               [08:25:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 83/84 [03:35<00:02,  2.60s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 83/84 [04:00<00:02,  2.90s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               True
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 83/84 [04:03<00:02,  2.93s/trial, best loss=?]100%|██████████| 84/84 [04:03<00:00, 243.82s/trial, best loss: -3449.9525508693705]100%|██████████| 84/84 [04:03<00:00,  2.90s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 58 since program start
2020-12-20 08:25:52.595722
Found saved Trials! Loading...
Rerunning from 84 trials to add another one.
 99%|█████████▉| 84/85 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 84/85 [00:00<00:00, 3837.63trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 84/85 [00:00<00:00, 3805.22trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 84/85 [01:24<00:01,  1.00s/trial, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 84/85 [01:24<00:01,  1.00s/trial, best loss=?]                                                               [08:27:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 84/85 [01:26<00:01,  1.03s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 84/85 [01:51<00:01,  1.33s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               True
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 84/85 [01:53<00:01,  1.35s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 84/85 [01:57<00:01,  1.40s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 84/85 [01:57<00:01,  1.40s/trial, best loss=?]                                                               [08:27:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 84/85 [01:59<00:01,  1.42s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 84/85 [02:22<00:01,  1.70s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               True
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 84/85 [02:24<00:01,  1.73s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 84/85 [02:28<00:01,  1.77s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 84/85 [02:28<00:01,  1.77s/trial, best loss=?]                                                               [08:28:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 84/85 [02:30<00:01,  1.79s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 84/85 [02:54<00:02,  2.07s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               True
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 84/85 [02:56<00:02,  2.10s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 84/85 [03:00<00:02,  2.15s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 84/85 [03:00<00:02,  2.15s/trial, best loss=?]                                                               [08:28:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 84/85 [03:02<00:02,  2.17s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 84/85 [03:26<00:02,  2.46s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               True
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 84/85 [03:29<00:02,  2.49s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 84/85 [03:33<00:02,  2.54s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 84/85 [03:33<00:02,  2.54s/trial, best loss=?]                                                               [08:29:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 84/85 [03:35<00:02,  2.56s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 84/85 [03:59<00:02,  2.86s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               True
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 84/85 [04:02<00:02,  2.89s/trial, best loss=?]100%|██████████| 85/85 [04:03<00:00, 243.25s/trial, best loss: -3449.9525508693705]100%|██████████| 85/85 [04:03<00:00,  2.86s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 59 since program start
2020-12-20 08:29:55.904691
Found saved Trials! Loading...
Rerunning from 85 trials to add another one.
 99%|█████████▉| 85/86 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 85/86 [00:00<00:00, 3842.68trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 85/86 [00:00<00:00, 3815.29trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 85/86 [01:24<00:00,  1.00trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 85/86 [01:24<00:00,  1.00trial/s, best loss=?]                                                               [08:31:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 85/86 [01:26<00:01,  1.02s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 85/86 [01:52<00:01,  1.32s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               True
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 85/86 [01:54<00:01,  1.34s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 85/86 [01:58<00:01,  1.39s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 85/86 [01:58<00:01,  1.39s/trial, best loss=?]                                                               [08:31:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 85/86 [01:59<00:01,  1.41s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 85/86 [02:23<00:01,  1.68s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               True
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 85/86 [02:25<00:01,  1.71s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 85/86 [02:29<00:01,  1.76s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 85/86 [02:29<00:01,  1.76s/trial, best loss=?]                                                               [08:32:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 85/86 [02:30<00:01,  1.78s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 85/86 [02:54<00:02,  2.06s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               True
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 85/86 [02:57<00:02,  2.09s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 85/86 [03:01<00:02,  2.14s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 85/86 [03:01<00:02,  2.14s/trial, best loss=?]                                                               [08:32:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 85/86 [03:03<00:02,  2.15s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 85/86 [03:27<00:02,  2.44s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               True
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 85/86 [03:29<00:02,  2.47s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 85/86 [03:34<00:02,  2.52s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 85/86 [03:34<00:02,  2.52s/trial, best loss=?]                                                               [08:33:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 85/86 [03:35<00:02,  2.54s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 85/86 [04:00<00:02,  2.83s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               True
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 85/86 [04:03<00:02,  2.86s/trial, best loss=?]100%|██████████| 86/86 [04:03<00:00, 243.91s/trial, best loss: -3449.9525508693705]100%|██████████| 86/86 [04:03<00:00,  2.84s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 60 since program start
2020-12-20 08:33:59.876872
Found saved Trials! Loading...
Rerunning from 86 trials to add another one.
 99%|█████████▉| 86/87 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 86/87 [00:00<00:00, 1134.72trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 86/87 [00:00<00:00, 1131.97trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 86/87 [01:24<00:00,  1.01trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 86/87 [01:24<00:00,  1.01trial/s, best loss=?]                                                               [08:35:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 86/87 [01:26<00:01,  1.01s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 86/87 [01:54<00:01,  1.33s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               True
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 86/87 [01:56<00:01,  1.36s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 86/87 [02:00<00:01,  1.40s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 86/87 [02:00<00:01,  1.40s/trial, best loss=?]                                                               [08:36:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 86/87 [02:02<00:01,  1.42s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 86/87 [02:28<00:01,  1.73s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               True
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 86/87 [02:31<00:01,  1.76s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 86/87 [02:35<00:01,  1.80s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 86/87 [02:35<00:01,  1.80s/trial, best loss=?]                                                               [08:36:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 86/87 [02:36<00:01,  1.82s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 86/87 [03:03<00:02,  2.13s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               True
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 86/87 [03:05<00:02,  2.16s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 86/87 [03:09<00:02,  2.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 86/87 [03:09<00:02,  2.21s/trial, best loss=?]                                                               [08:37:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 86/87 [03:11<00:02,  2.23s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 86/87 [03:38<00:02,  2.54s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               True
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 86/87 [03:41<00:02,  2.57s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 86/87 [03:45<00:02,  2.62s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 86/87 [03:45<00:02,  2.62s/trial, best loss=?]                                                               [08:37:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 86/87 [03:47<00:02,  2.64s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 86/87 [04:14<00:02,  2.96s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               True
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 86/87 [04:17<00:02,  2.99s/trial, best loss=?]100%|██████████| 87/87 [04:17<00:00, 257.90s/trial, best loss: -3449.9525508693705]100%|██████████| 87/87 [04:17<00:00,  2.96s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 61 since program start
2020-12-20 08:38:17.840901
Found saved Trials! Loading...
Rerunning from 87 trials to add another one.
 99%|█████████▉| 87/88 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 87/88 [00:00<00:00, 4023.51trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 87/88 [00:00<00:00, 3994.58trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 87/88 [01:24<00:00,  1.03trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 87/88 [01:24<00:00,  1.03trial/s, best loss=?]                                                               [08:39:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 87/88 [01:26<00:00,  1.01trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 87/88 [01:53<00:01,  1.30s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               True
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 87/88 [01:55<00:01,  1.32s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 87/88 [01:59<00:01,  1.37s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 87/88 [01:59<00:01,  1.37s/trial, best loss=?]                                                               [08:40:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 87/88 [02:00<00:01,  1.39s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 87/88 [02:25<00:01,  1.67s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               True
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 87/88 [02:27<00:01,  1.70s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 87/88 [02:31<00:01,  1.75s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 87/88 [02:31<00:01,  1.75s/trial, best loss=?]                                                               [08:40:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 87/88 [02:33<00:01,  1.76s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 87/88 [02:58<00:02,  2.05s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               True
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 87/88 [03:00<00:02,  2.08s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 87/88 [03:05<00:02,  2.13s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 87/88 [03:05<00:02,  2.13s/trial, best loss=?]                                                               [08:41:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 87/88 [03:06<00:02,  2.15s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 87/88 [03:32<00:02,  2.44s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               True
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 87/88 [03:34<00:02,  2.47s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 87/88 [03:39<00:02,  2.52s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 87/88 [03:39<00:02,  2.52s/trial, best loss=?]                                                               [08:41:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 87/88 [03:41<00:02,  2.54s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 87/88 [04:07<00:02,  2.84s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               True
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 87/88 [04:09<00:02,  2.87s/trial, best loss=?]100%|██████████| 88/88 [04:10<00:00, 250.46s/trial, best loss: -3449.9525508693705]100%|██████████| 88/88 [04:10<00:00,  2.85s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 62 since program start
2020-12-20 08:42:28.354308
Found saved Trials! Loading...
Rerunning from 88 trials to add another one.
 99%|█████████▉| 88/89 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 88/89 [00:00<00:00, 3975.52trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 88/89 [00:00<00:00, 3947.12trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 88/89 [01:24<00:00,  1.05trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 88/89 [01:24<00:00,  1.05trial/s, best loss=?]                                                               [08:43:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 88/89 [01:25<00:00,  1.03trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 88/89 [01:51<00:01,  1.26s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               True
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 88/89 [01:53<00:01,  1.29s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 88/89 [01:57<00:01,  1.33s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 88/89 [01:57<00:01,  1.33s/trial, best loss=?]                                                               [08:44:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 88/89 [01:58<00:01,  1.35s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 88/89 [02:22<00:01,  1.62s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               True
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 88/89 [02:24<00:01,  1.64s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 88/89 [02:28<00:01,  1.69s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 88/89 [02:28<00:01,  1.69s/trial, best loss=?]                                                               [08:44:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 88/89 [02:30<00:01,  1.70s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 88/89 [02:53<00:01,  1.98s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               True
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 88/89 [02:56<00:02,  2.00s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 88/89 [03:00<00:02,  2.05s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 88/89 [03:00<00:02,  2.05s/trial, best loss=?]                                                               [08:45:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 88/89 [03:02<00:02,  2.07s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 88/89 [03:26<00:02,  2.35s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               True
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 88/89 [03:28<00:02,  2.37s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 88/89 [03:33<00:02,  2.43s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 88/89 [03:33<00:02,  2.43s/trial, best loss=?]                                                               [08:46:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 88/89 [03:35<00:02,  2.44s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 88/89 [03:59<00:02,  2.72s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               True
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 88/89 [04:02<00:02,  2.76s/trial, best loss=?]100%|██████████| 89/89 [04:03<00:00, 243.07s/trial, best loss: -3449.9525508693705]100%|██████████| 89/89 [04:03<00:00,  2.73s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 63 since program start
2020-12-20 08:46:31.484579
Found saved Trials! Loading...
Rerunning from 89 trials to add another one.
 99%|█████████▉| 89/90 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 89/90 [00:00<00:00, 1219.68trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 89/90 [00:00<00:00, 1216.64trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 89/90 [01:24<00:00,  1.06trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 89/90 [01:24<00:00,  1.06trial/s, best loss=?]                                                               [08:47:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 89/90 [01:25<00:00,  1.04trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 89/90 [01:53<00:01,  1.28s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               True
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 89/90 [01:56<00:01,  1.30s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 89/90 [02:00<00:01,  1.35s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 89/90 [02:00<00:01,  1.35s/trial, best loss=?]                                                               [08:48:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 89/90 [02:01<00:01,  1.37s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 89/90 [02:27<00:01,  1.66s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               True
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 89/90 [02:30<00:01,  1.69s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 89/90 [02:34<00:01,  1.73s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 89/90 [02:34<00:01,  1.73s/trial, best loss=?]                                                               [08:49:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 89/90 [02:35<00:01,  1.75s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 89/90 [03:02<00:02,  2.05s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               True
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 89/90 [03:04<00:02,  2.07s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 89/90 [03:08<00:02,  2.12s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 89/90 [03:08<00:02,  2.12s/trial, best loss=?]                                                               [08:49:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 89/90 [03:10<00:02,  2.14s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 89/90 [03:37<00:02,  2.44s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               True
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 89/90 [03:40<00:02,  2.47s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 89/90 [03:44<00:02,  2.52s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 89/90 [03:44<00:02,  2.52s/trial, best loss=?]                                                               [08:50:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 89/90 [03:46<00:02,  2.54s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 89/90 [04:13<00:02,  2.85s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               True
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 89/90 [04:16<00:02,  2.88s/trial, best loss=?]100%|██████████| 90/90 [04:16<00:00, 256.93s/trial, best loss: -3449.9525508693705]100%|██████████| 90/90 [04:16<00:00,  2.85s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 64 since program start
2020-12-20 08:50:48.469365
Found saved Trials! Loading...
Rerunning from 90 trials to add another one.
 99%|█████████▉| 90/91 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 90/91 [00:00<00:00, 4060.05trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 90/91 [00:00<00:00, 4031.09trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 90/91 [01:24<00:00,  1.06trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 90/91 [01:24<00:00,  1.06trial/s, best loss=?]                                                               [08:52:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 90/91 [01:26<00:00,  1.04trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 90/91 [01:53<00:01,  1.27s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               True
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 90/91 [01:56<00:01,  1.29s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 90/91 [02:00<00:01,  1.33s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 90/91 [02:00<00:01,  1.33s/trial, best loss=?]                                                               [08:52:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 90/91 [02:01<00:01,  1.35s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 90/91 [02:27<00:01,  1.64s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               True
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 90/91 [02:29<00:01,  1.67s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 90/91 [02:33<00:01,  1.71s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 90/91 [02:33<00:01,  1.71s/trial, best loss=?]                                                               [08:53:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 90/91 [02:35<00:01,  1.73s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 90/91 [03:01<00:02,  2.02s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               True
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 90/91 [03:03<00:02,  2.04s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 90/91 [03:08<00:02,  2.09s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 90/91 [03:08<00:02,  2.09s/trial, best loss=?]                                                               [08:53:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 90/91 [03:09<00:02,  2.11s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 90/91 [03:36<00:02,  2.41s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               True
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 90/91 [03:39<00:02,  2.43s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 90/91 [03:43<00:02,  2.48s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 90/91 [03:43<00:02,  2.48s/trial, best loss=?]                                                               [08:54:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 90/91 [03:45<00:02,  2.50s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 90/91 [04:12<00:02,  2.80s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               True
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 90/91 [04:15<00:02,  2.83s/trial, best loss=?]100%|██████████| 91/91 [04:15<00:00, 255.65s/trial, best loss: -3449.9525508693705]100%|██████████| 91/91 [04:15<00:00,  2.81s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 65 since program start
2020-12-20 08:55:04.178934
Found saved Trials! Loading...
Rerunning from 91 trials to add another one.
 99%|█████████▉| 91/92 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 91/92 [00:00<00:00, 4154.67trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 91/92 [00:00<00:00, 4125.35trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 91/92 [01:23<00:00,  1.08trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.3, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 91/92 [01:23<00:00,  1.08trial/s, best loss=?]                                                               [08:56:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 91/92 [01:25<00:00,  1.06trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 91/92 [01:53<00:01,  1.25s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               True
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 91/92 [01:55<00:01,  1.27s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 91/92 [01:59<00:01,  1.31s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 91/92 [01:59<00:01,  1.31s/trial, best loss=?]                                                               [08:57:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 91/92 [02:01<00:01,  1.33s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 91/92 [02:27<00:01,  1.62s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               True
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 91/92 [02:29<00:01,  1.64s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 91/92 [02:33<00:01,  1.69s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 91/92 [02:33<00:01,  1.69s/trial, best loss=?]                                                               [08:57:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 91/92 [02:34<00:01,  1.70s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 91/92 [03:01<00:01,  2.00s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               True
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 91/92 [03:04<00:02,  2.02s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 91/92 [03:08<00:02,  2.07s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 91/92 [03:08<00:02,  2.07s/trial, best loss=?]                                                               [08:58:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 91/92 [03:09<00:02,  2.09s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 91/92 [03:36<00:02,  2.38s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               True
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 91/92 [03:39<00:02,  2.41s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 91/92 [03:43<00:02,  2.46s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 91/92 [03:43<00:02,  2.46s/trial, best loss=?]                                                               [08:58:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 91/92 [03:45<00:02,  2.48s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 91/92 [04:12<00:02,  2.78s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               True
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 91/92 [04:15<00:02,  2.81s/trial, best loss=?]100%|██████████| 92/92 [04:16<00:00, 256.16s/trial, best loss: -3449.9525508693705]100%|██████████| 92/92 [04:16<00:00,  2.78s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 66 since program start
2020-12-20 08:59:20.395052
Found saved Trials! Loading...
Rerunning from 92 trials to add another one.
 99%|█████████▉| 92/93 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 92/93 [00:00<00:00, 1215.71trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 92/93 [00:00<00:00, 1212.74trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 92/93 [01:25<00:00,  1.08trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 92/93 [01:25<00:00,  1.08trial/s, best loss=?]                                                               [09:00:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 92/93 [01:27<00:00,  1.05trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 92/93 [02:27<00:01,  1.60s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               True
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 92/93 [02:30<00:01,  1.63s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 92/93 [02:34<00:01,  1.68s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 92/93 [02:34<00:01,  1.68s/trial, best loss=?]                                                               [09:01:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 92/93 [02:35<00:01,  1.69s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 92/93 [03:30<00:02,  2.29s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               True
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 92/93 [03:33<00:02,  2.32s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 92/93 [03:37<00:02,  2.37s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 92/93 [03:37<00:02,  2.37s/trial, best loss=?]                                                               [09:02:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 92/93 [03:39<00:02,  2.38s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 92/93 [04:36<00:03,  3.00s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               True
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 92/93 [04:39<00:03,  3.03s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 92/93 [04:43<00:03,  3.08s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 92/93 [04:43<00:03,  3.08s/trial, best loss=?]                                                               [09:04:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 92/93 [04:45<00:03,  3.10s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 92/93 [05:42<00:03,  3.72s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               True
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 92/93 [05:44<00:03,  3.75s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 92/93 [05:49<00:03,  3.80s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 92/93 [05:49<00:03,  3.80s/trial, best loss=?]                                                               [09:05:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 92/93 [05:51<00:03,  3.82s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 92/93 [06:47<00:04,  4.43s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               True
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 92/93 [06:51<00:04,  4.47s/trial, best loss=?]100%|██████████| 93/93 [06:51<00:00, 411.69s/trial, best loss: -3449.9525508693705]100%|██████████| 93/93 [06:51<00:00,  4.43s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 67 since program start
2020-12-20 09:06:12.149155
Found saved Trials! Loading...
Rerunning from 93 trials to add another one.
 99%|█████████▉| 93/94 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 93/94 [00:00<00:00, 4152.02trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 93/94 [00:00<00:00, 4122.32trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 93/94 [01:23<00:00,  1.11trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 93/94 [01:23<00:00,  1.11trial/s, best loss=?]                                                               [09:07:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 93/94 [01:25<00:00,  1.09trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 93/94 [01:52<00:01,  1.21s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               True
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 93/94 [01:54<00:01,  1.24s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 93/94 [01:58<00:01,  1.28s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 93/94 [01:58<00:01,  1.28s/trial, best loss=?]                                                               [09:08:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 93/94 [02:00<00:01,  1.29s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 93/94 [02:26<00:01,  1.57s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               True
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 93/94 [02:28<00:01,  1.60s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 93/94 [02:32<00:01,  1.64s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 93/94 [02:32<00:01,  1.64s/trial, best loss=?]                                                               [09:08:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 93/94 [02:33<00:01,  1.65s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 93/94 [03:00<00:01,  1.94s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               True
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 93/94 [03:02<00:01,  1.96s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 93/94 [03:06<00:02,  2.01s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 93/94 [03:06<00:02,  2.01s/trial, best loss=?]                                                               [09:09:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 93/94 [03:08<00:02,  2.02s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 93/94 [03:34<00:02,  2.31s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               True
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 93/94 [03:37<00:02,  2.34s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 93/94 [03:41<00:02,  2.38s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 93/94 [03:41<00:02,  2.38s/trial, best loss=?]                                                               [09:09:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 93/94 [03:43<00:02,  2.40s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 93/94 [04:10<00:02,  2.69s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               True
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 93/94 [04:13<00:02,  2.72s/trial, best loss=?]100%|██████████| 94/94 [04:13<00:00, 253.84s/trial, best loss: -3449.9525508693705]100%|██████████| 94/94 [04:13<00:00,  2.70s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 68 since program start
2020-12-20 09:10:26.043814
Found saved Trials! Loading...
Rerunning from 94 trials to add another one.
 99%|█████████▉| 94/95 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 94/95 [00:00<00:00, 4329.77trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 94/95 [00:00<00:00, 4299.22trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 94/95 [01:23<00:00,  1.12trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 94/95 [01:23<00:00,  1.12trial/s, best loss=?]                                                               [09:11:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 94/95 [01:25<00:00,  1.10trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 94/95 [03:15<00:02,  2.08s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               True
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 94/95 [03:18<00:02,  2.12s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 94/95 [03:22<00:02,  2.16s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 94/95 [03:22<00:02,  2.16s/trial, best loss=?]                                                               [09:13:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 94/95 [03:24<00:02,  2.18s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 94/95 [05:04<00:03,  3.24s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               True
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 94/95 [05:08<00:03,  3.28s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 94/95 [05:12<00:03,  3.32s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 94/95 [05:12<00:03,  3.32s/trial, best loss=?]                                                               [09:15:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 94/95 [05:13<00:03,  3.34s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 94/95 [06:54<00:04,  4.41s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               True
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 94/95 [06:58<00:04,  4.45s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 94/95 [07:02<00:04,  4.50s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 94/95 [07:02<00:04,  4.50s/trial, best loss=?]                                                               [09:17:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 94/95 [07:04<00:04,  4.51s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 94/95 [08:50<00:05,  5.64s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               True
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 94/95 [08:54<00:05,  5.68s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 94/95 [08:58<00:05,  5.73s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 94/95 [08:58<00:05,  5.73s/trial, best loss=?]                                                               [09:19:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 94/95 [09:00<00:05,  5.75s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 94/95 [10:49<00:06,  6.91s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               True
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 94/95 [10:54<00:06,  6.96s/trial, best loss=?]100%|██████████| 95/95 [10:54<00:00, 654.99s/trial, best loss: -3449.9525508693705]100%|██████████| 95/95 [10:54<00:00,  6.89s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 69 since program start
2020-12-20 09:21:21.096175
Found saved Trials! Loading...
Rerunning from 95 trials to add another one.
 99%|█████████▉| 95/96 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 95/96 [00:00<00:00, 1301.54trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 95/96 [00:00<00:00, 1298.02trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 95/96 [01:24<00:00,  1.13trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 95/96 [01:24<00:00,  1.13trial/s, best loss=?]                                                               [09:22:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 95/96 [01:26<00:00,  1.09trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 95/96 [04:02<00:02,  2.55s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               True
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 95/96 [04:07<00:02,  2.60s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 95/96 [04:10<00:02,  2.64s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 95/96 [04:11<00:02,  2.64s/trial, best loss=?]                                                               [09:25:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 95/96 [04:13<00:02,  2.67s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 95/96 [06:31<00:04,  4.12s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               True
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 95/96 [06:35<00:04,  4.17s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 95/96 [06:39<00:04,  4.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 95/96 [06:39<00:04,  4.21s/trial, best loss=?]                                                               [09:28:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 95/96 [06:41<00:04,  4.23s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 95/96 [09:01<00:05,  5.70s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               True
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 95/96 [09:06<00:05,  5.75s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 95/96 [09:10<00:05,  5.79s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 95/96 [09:10<00:05,  5.79s/trial, best loss=?]                                                               [09:30:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 95/96 [09:12<00:05,  5.82s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 95/96 [11:41<00:07,  7.39s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               True
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 95/96 [11:46<00:07,  7.44s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 95/96 [11:51<00:07,  7.49s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 95/96 [11:51<00:07,  7.49s/trial, best loss=?]                                                               [09:33:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 95/96 [11:53<00:07,  7.51s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 95/96 [14:26<00:09,  9.13s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               True
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 95/96 [14:33<00:09,  9.19s/trial, best loss=?]100%|██████████| 96/96 [14:33<00:00, 873.69s/trial, best loss: -3449.9525508693705]100%|██████████| 96/96 [14:33<00:00,  9.10s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 70 since program start
2020-12-20 09:35:54.850200
Found saved Trials! Loading...
Rerunning from 96 trials to add another one.
 99%|█████████▉| 96/97 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 96/97 [00:00<00:00, 4350.09trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 96/97 [00:00<00:00, 4319.39trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 96/97 [01:24<00:00,  1.14trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 96/97 [01:24<00:00,  1.14trial/s, best loss=?]                                                               [09:37:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 96/97 [01:26<00:00,  1.12trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 96/97 [01:49<00:01,  1.14s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               True
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 96/97 [01:51<00:01,  1.16s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 96/97 [01:55<00:01,  1.21s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 96/97 [01:55<00:01,  1.21s/trial, best loss=?]                                                               [09:37:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 96/97 [01:57<00:01,  1.22s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 96/97 [02:18<00:01,  1.44s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               True
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 96/97 [02:20<00:01,  1.46s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 96/97 [02:24<00:01,  1.51s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 96/97 [02:24<00:01,  1.51s/trial, best loss=?]                                                               [09:38:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 96/97 [02:26<00:01,  1.52s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 96/97 [02:47<00:01,  1.75s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               True
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 96/97 [02:50<00:01,  1.77s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 96/97 [02:54<00:01,  1.82s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 96/97 [02:54<00:01,  1.82s/trial, best loss=?]                                                               [09:38:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 96/97 [02:55<00:01,  1.83s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 96/97 [03:17<00:02,  2.06s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               True
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 96/97 [03:20<00:02,  2.09s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 96/97 [03:24<00:02,  2.13s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 96/97 [03:24<00:02,  2.13s/trial, best loss=?]                                                               [09:39:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 96/97 [03:26<00:02,  2.15s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 96/97 [03:48<00:02,  2.38s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               True
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 96/97 [03:51<00:02,  2.41s/trial, best loss=?]100%|██████████| 97/97 [03:51<00:00, 231.83s/trial, best loss: -3449.9525508693705]100%|██████████| 97/97 [03:51<00:00,  2.39s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 71 since program start
2020-12-20 09:39:46.740311
Found saved Trials! Loading...
Rerunning from 97 trials to add another one.
 99%|█████████▉| 97/98 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 97/98 [00:00<00:00, 4331.11trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 97/98 [00:00<00:00, 4299.21trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 97/98 [01:24<00:00,  1.15trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 97/98 [01:24<00:00,  1.15trial/s, best loss=?]                                                               [09:41:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 97/98 [01:26<00:00,  1.13trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 97/98 [01:55<00:01,  1.19s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               True
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 97/98 [01:57<00:01,  1.21s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 97/98 [02:01<00:01,  1.25s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 97/98 [02:01<00:01,  1.25s/trial, best loss=?]                                                               [09:41:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 97/98 [02:02<00:01,  1.26s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 97/98 [02:28<00:01,  1.54s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               True
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 97/98 [02:31<00:01,  1.56s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 97/98 [02:35<00:01,  1.60s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 97/98 [02:35<00:01,  1.60s/trial, best loss=?]                                                               [09:42:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 97/98 [02:36<00:01,  1.61s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 97/98 [03:03<00:01,  1.89s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               True
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 97/98 [03:05<00:01,  1.92s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 97/98 [03:10<00:01,  1.96s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 97/98 [03:10<00:01,  1.96s/trial, best loss=?]                                                               [09:42:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 97/98 [03:11<00:01,  1.98s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 97/98 [03:38<00:02,  2.26s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               True
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 97/98 [03:41<00:02,  2.28s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 97/98 [03:45<00:02,  2.33s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 97/98 [03:45<00:02,  2.33s/trial, best loss=?]                                                               [09:43:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 97/98 [03:47<00:02,  2.34s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 97/98 [04:15<00:02,  2.63s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               True
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 97/98 [04:17<00:02,  2.66s/trial, best loss=?]100%|██████████| 98/98 [04:18<00:00, 258.57s/trial, best loss: -3449.9525508693705]100%|██████████| 98/98 [04:18<00:00,  2.64s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 72 since program start
2020-12-20 09:44:05.371123
Found saved Trials! Loading...
Rerunning from 98 trials to add another one.
 99%|█████████▉| 98/99 [00:00<?, ?trial/s, best loss=?]                                                       New call of f
 99%|█████████▉| 98/99 [00:00<00:00, 1304.60trial/s, best loss=?]                                                                 New call of hyperopt_train_test
 99%|█████████▉| 98/99 [00:00<00:00, 1301.37trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 98/99 [01:24<00:00,  1.15trial/s, best loss=?]                                                               XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 98/99 [01:24<00:00,  1.15trial/s, best loss=?]                                                               [09:45:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 98/99 [01:26<00:00,  1.13trial/s, best loss=?]                                                               predict called
 99%|█████████▉| 98/99 [02:15<00:01,  1.38s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               (259530, 139)
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               True
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 98/99 [02:18<00:01,  1.41s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 98/99 [02:22<00:01,  1.45s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 98/99 [02:22<00:01,  1.45s/trial, best loss=?]                                                               [09:46:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 98/99 [02:23<00:01,  1.47s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 98/99 [03:09<00:01,  1.93s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               (271631, 139)
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               True
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 98/99 [03:12<00:01,  1.96s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 98/99 [03:16<00:02,  2.00s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 98/99 [03:16<00:02,  2.00s/trial, best loss=?]                                                               [09:47:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 98/99 [03:17<00:02,  2.02s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 98/99 [04:04<00:02,  2.49s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               (286101, 139)
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               True
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 98/99 [04:07<00:02,  2.52s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 98/99 [04:11<00:02,  2.57s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 98/99 [04:11<00:02,  2.57s/trial, best loss=?]                                                               [09:48:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 98/99 [04:13<00:02,  2.58s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 98/99 [05:00<00:03,  3.06s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               (289002, 139)
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               True
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 98/99 [05:03<00:03,  3.09s/trial, best loss=?]                                                               Model used for fitting:
 99%|█████████▉| 98/99 [05:07<00:03,  3.14s/trial, best loss=?]                                                               XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 98/99 [05:07<00:03,  3.14s/trial, best loss=?]                                                               [09:49:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 98/99 [05:09<00:03,  3.16s/trial, best loss=?]                                                               predict called
 99%|█████████▉| 98/99 [05:57<00:03,  3.65s/trial, best loss=?]                                                               Type of X:
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               Shape of X:
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               (323071, 139)
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               Type of y:
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               <class 'numpy.ndarray'>
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               model fitted ?
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               True
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]                                                               y is not None
 99%|█████████▉| 98/99 [06:00<00:03,  3.68s/trial, best loss=?]100%|██████████| 99/99 [06:01<00:00, 361.61s/trial, best loss: -3449.9525508693705]100%|██████████| 99/99 [06:01<00:00,  3.65s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 73 since program start
2020-12-20 09:50:07.041765
Found saved Trials! Loading...
Rerunning from 99 trials to add another one.
 99%|█████████▉| 99/100 [00:00<?, ?trial/s, best loss=?]                                                        New call of f
 99%|█████████▉| 99/100 [00:00<00:00, 4305.60trial/s, best loss=?]                                                                  New call of hyperopt_train_test
 99%|█████████▉| 99/100 [00:00<00:00, 4269.82trial/s, best loss=?]                                                                  Model used for fitting:
 99%|█████████▉| 99/100 [01:24<00:00,  1.17trial/s, best loss=?]                                                                XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 99/100 [01:24<00:00,  1.17trial/s, best loss=?]                                                                [09:51:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 99/100 [01:26<00:00,  1.14trial/s, best loss=?]                                                                predict called
 99%|█████████▉| 99/100 [03:04<00:01,  1.86s/trial, best loss=?]                                                                Type of X:
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                Shape of X:
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                (259530, 139)
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                Type of y:
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                <class 'numpy.ndarray'>
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                model fitted ?
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                True
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                y is not None
 99%|█████████▉| 99/100 [03:07<00:01,  1.90s/trial, best loss=?]                                                                Model used for fitting:
 99%|█████████▉| 99/100 [03:11<00:01,  1.94s/trial, best loss=?]                                                                XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 99/100 [03:11<00:01,  1.94s/trial, best loss=?]                                                                [09:53:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 99/100 [03:13<00:01,  1.96s/trial, best loss=?]                                                                predict called
 99%|█████████▉| 99/100 [04:40<00:02,  2.83s/trial, best loss=?]                                                                Type of X:
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                Shape of X:
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                (271631, 139)
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                Type of y:
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                <class 'numpy.ndarray'>
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                model fitted ?
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                True
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                y is not None
 99%|█████████▉| 99/100 [04:43<00:02,  2.87s/trial, best loss=?]                                                                Model used for fitting:
 99%|█████████▉| 99/100 [04:47<00:02,  2.91s/trial, best loss=?]                                                                XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 99/100 [04:47<00:02,  2.91s/trial, best loss=?]                                                                [09:54:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 99/100 [04:49<00:02,  2.92s/trial, best loss=?]                                                                predict called
 99%|█████████▉| 99/100 [06:17<00:03,  3.81s/trial, best loss=?]                                                                Type of X:
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                Shape of X:
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                (286101, 139)
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                Type of y:
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                <class 'numpy.ndarray'>
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                model fitted ?
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                True
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                y is not None
 99%|█████████▉| 99/100 [06:21<00:03,  3.85s/trial, best loss=?]                                                                Model used for fitting:
 99%|█████████▉| 99/100 [06:25<00:03,  3.90s/trial, best loss=?]                                                                XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 99/100 [06:25<00:03,  3.90s/trial, best loss=?]                                                                [09:56:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 99/100 [06:27<00:03,  3.92s/trial, best loss=?]                                                                predict called
 99%|█████████▉| 99/100 [08:01<00:04,  4.86s/trial, best loss=?]                                                                Type of X:
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                Shape of X:
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                (289002, 139)
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                Type of y:
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                <class 'numpy.ndarray'>
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                model fitted ?
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                True
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                y is not None
 99%|█████████▉| 99/100 [08:05<00:04,  4.90s/trial, best loss=?]                                                                Model used for fitting:
 99%|█████████▉| 99/100 [08:09<00:04,  4.94s/trial, best loss=?]                                                                XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 99/100 [08:09<00:04,  4.94s/trial, best loss=?]                                                                [09:58:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 99/100 [08:11<00:04,  4.96s/trial, best loss=?]                                                                predict called
 99%|█████████▉| 99/100 [09:47<00:05,  5.93s/trial, best loss=?]                                                                Type of X:
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                Shape of X:
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                (323071, 139)
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                Type of y:
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                <class 'numpy.ndarray'>
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                model fitted ?
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                True
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]                                                                y is not None
 99%|█████████▉| 99/100 [09:51<00:05,  5.98s/trial, best loss=?]100%|██████████| 100/100 [09:52<00:00, 592.41s/trial, best loss: -3449.9525508693705]100%|██████████| 100/100 [09:52<00:00,  5.92s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 74 since program start
2020-12-20 09:59:59.510494
Found saved Trials! Loading...
Rerunning from 100 trials to add another one.
 99%|█████████▉| 100/101 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 100/101 [00:00<00:00, 4482.39trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 100/101 [00:00<00:00, 4450.24trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 100/101 [01:24<00:00,  1.19trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 100/101 [01:24<00:00,  1.19trial/s, best loss=?]                                                                 [10:01:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 100/101 [01:25<00:00,  1.16trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 100/101 [01:51<00:01,  1.12s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 True
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 100/101 [01:53<00:01,  1.14s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 100/101 [01:57<00:01,  1.18s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 100/101 [01:57<00:01,  1.18s/trial, best loss=?]                                                                 [10:01:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 100/101 [01:59<00:01,  1.19s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 100/101 [02:23<00:01,  1.43s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 True
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 100/101 [02:25<00:01,  1.45s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 100/101 [02:29<00:01,  1.49s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 100/101 [02:29<00:01,  1.49s/trial, best loss=?]                                                                 [10:02:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 100/101 [02:30<00:01,  1.51s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 100/101 [02:55<00:01,  1.75s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 True
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 100/101 [02:57<00:01,  1.77s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 100/101 [03:01<00:01,  1.82s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 100/101 [03:01<00:01,  1.82s/trial, best loss=?]                                                                 [10:03:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 100/101 [03:03<00:01,  1.83s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 100/101 [03:28<00:02,  2.08s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 True
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 100/101 [03:30<00:02,  2.11s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 100/101 [03:34<00:02,  2.15s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 100/101 [03:34<00:02,  2.15s/trial, best loss=?]                                                                 [10:03:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 100/101 [03:36<00:02,  2.17s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 100/101 [04:01<00:02,  2.42s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 True
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 100/101 [04:04<00:02,  2.44s/trial, best loss=?]100%|██████████| 101/101 [04:05<00:00, 245.03s/trial, best loss: -3449.9525508693705]100%|██████████| 101/101 [04:05<00:00,  2.43s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 75 since program start
2020-12-20 10:04:04.597590
Found saved Trials! Loading...
Rerunning from 101 trials to add another one.
 99%|█████████▉| 101/102 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 101/102 [00:00<00:00, 1299.26trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 101/102 [00:00<00:00, 1296.19trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 101/102 [01:24<00:00,  1.19trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 101/102 [01:24<00:00,  1.19trial/s, best loss=?]                                                                 [10:05:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 101/102 [01:27<00:00,  1.15trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 101/102 [06:22<00:03,  3.78s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 True
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 101/102 [06:28<00:03,  3.85s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 101/102 [06:32<00:03,  3.89s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 101/102 [06:32<00:03,  3.89s/trial, best loss=?]                                                                 [10:10:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 101/102 [06:35<00:03,  3.91s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 101/102 [10:59<00:06,  6.53s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 True
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 101/102 [11:06<00:06,  6.60s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 101/102 [11:10<00:06,  6.64s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 101/102 [11:10<00:06,  6.64s/trial, best loss=?]                                                                 [10:15:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 101/102 [11:12<00:06,  6.66s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 101/102 [15:40<00:09,  9.31s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 True
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 101/102 [15:48<00:09,  9.39s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 101/102 [15:52<00:09,  9.43s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 101/102 [15:52<00:09,  9.43s/trial, best loss=?]                                                                 [10:19:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 101/102 [15:54<00:09,  9.45s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 101/102 [20:39<00:12, 12.28s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 True
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 101/102 [20:47<00:12, 12.35s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 101/102 [20:52<00:12, 12.40s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 101/102 [20:52<00:12, 12.40s/trial, best loss=?]                                                                 [10:24:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 101/102 [20:54<00:12, 12.42s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 101/102 [25:45<00:15, 15.30s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 True
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 101/102 [25:53<00:15, 15.38s/trial, best loss=?]100%|██████████| 102/102 [25:54<00:00, 1554.20s/trial, best loss: -3449.9525508693705]100%|██████████| 102/102 [25:54<00:00, 15.24s/trial, best loss: -3449.9525508693705]  
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 76 since program start
2020-12-20 10:29:58.857264
Found saved Trials! Loading...
Rerunning from 102 trials to add another one.
 99%|█████████▉| 102/103 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 102/103 [00:00<00:00, 4551.75trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 102/103 [00:00<00:00, 4519.01trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 102/103 [01:24<00:00,  1.21trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 102/103 [01:24<00:00,  1.21trial/s, best loss=?]                                                                 [10:31:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 102/103 [01:25<00:00,  1.19trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 102/103 [01:50<00:01,  1.09s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 True
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 102/103 [01:52<00:01,  1.11s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 102/103 [01:56<00:01,  1.15s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 102/103 [01:56<00:01,  1.15s/trial, best loss=?]                                                                 [10:31:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 102/103 [01:58<00:01,  1.16s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 102/103 [02:20<00:01,  1.38s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 True
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 102/103 [02:23<00:01,  1.40s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 102/103 [02:27<00:01,  1.44s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 102/103 [02:27<00:01,  1.44s/trial, best loss=?]                                                                 [10:32:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 102/103 [02:28<00:01,  1.46s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 102/103 [02:51<00:01,  1.68s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 True
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 102/103 [02:54<00:01,  1.71s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 102/103 [02:58<00:01,  1.75s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 102/103 [02:58<00:01,  1.75s/trial, best loss=?]                                                                 [10:32:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 102/103 [03:00<00:01,  1.77s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 102/103 [03:23<00:01,  2.00s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 True
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 102/103 [03:25<00:02,  2.02s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 102/103 [03:30<00:02,  2.06s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 102/103 [03:30<00:02,  2.06s/trial, best loss=?]                                                                 [10:33:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 102/103 [03:32<00:02,  2.08s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 102/103 [03:55<00:02,  2.31s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 True
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 102/103 [03:58<00:02,  2.34s/trial, best loss=?]100%|██████████| 103/103 [03:59<00:00, 239.28s/trial, best loss: -3449.9525508693705]100%|██████████| 103/103 [03:59<00:00,  2.32s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 77 since program start
2020-12-20 10:33:58.202101
Found saved Trials! Loading...
Rerunning from 103 trials to add another one.
 99%|█████████▉| 103/104 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 103/104 [00:00<00:00, 4670.11trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 103/104 [00:00<00:00, 4637.08trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 103/104 [01:23<00:00,  1.23trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 103/104 [01:23<00:00,  1.23trial/s, best loss=?]                                                                 [10:35:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 103/104 [01:25<00:00,  1.20trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 103/104 [02:25<00:01,  1.41s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 True
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 103/104 [02:29<00:01,  1.45s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 103/104 [02:33<00:01,  1.49s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 103/104 [02:33<00:01,  1.49s/trial, best loss=?]                                                                 [10:36:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 103/104 [02:34<00:01,  1.50s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 103/104 [03:30<00:02,  2.04s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 True
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 103/104 [03:34<00:02,  2.08s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 103/104 [03:38<00:02,  2.12s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 103/104 [03:38<00:02,  2.12s/trial, best loss=?]                                                                 [10:37:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 103/104 [03:39<00:02,  2.13s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 103/104 [04:35<00:02,  2.68s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 True
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 103/104 [04:40<00:02,  2.72s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 103/104 [04:44<00:02,  2.76s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 103/104 [04:44<00:02,  2.76s/trial, best loss=?]                                                                 [10:38:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 103/104 [04:46<00:02,  2.78s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 103/104 [05:43<00:03,  3.33s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 True
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 103/104 [05:47<00:03,  3.38s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 103/104 [05:52<00:03,  3.42s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 103/104 [05:52<00:03,  3.42s/trial, best loss=?]                                                                 [10:39:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 103/104 [05:53<00:03,  3.44s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 103/104 [06:52<00:04,  4.01s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 True
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 103/104 [06:57<00:04,  4.05s/trial, best loss=?]100%|██████████| 104/104 [06:58<00:00, 418.23s/trial, best loss: -3449.9525508693705]100%|██████████| 104/104 [06:58<00:00,  4.02s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 78 since program start
2020-12-20 10:40:56.496418
Found saved Trials! Loading...
Rerunning from 104 trials to add another one.
 99%|█████████▉| 104/105 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 104/105 [00:00<00:00, 1384.81trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 104/105 [00:00<00:00, 1381.44trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 104/105 [01:23<00:00,  1.25trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 104/105 [01:23<00:00,  1.25trial/s, best loss=?]                                                                 [10:42:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 104/105 [01:25<00:00,  1.22trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 104/105 [02:22<00:01,  1.37s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 True
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 104/105 [02:25<00:01,  1.40s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 104/105 [02:29<00:01,  1.43s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 104/105 [02:29<00:01,  1.43s/trial, best loss=?]                                                                 [10:43:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 104/105 [02:30<00:01,  1.45s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 104/105 [03:24<00:01,  1.96s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 True
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 104/105 [03:27<00:01,  1.99s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 104/105 [03:31<00:02,  2.03s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 104/105 [03:31<00:02,  2.03s/trial, best loss=?]                                                                 [10:44:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 104/105 [03:32<00:02,  2.04s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 104/105 [04:27<00:02,  2.57s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 True
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 104/105 [04:30<00:02,  2.60s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 104/105 [04:34<00:02,  2.64s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 104/105 [04:34<00:02,  2.64s/trial, best loss=?]                                                                 [10:45:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 104/105 [04:36<00:02,  2.66s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 104/105 [05:31<00:03,  3.19s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 True
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 104/105 [05:34<00:03,  3.22s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 104/105 [05:38<00:03,  3.26s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 104/105 [05:38<00:03,  3.26s/trial, best loss=?]                                                                 [10:46:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 104/105 [05:40<00:03,  3.27s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 104/105 [06:36<00:03,  3.81s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 True
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 104/105 [06:39<00:03,  3.84s/trial, best loss=?]100%|██████████| 105/105 [06:40<00:00, 400.48s/trial, best loss: -3449.9525508693705]100%|██████████| 105/105 [06:40<00:00,  3.81s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 79 since program start
2020-12-20 10:47:37.039986
Found saved Trials! Loading...
Rerunning from 105 trials to add another one.
 99%|█████████▉| 105/106 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 105/106 [00:00<00:00, 4683.43trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 105/106 [00:00<00:00, 4648.88trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 105/106 [01:24<00:00,  1.24trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 105/106 [01:24<00:00,  1.24trial/s, best loss=?]                                                                 [10:49:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 105/106 [01:26<00:00,  1.21trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 105/106 [03:57<00:02,  2.26s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 True
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 105/106 [04:01<00:02,  2.30s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 105/106 [04:05<00:02,  2.34s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 105/106 [04:05<00:02,  2.34s/trial, best loss=?]                                                                 [10:51:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 105/106 [04:07<00:02,  2.36s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 105/106 [06:25<00:03,  3.67s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 True
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 105/106 [06:29<00:03,  3.71s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 105/106 [06:33<00:03,  3.75s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 105/106 [06:33<00:03,  3.75s/trial, best loss=?]                                                                 [10:54:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 105/106 [06:35<00:03,  3.76s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 105/106 [08:56<00:05,  5.11s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 True
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 105/106 [09:00<00:05,  5.15s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 105/106 [09:05<00:05,  5.19s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 105/106 [09:05<00:05,  5.19s/trial, best loss=?]                                                                 [10:56:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 105/106 [09:07<00:05,  5.21s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 105/106 [11:33<00:06,  6.61s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 True
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 105/106 [11:38<00:06,  6.65s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 105/106 [11:42<00:06,  6.69s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 105/106 [11:42<00:06,  6.69s/trial, best loss=?]                                                                 [10:59:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 105/106 [11:44<00:06,  6.71s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 105/106 [14:12<00:08,  8.12s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 True
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 105/106 [14:17<00:08,  8.17s/trial, best loss=?]100%|██████████| 106/106 [14:18<00:00, 858.11s/trial, best loss: -3449.9525508693705]100%|██████████| 106/106 [14:18<00:00,  8.10s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 80 since program start
2020-12-20 11:01:55.210856
Found saved Trials! Loading...
Rerunning from 106 trials to add another one.
 99%|█████████▉| 106/107 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 106/107 [00:00<00:00, 4717.80trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 106/107 [00:00<00:00, 4684.50trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 106/107 [01:24<00:00,  1.26trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.3, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 106/107 [01:24<00:00,  1.26trial/s, best loss=?]                                                                 [11:03:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 106/107 [01:25<00:00,  1.24trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 106/107 [01:49<00:01,  1.04s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 True
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 106/107 [01:51<00:01,  1.06s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 106/107 [01:55<00:01,  1.09s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 106/107 [01:55<00:01,  1.09s/trial, best loss=?]                                                                 [11:03:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 106/107 [01:57<00:01,  1.11s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 106/107 [02:20<00:01,  1.32s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 True
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 106/107 [02:22<00:01,  1.34s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 106/107 [02:26<00:01,  1.38s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 106/107 [02:26<00:01,  1.38s/trial, best loss=?]                                                                 [11:04:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 106/107 [02:27<00:01,  1.39s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 106/107 [02:50<00:01,  1.61s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 True
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 106/107 [02:53<00:01,  1.63s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 106/107 [02:57<00:01,  1.67s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 106/107 [02:57<00:01,  1.67s/trial, best loss=?]                                                                 [11:04:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 106/107 [02:59<00:01,  1.69s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 106/107 [03:22<00:01,  1.91s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 True
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 106/107 [03:25<00:01,  1.93s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 106/107 [03:29<00:01,  1.98s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 106/107 [03:29<00:01,  1.98s/trial, best loss=?]                                                                 [11:05:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 106/107 [03:31<00:01,  1.99s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 106/107 [03:54<00:02,  2.21s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 True
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 106/107 [03:57<00:02,  2.24s/trial, best loss=?]100%|██████████| 107/107 [03:58<00:00, 238.10s/trial, best loss: -3449.9525508693705]100%|██████████| 107/107 [03:58<00:00,  2.23s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 81 since program start
2020-12-20 11:05:53.372465
Found saved Trials! Loading...
Rerunning from 107 trials to add another one.
 99%|█████████▉| 107/108 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 107/108 [00:00<00:00, 1424.04trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 107/108 [00:00<00:00, 1420.52trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 107/108 [01:24<00:00,  1.27trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 107/108 [01:24<00:00,  1.27trial/s, best loss=?]                                                                 [11:07:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 107/108 [01:26<00:00,  1.23trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 107/108 [04:34<00:02,  2.57s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 True
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 107/108 [04:39<00:02,  2.62s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 107/108 [04:44<00:02,  2.65s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 107/108 [04:44<00:02,  2.65s/trial, best loss=?]                                                                 [11:10:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 107/108 [04:45<00:02,  2.67s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 107/108 [07:29<00:04,  4.21s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 True
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 107/108 [07:35<00:04,  4.26s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 107/108 [07:39<00:04,  4.29s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 107/108 [07:39<00:04,  4.29s/trial, best loss=?]                                                                 [11:13:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 107/108 [07:41<00:04,  4.31s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 107/108 [10:29<00:05,  5.88s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 True
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 107/108 [10:35<00:05,  5.94s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 107/108 [10:39<00:05,  5.98s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 107/108 [10:39<00:05,  5.98s/trial, best loss=?]                                                                 [11:16:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 107/108 [10:41<00:05,  6.00s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 107/108 [13:39<00:07,  7.66s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 True
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 107/108 [13:44<00:07,  7.71s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 107/108 [13:49<00:07,  7.75s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 107/108 [13:49<00:07,  7.75s/trial, best loss=?]                                                                 [11:19:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 107/108 [13:51<00:07,  7.77s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 107/108 [16:55<00:09,  9.49s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 True
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 107/108 [17:02<00:09,  9.55s/trial, best loss=?]100%|██████████| 108/108 [17:02<00:00, 1022.73s/trial, best loss: -3449.9525508693705]100%|██████████| 108/108 [17:02<00:00,  9.47s/trial, best loss: -3449.9525508693705]  
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 82 since program start
2020-12-20 11:22:56.165753
Found saved Trials! Loading...
Rerunning from 108 trials to add another one.
 99%|█████████▉| 108/109 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 108/109 [00:00<00:00, 4894.22trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 108/109 [00:00<00:00, 4858.89trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 108/109 [01:23<00:00,  1.29trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 108/109 [01:23<00:00,  1.29trial/s, best loss=?]                                                                 [11:24:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 108/109 [01:25<00:00,  1.26trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 108/109 [02:16<00:01,  1.26s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 True
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 108/109 [02:20<00:01,  1.30s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 108/109 [02:23<00:01,  1.33s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 108/109 [02:23<00:01,  1.33s/trial, best loss=?]                                                                 [11:25:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 108/109 [02:25<00:01,  1.35s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 108/109 [03:12<00:01,  1.78s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 True
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 108/109 [03:16<00:01,  1.82s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 108/109 [03:20<00:01,  1.86s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 108/109 [03:20<00:01,  1.86s/trial, best loss=?]                                                                 [11:26:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 108/109 [03:22<00:01,  1.87s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 108/109 [04:09<00:02,  2.31s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 True
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 108/109 [04:14<00:02,  2.35s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 108/109 [04:18<00:02,  2.39s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 108/109 [04:18<00:02,  2.39s/trial, best loss=?]                                                                 [11:27:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 108/109 [04:20<00:02,  2.41s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 108/109 [05:09<00:02,  2.86s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 True
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 108/109 [05:13<00:02,  2.90s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 108/109 [05:18<00:02,  2.95s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 108/109 [05:18<00:02,  2.95s/trial, best loss=?]                                                                 [11:28:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 108/109 [05:19<00:02,  2.96s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 108/109 [06:09<00:03,  3.42s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 True
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 108/109 [06:14<00:03,  3.46s/trial, best loss=?]100%|██████████| 109/109 [06:14<00:00, 374.83s/trial, best loss: -3449.9525508693705]100%|██████████| 109/109 [06:14<00:00,  3.44s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 83 since program start
2020-12-20 11:29:11.057353
Found saved Trials! Loading...
Rerunning from 109 trials to add another one.
 99%|█████████▉| 109/110 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 109/110 [00:00<00:00, 4865.94trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 109/110 [00:00<00:00, 4831.89trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 109/110 [01:25<00:00,  1.28trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 109/110 [01:25<00:00,  1.28trial/s, best loss=?]                                                                 [11:30:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 109/110 [01:26<00:00,  1.25trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 109/110 [01:50<00:01,  1.01s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 True
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 109/110 [01:52<00:01,  1.03s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 109/110 [01:56<00:01,  1.07s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 109/110 [01:56<00:01,  1.07s/trial, best loss=?]                                                                 [11:31:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 109/110 [01:57<00:01,  1.08s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 109/110 [02:19<00:01,  1.28s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 True
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 109/110 [02:21<00:01,  1.30s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 109/110 [02:25<00:01,  1.33s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 109/110 [02:25<00:01,  1.33s/trial, best loss=?]                                                                 [11:31:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 109/110 [02:26<00:01,  1.35s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 109/110 [02:48<00:01,  1.55s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 True
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 109/110 [02:50<00:01,  1.57s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 109/110 [02:55<00:01,  1.61s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 109/110 [02:55<00:01,  1.61s/trial, best loss=?]                                                                 [11:32:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 109/110 [02:56<00:01,  1.62s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 109/110 [03:18<00:01,  1.82s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 True
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 109/110 [03:21<00:01,  1.85s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 109/110 [03:25<00:01,  1.89s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 109/110 [03:25<00:01,  1.89s/trial, best loss=?]                                                                 [11:32:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 109/110 [03:27<00:01,  1.90s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 109/110 [03:49<00:02,  2.11s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 True
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 109/110 [03:52<00:02,  2.13s/trial, best loss=?]100%|██████████| 110/110 [03:53<00:00, 233.14s/trial, best loss: -3449.9525508693705]100%|██████████| 110/110 [03:53<00:00,  2.12s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 84 since program start
2020-12-20 11:33:04.254803
Found saved Trials! Loading...
Rerunning from 110 trials to add another one.
 99%|█████████▉| 110/111 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 110/111 [00:00<00:00, 1365.40trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 110/111 [00:00<00:00, 1362.29trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 110/111 [01:24<00:00,  1.31trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 110/111 [01:24<00:00,  1.31trial/s, best loss=?]                                                                 [11:34:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 110/111 [01:25<00:00,  1.28trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 110/111 [02:21<00:01,  1.29s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 True
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 110/111 [02:24<00:01,  1.31s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 110/111 [02:28<00:01,  1.35s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 110/111 [02:28<00:01,  1.35s/trial, best loss=?]                                                                 [11:35:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 110/111 [02:29<00:01,  1.36s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 110/111 [03:18<00:01,  1.81s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 True
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 110/111 [03:21<00:01,  1.83s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 110/111 [03:25<00:01,  1.87s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 110/111 [03:25<00:01,  1.87s/trial, best loss=?]                                                                 [11:36:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 110/111 [03:26<00:01,  1.88s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 110/111 [04:18<00:02,  2.35s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 True
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 110/111 [04:20<00:02,  2.37s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 110/111 [04:25<00:02,  2.41s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 110/111 [04:25<00:02,  2.41s/trial, best loss=?]                                                                 [11:37:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 110/111 [04:26<00:02,  2.43s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 110/111 [05:17<00:02,  2.89s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 True
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 110/111 [05:20<00:02,  2.92s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 110/111 [05:25<00:02,  2.96s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 110/111 [05:25<00:02,  2.96s/trial, best loss=?]                                                                 [11:38:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 110/111 [05:26<00:02,  2.97s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 110/111 [06:17<00:03,  3.43s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 True
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 110/111 [06:20<00:03,  3.46s/trial, best loss=?]100%|██████████| 111/111 [06:21<00:00, 381.14s/trial, best loss: -3449.9525508693705]100%|██████████| 111/111 [06:21<00:00,  3.43s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 85 since program start
2020-12-20 11:39:25.457884
Found saved Trials! Loading...
Rerunning from 111 trials to add another one.
 99%|█████████▉| 111/112 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 111/112 [00:00<00:00, 4895.87trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 111/112 [00:00<00:00, 4861.26trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 111/112 [01:24<00:00,  1.32trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 111/112 [01:24<00:00,  1.32trial/s, best loss=?]                                                                 [11:40:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 111/112 [01:26<00:00,  1.29trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 111/112 [01:53<00:01,  1.02s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 True
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 111/112 [01:55<00:01,  1.04s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 111/112 [01:59<00:01,  1.08s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 111/112 [01:59<00:01,  1.08s/trial, best loss=?]                                                                 [11:41:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 111/112 [02:01<00:01,  1.09s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 111/112 [02:26<00:01,  1.32s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 True
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 111/112 [02:28<00:01,  1.34s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 111/112 [02:32<00:01,  1.38s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 111/112 [02:32<00:01,  1.38s/trial, best loss=?]                                                                 [11:41:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 111/112 [02:34<00:01,  1.39s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 111/112 [02:59<00:01,  1.62s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 True
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 111/112 [03:02<00:01,  1.64s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 111/112 [03:06<00:01,  1.68s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 111/112 [03:06<00:01,  1.68s/trial, best loss=?]                                                                 [11:42:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 111/112 [03:08<00:01,  1.70s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 111/112 [03:34<00:01,  1.93s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 True
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 111/112 [03:36<00:01,  1.95s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 111/112 [03:40<00:01,  1.99s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 111/112 [03:40<00:01,  1.99s/trial, best loss=?]                                                                 [11:43:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 111/112 [03:42<00:02,  2.01s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 111/112 [04:09<00:02,  2.24s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 True
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 111/112 [04:11<00:02,  2.27s/trial, best loss=?]100%|██████████| 112/112 [04:12<00:00, 252.36s/trial, best loss: -3449.9525508693705]100%|██████████| 112/112 [04:12<00:00,  2.25s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 86 since program start
2020-12-20 11:43:37.878236
Found saved Trials! Loading...
Rerunning from 112 trials to add another one.
 99%|█████████▉| 112/113 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 112/113 [00:00<00:00, 4961.31trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 112/113 [00:00<00:00, 4926.71trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 112/113 [01:23<00:00,  1.34trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 112/113 [01:23<00:00,  1.34trial/s, best loss=?]                                                                 [11:45:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 112/113 [01:25<00:00,  1.31trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 112/113 [06:41<00:03,  3.58s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 True
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 112/113 [06:49<00:03,  3.66s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 112/113 [06:53<00:03,  3.70s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 112/113 [06:53<00:03,  3.70s/trial, best loss=?]                                                                 [11:50:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 112/113 [06:55<00:03,  3.71s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 112/113 [11:45<00:06,  6.30s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 True
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 112/113 [11:54<00:06,  6.38s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 112/113 [11:58<00:06,  6.42s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 112/113 [11:58<00:06,  6.42s/trial, best loss=?]                                                                 [11:55:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 112/113 [12:00<00:06,  6.43s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 112/113 [16:55<00:09,  9.07s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 True
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 112/113 [17:04<00:09,  9.15s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 112/113 [17:09<00:09,  9.19s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 112/113 [17:09<00:09,  9.19s/trial, best loss=?]                                                                 [12:00:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 112/113 [17:10<00:09,  9.20s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 112/113 [22:12<00:11, 11.89s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 True
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 112/113 [22:21<00:11, 11.98s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 112/113 [22:26<00:12, 12.02s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 112/113 [22:26<00:12, 12.02s/trial, best loss=?]                                                                 [12:06:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 112/113 [22:28<00:12, 12.04s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 112/113 [27:34<00:14, 14.77s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 True
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 112/113 [27:45<00:14, 14.87s/trial, best loss=?]100%|██████████| 113/113 [27:45<00:00, 1665.79s/trial, best loss: -3449.9525508693705]100%|██████████| 113/113 [27:45<00:00, 14.74s/trial, best loss: -3449.9525508693705]  
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 87 since program start
2020-12-20 12:11:23.726505
Found saved Trials! Loading...
Rerunning from 113 trials to add another one.
 99%|█████████▉| 113/114 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 113/114 [00:00<00:00, 1478.96trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 113/114 [00:00<00:00, 1475.30trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 113/114 [01:23<00:00,  1.35trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 113/114 [01:23<00:00,  1.35trial/s, best loss=?]                                                                 [12:12:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 113/114 [01:25<00:00,  1.32trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 113/114 [01:52<00:00,  1.00trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 True
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 113/114 [01:54<00:01,  1.02s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 113/114 [01:58<00:01,  1.05s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 113/114 [01:58<00:01,  1.05s/trial, best loss=?]                                                                 [12:13:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 113/114 [02:00<00:01,  1.06s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 113/114 [02:25<00:01,  1.29s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 True
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 113/114 [02:27<00:01,  1.31s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 113/114 [02:31<00:01,  1.34s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 113/114 [02:31<00:01,  1.34s/trial, best loss=?]                                                                 [12:13:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 113/114 [02:32<00:01,  1.35s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 113/114 [02:58<00:01,  1.58s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 True
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 113/114 [03:00<00:01,  1.60s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 113/114 [03:04<00:01,  1.64s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 113/114 [03:04<00:01,  1.64s/trial, best loss=?]                                                                 [12:14:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 113/114 [03:06<00:01,  1.65s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 113/114 [03:32<00:01,  1.88s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 True
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 113/114 [03:34<00:01,  1.90s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 113/114 [03:39<00:01,  1.94s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 113/114 [03:39<00:01,  1.94s/trial, best loss=?]                                                                 [12:15:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 113/114 [03:41<00:01,  1.96s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 113/114 [04:07<00:02,  2.19s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 True
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 113/114 [04:10<00:02,  2.22s/trial, best loss=?]100%|██████████| 114/114 [04:11<00:00, 251.06s/trial, best loss: -3449.9525508693705]100%|██████████| 114/114 [04:11<00:00,  2.20s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 88 since program start
2020-12-20 12:15:34.849136
Found saved Trials! Loading...
Rerunning from 114 trials to add another one.
 99%|█████████▉| 114/115 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 114/115 [00:00<00:00, 5149.49trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 114/115 [00:00<00:00, 5110.09trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 114/115 [01:24<00:00,  1.36trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 114/115 [01:24<00:00,  1.36trial/s, best loss=?]                                                                 [12:17:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [01:27<00:00,  1.31trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [03:46<00:01,  1.99s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [03:51<00:02,  2.03s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 114/115 [03:54<00:02,  2.06s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 114/115 [03:54<00:02,  2.06s/trial, best loss=?]                                                                 [12:19:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [03:57<00:02,  2.08s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [06:02<00:03,  3.18s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [06:06<00:03,  3.22s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 114/115 [06:10<00:03,  3.25s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 114/115 [06:10<00:03,  3.25s/trial, best loss=?]                                                                 [12:21:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [06:13<00:03,  3.27s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [08:19<00:04,  4.38s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [08:24<00:04,  4.43s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 114/115 [08:28<00:04,  4.46s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 114/115 [08:28<00:04,  4.46s/trial, best loss=?]                                                                 [12:24:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [08:31<00:04,  4.49s/trial, best loss=?]START OF OPTIMIZATION:
Run 0 since program start
2020-12-20 13:40:28.103204
Found saved Trials! Loading...
Rerunning from 114 trials to add another one.
 99%|█████████▉| 114/115 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 114/115 [00:00<00:00, 5039.42trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 114/115 [00:00<00:00, 4999.38trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 114/115 [01:24<00:00,  1.36trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 114/115 [01:24<00:00,  1.36trial/s, best loss=?]                                                                 [13:41:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [01:27<00:00,  1.30trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [06:01<00:03,  3.17s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [06:07<00:03,  3.23s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 114/115 [06:12<00:03,  3.26s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 114/115 [06:12<00:03,  3.26s/trial, best loss=?]                                                                 [13:46:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [06:14<00:03,  3.29s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [10:14<00:05,  5.39s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [10:21<00:05,  5.45s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 114/115 [10:25<00:05,  5.49s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 114/115 [10:25<00:05,  5.49s/trial, best loss=?]                                                                 [13:50:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [10:28<00:05,  5.51s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [14:33<00:07,  7.67s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [14:41<00:07,  7.73s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 114/115 [14:46<00:07,  7.77s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 114/115 [14:46<00:07,  7.77s/trial, best loss=?]                                                                 [13:55:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [14:49<00:07,  7.80s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [19:11<00:10, 10.10s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [19:19<00:10, 10.17s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 114/115 [19:24<00:10, 10.21s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 114/115 [19:24<00:10, 10.21s/trial, best loss=?]                                                                 [13:59:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 114/115 [19:27<00:10, 10.24s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 114/115 [23:58<00:12, 12.62s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 True
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 114/115 [24:07<00:12, 12.70s/trial, best loss=?]100%|██████████| 115/115 [24:08<00:00, 1448.35s/trial, best loss: -3449.9525508693705]100%|██████████| 115/115 [24:08<00:00, 12.59s/trial, best loss: -3449.9525508693705]  
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 1 since program start
2020-12-20 14:04:36.680275
Found saved Trials! Loading...
Rerunning from 115 trials to add another one.
 99%|█████████▉| 115/116 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 115/116 [00:00<00:00, 4962.81trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 115/116 [00:00<00:00, 4928.73trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 115/116 [01:24<00:00,  1.36trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 115/116 [01:24<00:00,  1.36trial/s, best loss=?]                                                                 [14:06:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 115/116 [01:27<00:00,  1.32trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 115/116 [01:51<00:00,  1.03trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 True
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 115/116 [01:53<00:00,  1.01trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 115/116 [01:58<00:01,  1.03s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 115/116 [01:58<00:01,  1.03s/trial, best loss=?]                                                                 [14:06:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 115/116 [02:00<00:01,  1.05s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 115/116 [02:23<00:01,  1.25s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 True
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 115/116 [02:25<00:01,  1.27s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 115/116 [02:30<00:01,  1.31s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 115/116 [02:30<00:01,  1.31s/trial, best loss=?]                                                                 [14:07:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 115/116 [02:32<00:01,  1.32s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 115/116 [02:55<00:01,  1.53s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 True
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 115/116 [02:58<00:01,  1.55s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 115/116 [03:03<00:01,  1.59s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 115/116 [03:03<00:01,  1.59s/trial, best loss=?]                                                                 [14:07:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 115/116 [03:05<00:01,  1.61s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 115/116 [03:29<00:01,  1.82s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 True
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 115/116 [03:31<00:01,  1.84s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 115/116 [03:36<00:01,  1.88s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 115/116 [03:36<00:01,  1.88s/trial, best loss=?]                                                                 [14:08:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 115/116 [03:39<00:01,  1.91s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 115/116 [04:03<00:02,  2.12s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 True
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 115/116 [04:06<00:02,  2.14s/trial, best loss=?]100%|██████████| 116/116 [04:07<00:00, 247.03s/trial, best loss: -3449.9525508693705]100%|██████████| 116/116 [04:07<00:00,  2.13s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 2 since program start
2020-12-20 14:08:43.773223
Found saved Trials! Loading...
Rerunning from 116 trials to add another one.
 99%|█████████▉| 116/117 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 116/117 [00:00<00:00, 5035.75trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 116/117 [00:00<00:00, 4996.91trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 116/117 [01:24<00:00,  1.38trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 116/117 [01:24<00:00,  1.38trial/s, best loss=?]                                                                 [14:10:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 116/117 [01:26<00:00,  1.34trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 116/117 [02:22<00:01,  1.23s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 True
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 116/117 [02:26<00:01,  1.27s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 116/117 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 116/117 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 [14:11:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 116/117 [02:33<00:01,  1.32s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 116/117 [03:25<00:01,  1.77s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 True
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 116/117 [03:29<00:01,  1.81s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 116/117 [03:33<00:01,  1.84s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 116/117 [03:33<00:01,  1.84s/trial, best loss=?]                                                                 [14:12:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 116/117 [03:36<00:01,  1.86s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 116/117 [04:28<00:02,  2.32s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 True
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 116/117 [04:33<00:02,  2.35s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 116/117 [04:37<00:02,  2.40s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 116/117 [04:37<00:02,  2.40s/trial, best loss=?]                                                                 [14:13:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 116/117 [04:40<00:02,  2.42s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 116/117 [05:34<00:02,  2.88s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 True
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 116/117 [05:38<00:02,  2.92s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 116/117 [05:43<00:02,  2.96s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 116/117 [05:43<00:02,  2.96s/trial, best loss=?]                                                                 [14:14:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 116/117 [05:46<00:02,  2.98s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 116/117 [06:41<00:03,  3.46s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 True
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 116/117 [06:46<00:03,  3.50s/trial, best loss=?]100%|██████████| 117/117 [06:47<00:00, 407.10s/trial, best loss: -3449.9525508693705]100%|██████████| 117/117 [06:47<00:00,  3.48s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 3 since program start
2020-12-20 14:15:30.939540
Found saved Trials! Loading...
Rerunning from 117 trials to add another one.
 99%|█████████▉| 117/118 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 117/118 [00:00<00:00, 5160.51trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 117/118 [00:00<00:00, 5124.14trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 117/118 [01:24<00:00,  1.39trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 117/118 [01:24<00:00,  1.39trial/s, best loss=?]                                                                 [14:16:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 117/118 [01:27<00:00,  1.34trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 117/118 [01:55<00:00,  1.02trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 True
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 117/118 [01:57<00:01,  1.00s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 117/118 [02:01<00:01,  1.04s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 117/118 [02:01<00:01,  1.04s/trial, best loss=?]                                                                 [14:17:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 117/118 [02:03<00:01,  1.06s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 117/118 [02:29<00:01,  1.28s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 True
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 117/118 [02:31<00:01,  1.30s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 117/118 [02:35<00:01,  1.33s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 117/118 [02:35<00:01,  1.33s/trial, best loss=?]                                                                 [14:18:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 117/118 [02:37<00:01,  1.35s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 117/118 [03:04<00:01,  1.57s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 True
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 117/118 [03:06<00:01,  1.59s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 117/118 [03:11<00:01,  1.63s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 117/118 [03:11<00:01,  1.63s/trial, best loss=?]                                                                 [14:18:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 117/118 [03:13<00:01,  1.65s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 117/118 [03:40<00:01,  1.88s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 True
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 117/118 [03:42<00:01,  1.90s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 117/118 [03:47<00:01,  1.94s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 117/118 [03:47<00:01,  1.94s/trial, best loss=?]                                                                 [14:19:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 117/118 [03:49<00:01,  1.96s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 117/118 [04:16<00:02,  2.20s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 True
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 117/118 [04:19<00:02,  2.22s/trial, best loss=?]100%|██████████| 118/118 [04:20<00:00, 260.22s/trial, best loss: -3449.9525508693705]100%|██████████| 118/118 [04:20<00:00,  2.21s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 4 since program start
2020-12-20 14:19:51.226178
Found saved Trials! Loading...
Rerunning from 118 trials to add another one.
 99%|█████████▉| 118/119 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 118/119 [00:00<00:00, 1565.64trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 118/119 [00:00<00:00, 1561.84trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 118/119 [01:24<00:00,  1.39trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 118/119 [01:24<00:00,  1.39trial/s, best loss=?]                                                                 [14:21:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 118/119 [01:27<00:00,  1.35trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 118/119 [02:17<00:01,  1.17s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 True
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 118/119 [02:20<00:01,  1.19s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 118/119 [02:24<00:01,  1.23s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 118/119 [02:24<00:01,  1.23s/trial, best loss=?]                                                                 [14:22:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 118/119 [02:27<00:01,  1.25s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 118/119 [03:14<00:01,  1.65s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 True
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 118/119 [03:17<00:01,  1.67s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 118/119 [03:21<00:01,  1.71s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 118/119 [03:21<00:01,  1.71s/trial, best loss=?]                                                                 [14:23:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 118/119 [03:23<00:01,  1.72s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 118/119 [04:10<00:02,  2.13s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 118/119 [04:13<00:02,  2.15s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 118/119 [04:13<00:02,  2.15s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 118/119 [04:13<00:02,  2.15s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 118/119 [04:13<00:02,  2.15s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 118/119 [04:13<00:02,  2.15s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 118/119 [04:14<00:02,  2.15s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 118/119 [04:14<00:02,  2.15s/trial, best loss=?]                                                                 True
 99%|█████████▉| 118/119 [04:14<00:02,  2.15s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 118/119 [04:14<00:02,  2.15s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 118/119 [04:18<00:02,  2.19s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 118/119 [04:18<00:02,  2.19s/trial, best loss=?]                                                                 [14:24:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 118/119 [04:21<00:02,  2.21s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 118/119 [05:09<00:02,  2.63s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 True
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 118/119 [05:12<00:02,  2.65s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 118/119 [05:17<00:02,  2.69s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 118/119 [05:17<00:02,  2.69s/trial, best loss=?]                                                                 [14:25:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 118/119 [05:20<00:02,  2.71s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 118/119 [06:09<00:03,  3.13s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 True
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 118/119 [06:13<00:03,  3.16s/trial, best loss=?]100%|██████████| 119/119 [06:13<00:00, 373.81s/trial, best loss: -3449.9525508693705]100%|██████████| 119/119 [06:13<00:00,  3.14s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 5 since program start
2020-12-20 14:26:05.096462
Found saved Trials! Loading...
Rerunning from 119 trials to add another one.
 99%|█████████▉| 119/120 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 119/120 [00:00<00:00, 4932.23trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 119/120 [00:00<00:00, 4900.18trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 119/120 [01:24<00:00,  1.41trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 119/120 [01:24<00:00,  1.41trial/s, best loss=?]                                                                 [14:27:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 119/120 [01:27<00:00,  1.36trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 119/120 [02:55<00:01,  1.48s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 True
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 119/120 [02:59<00:01,  1.51s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 119/120 [03:04<00:01,  1.55s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 119/120 [03:04<00:01,  1.55s/trial, best loss=?]                                                                 [14:29:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 119/120 [03:06<00:01,  1.57s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 119/120 [04:29<00:02,  2.26s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 True
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 119/120 [04:33<00:02,  2.30s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 119/120 [04:37<00:02,  2.33s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 119/120 [04:37<00:02,  2.33s/trial, best loss=?]                                                                 [14:30:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 119/120 [04:40<00:02,  2.35s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 119/120 [06:02<00:03,  3.05s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 True
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 119/120 [06:07<00:03,  3.08s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 119/120 [06:11<00:03,  3.12s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 119/120 [06:11<00:03,  3.12s/trial, best loss=?]                                                                 [14:32:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 119/120 [06:14<00:03,  3.14s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 119/120 [07:39<00:03,  3.86s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 True
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 119/120 [07:44<00:03,  3.90s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 119/120 [07:49<00:03,  3.94s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 119/120 [07:49<00:03,  3.94s/trial, best loss=?]                                                                 [14:33:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 119/120 [07:51<00:03,  3.96s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 119/120 [09:19<00:04,  4.70s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 True
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 119/120 [09:24<00:04,  4.74s/trial, best loss=?]100%|██████████| 120/120 [09:24<00:00, 564.89s/trial, best loss: -3449.9525508693705]100%|██████████| 120/120 [09:24<00:00,  4.71s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 6 since program start
2020-12-20 14:35:30.052457
Found saved Trials! Loading...
Rerunning from 120 trials to add another one.
 99%|█████████▉| 120/121 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 120/121 [00:00<00:00, 5233.39trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 120/121 [00:00<00:00, 5196.71trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 120/121 [01:24<00:00,  1.43trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 120/121 [01:24<00:00,  1.43trial/s, best loss=?]                                                                 [14:36:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 120/121 [01:26<00:00,  1.38trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 120/121 [01:29<00:00,  1.34trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 True
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 120/121 [01:30<00:00,  1.33trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 120/121 [01:34<00:00,  1.27trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 120/121 [01:34<00:00,  1.27trial/s, best loss=?]                                                                 [14:37:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 120/121 [01:36<00:00,  1.24trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 120/121 [01:39<00:00,  1.21trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 True
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 120/121 [01:39<00:00,  1.20trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 120/121 [01:43<00:00,  1.15trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 120/121 [01:43<00:00,  1.15trial/s, best loss=?]                                                                 [14:37:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 120/121 [01:46<00:00,  1.13trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 120/121 [01:48<00:00,  1.10trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 True
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 120/121 [01:49<00:00,  1.10trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 120/121 [01:54<00:00,  1.05trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 120/121 [01:54<00:00,  1.05trial/s, best loss=?]                                                                 [14:37:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 120/121 [01:56<00:00,  1.03trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 120/121 [01:59<00:00,  1.01trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 True
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 120/121 [01:59<00:00,  1.00trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 120/121 [02:04<00:01,  1.04s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 120/121 [02:04<00:01,  1.04s/trial, best loss=?]                                                                 [14:37:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 120/121 [02:06<00:01,  1.06s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 120/121 [02:09<00:01,  1.08s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 True
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 120/121 [02:10<00:01,  1.09s/trial, best loss=?]100%|██████████| 121/121 [02:11<00:00, 131.08s/trial, best loss: -3449.9525508693705]100%|██████████| 121/121 [02:11<00:00,  1.08s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 7 since program start
2020-12-20 14:37:41.199316
Found saved Trials! Loading...
Rerunning from 121 trials to add another one.
 99%|█████████▉| 121/122 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 121/122 [00:00<00:00, 5157.63trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 121/122 [00:00<00:00, 5121.66trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 121/122 [01:24<00:00,  1.43trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.3, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 121/122 [01:24<00:00,  1.43trial/s, best loss=?]                                                                 [14:39:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 121/122 [01:28<00:00,  1.37trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 121/122 [04:32<00:02,  2.26s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 True
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 121/122 [04:38<00:02,  2.30s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 121/122 [04:42<00:02,  2.33s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 121/122 [04:42<00:02,  2.33s/trial, best loss=?]                                                                 [14:42:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 121/122 [04:45<00:02,  2.36s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 121/122 [07:28<00:03,  3.70s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 True
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 121/122 [07:33<00:03,  3.75s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 121/122 [07:37<00:03,  3.78s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 121/122 [07:37<00:03,  3.78s/trial, best loss=?]                                                                 [14:45:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 121/122 [07:40<00:03,  3.81s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 121/122 [10:27<00:05,  5.18s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 True
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 121/122 [10:32<00:05,  5.23s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 121/122 [10:37<00:05,  5.27s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 121/122 [10:37<00:05,  5.27s/trial, best loss=?]                                                                 [14:48:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 121/122 [10:40<00:05,  5.29s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 121/122 [13:36<00:06,  6.75s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 True
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 121/122 [13:41<00:06,  6.79s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 121/122 [13:46<00:06,  6.83s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 121/122 [13:46<00:06,  6.83s/trial, best loss=?]                                                                 [14:51:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 121/122 [13:49<00:06,  6.86s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 121/122 [16:51<00:08,  8.36s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 True
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 121/122 [16:58<00:08,  8.42s/trial, best loss=?]100%|██████████| 122/122 [16:59<00:00, 1019.04s/trial, best loss: -3449.9525508693705]100%|██████████| 122/122 [16:59<00:00,  8.35s/trial, best loss: -3449.9525508693705]  
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 8 since program start
2020-12-20 14:54:40.357914
Found saved Trials! Loading...
Rerunning from 122 trials to add another one.
 99%|█████████▉| 122/123 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 122/123 [00:00<00:00, 5300.06trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 122/123 [00:00<00:00, 5263.43trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 122/123 [01:25<00:00,  1.42trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 122/123 [01:25<00:00,  1.42trial/s, best loss=?]                                                                 [14:56:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 122/123 [01:28<00:00,  1.38trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 122/123 [02:23<00:01,  1.18s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 True
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 122/123 [02:27<00:01,  1.21s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 122/123 [02:31<00:01,  1.24s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 122/123 [02:31<00:01,  1.24s/trial, best loss=?]                                                                 [14:57:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 122/123 [02:34<00:01,  1.26s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 122/123 [03:25<00:01,  1.68s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 True
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 122/123 [03:29<00:01,  1.72s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 122/123 [03:33<00:01,  1.75s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 122/123 [03:33<00:01,  1.75s/trial, best loss=?]                                                                 [14:58:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 122/123 [03:35<00:01,  1.77s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 122/123 [04:27<00:02,  2.20s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 True
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 122/123 [04:32<00:02,  2.23s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 122/123 [04:36<00:02,  2.27s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 122/123 [04:36<00:02,  2.27s/trial, best loss=?]                                                                 [14:59:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 122/123 [04:39<00:02,  2.29s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 122/123 [05:31<00:02,  2.72s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 True
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 122/123 [05:36<00:02,  2.75s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 122/123 [05:40<00:02,  2.79s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 122/123 [05:40<00:02,  2.79s/trial, best loss=?]                                                                 [15:00:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 122/123 [05:43<00:02,  2.82s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 122/123 [06:37<00:03,  3.25s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 True
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 122/123 [06:41<00:03,  3.29s/trial, best loss=?]100%|██████████| 123/123 [06:42<00:00, 402.34s/trial, best loss: -3449.9525508693705]100%|██████████| 123/123 [06:42<00:00,  3.27s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 9 since program start
2020-12-20 15:01:22.761500
Found saved Trials! Loading...
Rerunning from 123 trials to add another one.
 99%|█████████▉| 123/124 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 123/124 [00:00<00:00, 5352.15trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 123/124 [00:00<00:00, 5314.82trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 123/124 [01:24<00:00,  1.45trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 123/124 [01:24<00:00,  1.45trial/s, best loss=?]                                                                 [15:02:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 123/124 [01:27<00:00,  1.40trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 123/124 [01:54<00:00,  1.08trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 True
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 123/124 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 123/124 [02:00<00:00,  1.02trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 123/124 [02:00<00:00,  1.02trial/s, best loss=?]                                                                 [15:03:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 123/124 [02:02<00:00,  1.00trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 123/124 [02:27<00:01,  1.20s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 True
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 123/124 [02:29<00:01,  1.22s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 123/124 [02:33<00:01,  1.25s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 123/124 [02:33<00:01,  1.25s/trial, best loss=?]                                                                 [15:03:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 123/124 [02:35<00:01,  1.27s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 123/124 [03:00<00:01,  1.47s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 True
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 123/124 [03:03<00:01,  1.49s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 123/124 [03:07<00:01,  1.53s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 123/124 [03:07<00:01,  1.53s/trial, best loss=?]                                                                 [15:04:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 123/124 [03:09<00:01,  1.54s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 123/124 [03:35<00:01,  1.75s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 True
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 123/124 [03:37<00:01,  1.77s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 123/124 [03:42<00:01,  1.81s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 123/124 [03:42<00:01,  1.81s/trial, best loss=?]                                                                 [15:05:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 123/124 [03:44<00:01,  1.83s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 123/124 [04:10<00:02,  2.04s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 True
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 123/124 [04:13<00:02,  2.06s/trial, best loss=?]100%|██████████| 124/124 [04:14<00:00, 254.00s/trial, best loss: -3449.9525508693705]100%|██████████| 124/124 [04:14<00:00,  2.05s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 10 since program start
2020-12-20 15:05:36.828588
Found saved Trials! Loading...
Rerunning from 124 trials to add another one.
 99%|█████████▉| 124/125 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 124/125 [00:00<00:00, 5475.71trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 124/125 [00:00<00:00, 5438.26trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 124/125 [01:25<00:00,  1.46trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 124/125 [01:25<00:00,  1.46trial/s, best loss=?]                                                                 [15:07:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 124/125 [01:27<00:00,  1.41trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 124/125 [01:54<00:00,  1.08trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 True
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 124/125 [01:56<00:00,  1.06trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 124/125 [02:00<00:00,  1.03trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 124/125 [02:00<00:00,  1.03trial/s, best loss=?]                                                                 [15:07:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 124/125 [02:03<00:00,  1.01trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 124/125 [02:27<00:01,  1.19s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 True
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 124/125 [02:30<00:01,  1.21s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 124/125 [02:34<00:01,  1.24s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 124/125 [02:34<00:01,  1.24s/trial, best loss=?]                                                                 [15:08:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 124/125 [02:36<00:01,  1.26s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 124/125 [03:01<00:01,  1.46s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 True
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 124/125 [03:03<00:01,  1.48s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 124/125 [03:08<00:01,  1.52s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 124/125 [03:08<00:01,  1.52s/trial, best loss=?]                                                                 [15:08:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 124/125 [03:10<00:01,  1.54s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 124/125 [03:36<00:01,  1.75s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 True
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 124/125 [03:38<00:01,  1.76s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 124/125 [03:43<00:01,  1.80s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 124/125 [03:43<00:01,  1.80s/trial, best loss=?]                                                                 [15:09:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 124/125 [03:45<00:01,  1.82s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 124/125 [04:12<00:02,  2.03s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 True
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 124/125 [04:14<00:02,  2.06s/trial, best loss=?]100%|██████████| 125/125 [04:15<00:00, 255.56s/trial, best loss: -3449.9525508693705]100%|██████████| 125/125 [04:15<00:00,  2.04s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 11 since program start
2020-12-20 15:09:52.498349
Found saved Trials! Loading...
Rerunning from 125 trials to add another one.
 99%|█████████▉| 125/126 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 125/126 [00:00<00:00, 5484.41trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 125/126 [00:00<00:00, 5446.87trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 125/126 [01:24<00:00,  1.48trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 125/126 [01:24<00:00,  1.48trial/s, best loss=?]                                                                 [15:11:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 125/126 [01:27<00:00,  1.43trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 125/126 [02:21<00:01,  1.13s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 True
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 125/126 [02:23<00:01,  1.15s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 125/126 [02:27<00:01,  1.18s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 125/126 [02:27<00:01,  1.18s/trial, best loss=?]                                                                 [15:12:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 125/126 [02:29<00:01,  1.20s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 125/126 [03:17<00:01,  1.58s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 True
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 125/126 [03:20<00:01,  1.60s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 125/126 [03:24<00:01,  1.63s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 125/126 [03:24<00:01,  1.63s/trial, best loss=?]                                                                 [15:13:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 125/126 [03:26<00:01,  1.65s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 125/126 [04:15<00:02,  2.05s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 True
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 125/126 [04:18<00:02,  2.07s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 125/126 [04:23<00:02,  2.11s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 125/126 [04:23<00:02,  2.11s/trial, best loss=?]                                                                 [15:14:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 125/126 [04:25<00:02,  2.13s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 125/126 [05:14<00:02,  2.52s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 True
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 125/126 [05:17<00:02,  2.54s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 125/126 [05:22<00:02,  2.58s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 125/126 [05:22<00:02,  2.58s/trial, best loss=?]                                                                 [15:15:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 125/126 [05:25<00:02,  2.60s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 125/126 [06:14<00:02,  3.00s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 True
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 125/126 [06:17<00:03,  3.02s/trial, best loss=?]100%|██████████| 126/126 [06:18<00:00, 378.38s/trial, best loss: -3449.9525508693705]100%|██████████| 126/126 [06:18<00:00,  3.00s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 12 since program start
2020-12-20 15:16:10.940010
Found saved Trials! Loading...
Rerunning from 126 trials to add another one.
 99%|█████████▉| 126/127 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 126/127 [00:00<00:00, 5623.23trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 126/127 [00:00<00:00, 5583.84trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 126/127 [01:24<00:00,  1.49trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 126/127 [01:24<00:00,  1.49trial/s, best loss=?]                                                                 [15:17:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 126/127 [01:27<00:00,  1.44trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 126/127 [01:30<00:00,  1.39trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 True
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 126/127 [01:31<00:00,  1.38trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 126/127 [01:35<00:00,  1.32trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 126/127 [01:35<00:00,  1.32trial/s, best loss=?]                                                                 [15:17:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 126/127 [01:37<00:00,  1.29trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 126/127 [01:39<00:00,  1.26trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 True
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 126/127 [01:40<00:00,  1.25trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 126/127 [01:44<00:00,  1.20trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 126/127 [01:44<00:00,  1.20trial/s, best loss=?]                                                                 [15:17:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 126/127 [01:47<00:00,  1.18trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 126/127 [01:49<00:00,  1.15trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 True
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 126/127 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 126/127 [01:55<00:00,  1.09trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 126/127 [01:55<00:00,  1.09trial/s, best loss=?]                                                                 [15:18:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 126/127 [01:57<00:00,  1.07trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 126/127 [02:00<00:00,  1.05trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 True
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 126/127 [02:00<00:00,  1.04trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 126/127 [02:05<00:00,  1.00trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 126/127 [02:05<00:00,  1.00trial/s, best loss=?]                                                                 [15:18:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 126/127 [02:08<00:01,  1.02s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 126/127 [02:10<00:01,  1.04s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 True
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 126/127 [02:11<00:01,  1.04s/trial, best loss=?]100%|██████████| 127/127 [02:12<00:00, 132.18s/trial, best loss: -3449.9525508693705]100%|██████████| 127/127 [02:12<00:00,  1.04s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 13 since program start
2020-12-20 15:18:23.181748
Found saved Trials! Loading...
Rerunning from 127 trials to add another one.
 99%|█████████▉| 127/128 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 127/128 [00:00<00:00, 5508.09trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 127/128 [00:00<00:00, 5469.86trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 127/128 [01:24<00:00,  1.50trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 127/128 [01:24<00:00,  1.50trial/s, best loss=?]                                                                 [15:19:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 127/128 [01:27<00:00,  1.45trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 127/128 [01:50<00:00,  1.14trial/s, best loss=?]                                                                 Type of X:
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 (259530, 139)
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 Type of y:
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 True
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 y is not None
 99%|█████████▉| 127/128 [01:53<00:00,  1.12trial/s, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 127/128 [01:57<00:00,  1.08trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 127/128 [01:57<00:00,  1.08trial/s, best loss=?]                                                                 [15:20:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 127/128 [01:59<00:00,  1.06trial/s, best loss=?]                                                                 predict called
 99%|█████████▉| 127/128 [02:21<00:01,  1.12s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 (271631, 139)
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 True
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 127/128 [02:24<00:01,  1.13s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 127/128 [02:28<00:01,  1.17s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 127/128 [02:28<00:01,  1.17s/trial, best loss=?]                                                                 [15:20:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 127/128 [02:30<00:01,  1.19s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 127/128 [02:52<00:01,  1.36s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 (286101, 139)
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 True
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 127/128 [02:55<00:01,  1.38s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 127/128 [03:00<00:01,  1.42s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 127/128 [03:00<00:01,  1.42s/trial, best loss=?]                                                                 [15:21:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 127/128 [03:02<00:01,  1.44s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 127/128 [03:25<00:01,  1.62s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 (289002, 139)
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 True
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 127/128 [03:27<00:01,  1.64s/trial, best loss=?]                                                                 Model used for fitting:
 99%|█████████▉| 127/128 [03:32<00:01,  1.67s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 127/128 [03:32<00:01,  1.67s/trial, best loss=?]                                                                 [15:21:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 127/128 [03:34<00:01,  1.69s/trial, best loss=?]                                                                 predict called
 99%|█████████▉| 127/128 [03:58<00:01,  1.88s/trial, best loss=?]                                                                 Type of X:
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 Shape of X:
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 (323071, 139)
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 Type of y:
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 model fitted ?
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 True
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]                                                                 y is not None
 99%|█████████▉| 127/128 [04:01<00:01,  1.90s/trial, best loss=?]100%|██████████| 128/128 [04:01<00:00, 241.72s/trial, best loss: -3449.9525508693705]100%|██████████| 128/128 [04:01<00:00,  1.89s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 14 since program start
2020-12-20 15:22:25.022779
Found saved Trials! Loading...
Rerunning from 128 trials to add another one.
 99%|█████████▉| 128/129 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 99%|█████████▉| 128/129 [00:00<00:00, 5601.62trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 99%|█████████▉| 128/129 [00:00<00:00, 5562.97trial/s, best loss=?]                                                                   Model used for fitting:
 99%|█████████▉| 128/129 [01:24<00:00,  1.51trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 128/129 [01:24<00:00,  1.51trial/s, best loss=?]                                                                 [15:23:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 128/129 [01:27<00:00,  1.46trial/s, best loss=?]START OF OPTIMIZATION:
Run 0 since program start
2020-12-20 15:25:45.605249
Found saved Trials! Loading...
Rerunning from 128 trials to add another one.
 81%|████████  | 128/158 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 81%|████████  | 128/158 [00:00<00:00, 5587.57trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 81%|████████  | 128/158 [00:00<00:00, 5531.62trial/s, best loss=?]                                                                   Model used for fitting:
 81%|████████  | 128/158 [01:24<00:19,  1.51trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 81%|████████  | 128/158 [01:24<00:19,  1.51trial/s, best loss=?]                                                                 [15:27:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 81%|████████  | 128/158 [01:28<00:20,  1.45trial/s, best loss=?]                                                                 predict called
 81%|████████  | 128/158 [07:43<01:48,  3.62s/trial, best loss=?]                                                                 Type of X:
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 Shape of X:
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 (259530, 139)
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 Type of y:
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 model fitted ?
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 True
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 y is not None
 81%|████████  | 128/158 [07:52<01:50,  3.69s/trial, best loss=?]                                                                 Model used for fitting:
 81%|████████  | 128/158 [07:56<01:51,  3.72s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 81%|████████  | 128/158 [07:56<01:51,  3.72s/trial, best loss=?]                                                                 [15:33:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 81%|████████  | 128/158 [07:58<01:52,  3.74s/trial, best loss=?]                                                                 predict called
 81%|████████  | 128/158 [13:45<03:13,  6.45s/trial, best loss=?]                                                                 Type of X:
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 Shape of X:
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 (271631, 139)
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 Type of y:
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 model fitted ?
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 True
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 y is not None
 81%|████████  | 128/158 [13:54<03:15,  6.52s/trial, best loss=?]                                                                 Model used for fitting:
 81%|████████  | 128/158 [13:58<03:16,  6.55s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 81%|████████  | 128/158 [13:58<03:16,  6.55s/trial, best loss=?]                                                                 [15:39:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 81%|████████  | 128/158 [14:00<03:17,  6.57s/trial, best loss=?]                                                                 predict called
 81%|████████  | 128/158 [20:01<04:41,  9.39s/trial, best loss=?]                                                                 Type of X:
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 Shape of X:
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 (286101, 139)
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 Type of y:
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 model fitted ?
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 True
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 y is not None
 81%|████████  | 128/158 [20:11<04:43,  9.46s/trial, best loss=?]                                                                 Model used for fitting:
 81%|████████  | 128/158 [20:15<04:44,  9.50s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 81%|████████  | 128/158 [20:15<04:44,  9.50s/trial, best loss=?]                                                                 [15:46:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 81%|████████  | 128/158 [20:17<04:45,  9.51s/trial, best loss=?]                                                                 predict called
 81%|████████  | 128/158 [26:14<06:09, 12.30s/trial, best loss=?]                                                                 Type of X:
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 Shape of X:
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 (289002, 139)
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 Type of y:
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 model fitted ?
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 True
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 y is not None
 81%|████████  | 128/158 [26:24<06:11, 12.38s/trial, best loss=?]                                                                 Model used for fitting:
 81%|████████  | 128/158 [26:28<06:12, 12.41s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 81%|████████  | 128/158 [26:28<06:12, 12.41s/trial, best loss=?]                                                                 [15:52:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 81%|████████  | 128/158 [26:30<06:12, 12.43s/trial, best loss=?]                                                                 predict called
 81%|████████  | 128/158 [32:30<07:37, 15.24s/trial, best loss=?]                                                                 Type of X:
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 Shape of X:
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 (323071, 139)
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 Type of y:
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 model fitted ?
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 True
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?]                                                                 y is not None
 81%|████████  | 128/158 [32:41<07:39, 15.33s/trial, best loss=?] 82%|████████▏ | 129/158 [32:42<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 82%|████████▏ | 129/158 [32:42<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 82%|████████▏ | 129/158 [32:42<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 82%|████████▏ | 129/158 [34:07<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 82%|████████▏ | 129/158 [34:07<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         [15:59:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 129/158 [34:10<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 82%|████████▏ | 129/158 [41:19<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         True
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 82%|████████▏ | 129/158 [41:27<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 82%|████████▏ | 129/158 [41:31<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 129/158 [41:31<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         [16:07:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 129/158 [41:33<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 82%|████████▏ | 129/158 [47:49<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         True
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 82%|████████▏ | 129/158 [47:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 82%|████████▏ | 129/158 [48:02<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 129/158 [48:02<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         [16:13:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 129/158 [48:04<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 82%|████████▏ | 129/158 [54:29<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         True
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 82%|████████▏ | 129/158 [54:38<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 82%|████████▏ | 129/158 [54:42<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 129/158 [54:42<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         [16:20:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 129/158 [54:45<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 82%|████████▏ | 129/158 [1:01:42<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           (289002, 139)
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           True
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 82%|████████▏ | 129/158 [1:01:52<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 82%|████████▏ | 129/158 [1:01:56<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 129/158 [1:01:56<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           [16:27:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 129/158 [1:01:58<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 82%|████████▏ | 129/158 [1:08:54<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           (323071, 139)
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           True
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 82%|████████▏ | 129/158 [1:09:03<15:48:28, 1962.36s/trial, best loss: -3449.9525508693705] 82%|████████▏ | 130/158 [1:09:04<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           New call of f
 82%|████████▏ | 130/158 [1:09:04<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           New call of hyperopt_train_test
 82%|████████▏ | 130/158 [1:09:04<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 82%|████████▏ | 130/158 [1:10:29<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 82%|████████▏ | 130/158 [1:10:29<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           [16:36:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 130/158 [1:10:31<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 82%|████████▏ | 130/158 [1:10:59<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           (259530, 139)
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           True
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 82%|████████▏ | 130/158 [1:11:02<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 82%|████████▏ | 130/158 [1:11:06<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 130/158 [1:11:06<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           [16:36:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 130/158 [1:11:07<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 82%|████████▏ | 130/158 [1:11:34<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           (271631, 139)
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           True
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 82%|████████▏ | 130/158 [1:11:36<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 82%|████████▏ | 130/158 [1:11:40<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 130/158 [1:11:40<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           [16:37:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 130/158 [1:11:42<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 82%|████████▏ | 130/158 [1:12:08<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           (286101, 139)
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           True
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 82%|████████▏ | 130/158 [1:12:11<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 82%|████████▏ | 130/158 [1:12:15<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 130/158 [1:12:15<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           [16:38:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 130/158 [1:12:17<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 82%|████████▏ | 130/158 [1:12:44<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           (289002, 139)
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           True
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 82%|████████▏ | 130/158 [1:12:46<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 82%|████████▏ | 130/158 [1:12:51<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 82%|████████▏ | 130/158 [1:12:51<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           [16:38:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 82%|████████▏ | 130/158 [1:12:53<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 82%|████████▏ | 130/158 [1:13:20<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           (323071, 139)
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           True
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 82%|████████▏ | 130/158 [1:13:23<15:46:32, 2028.31s/trial, best loss: -3449.9525508693705] 83%|████████▎ | 131/158 [1:13:24<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           New call of f
 83%|████████▎ | 131/158 [1:13:24<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           New call of hyperopt_train_test
 83%|████████▎ | 131/158 [1:13:24<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 83%|████████▎ | 131/158 [1:14:48<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 83%|████████▎ | 131/158 [1:14:48<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           [16:40:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 131/158 [1:14:50<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           (259530, 139)
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           True
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 83%|████████▎ | 131/158 [1:14:53<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 83%|████████▎ | 131/158 [1:14:57<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 131/158 [1:14:57<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           [16:40:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 131/158 [1:14:59<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 83%|████████▎ | 131/158 [1:15:01<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           (271631, 139)
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           True
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 83%|████████▎ | 131/158 [1:15:02<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 83%|████████▎ | 131/158 [1:15:06<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 131/158 [1:15:06<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           [16:40:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 131/158 [1:15:07<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           (286101, 139)
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           True
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 83%|████████▎ | 131/158 [1:15:10<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 83%|████████▎ | 131/158 [1:15:15<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 131/158 [1:15:15<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           [16:41:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 131/158 [1:15:16<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           (289002, 139)
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           True
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 83%|████████▎ | 131/158 [1:15:19<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Model used for fitting:
 83%|████████▎ | 131/158 [1:15:24<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 83%|████████▎ | 131/158 [1:15:24<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           [16:41:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 83%|████████▎ | 131/158 [1:15:25<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           predict called
 83%|████████▎ | 131/158 [1:15:28<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of X:
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'pandas.core.frame.DataFrame'>
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Shape of X:
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           (323071, 139)
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           Type of y:
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           <class 'numpy.ndarray'>
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           model fitted ?
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           True
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705]                                                                                           y is not None
 83%|████████▎ | 131/158 [1:15:29<11:13:57, 1497.67s/trial, best loss: -3449.9525508693705] 84%|████████▎ | 132/158 [1:15:29<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                           New call of f
 84%|████████▎ | 132/158 [1:15:29<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          New call of hyperopt_train_test
 84%|████████▎ | 132/158 [1:15:29<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Model used for fitting:
 84%|████████▎ | 132/158 [1:16:54<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 84%|████████▎ | 132/158 [1:16:54<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          [16:42:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▎ | 132/158 [1:16:56<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          predict called
 84%|████████▎ | 132/158 [1:17:48<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of X:
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'pandas.core.frame.DataFrame'>
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Shape of X:
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          (259530, 139)
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of y:
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'numpy.ndarray'>
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          model fitted ?
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          True
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          y is not None
 84%|████████▎ | 132/158 [1:17:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Model used for fitting:
 84%|████████▎ | 132/158 [1:17:54<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▎ | 132/158 [1:17:54<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          [16:43:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▎ | 132/158 [1:17:56<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          predict called
 84%|████████▎ | 132/158 [1:18:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of X:
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'pandas.core.frame.DataFrame'>
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Shape of X:
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          (271631, 139)
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of y:
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'numpy.ndarray'>
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          model fitted ?
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          True
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          y is not None
 84%|████████▎ | 132/158 [1:18:45<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Model used for fitting:
 84%|████████▎ | 132/158 [1:18:49<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▎ | 132/158 [1:18:49<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          [16:44:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▎ | 132/158 [1:18:50<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          predict called
 84%|████████▎ | 132/158 [1:19:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of X:
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'pandas.core.frame.DataFrame'>
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Shape of X:
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          (286101, 139)
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of y:
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'numpy.ndarray'>
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          model fitted ?
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          True
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          y is not None
 84%|████████▎ | 132/158 [1:19:42<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Model used for fitting:
 84%|████████▎ | 132/158 [1:19:46<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▎ | 132/158 [1:19:46<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          [16:45:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▎ | 132/158 [1:19:47<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          predict called
 84%|████████▎ | 132/158 [1:20:36<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of X:
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'pandas.core.frame.DataFrame'>
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Shape of X:
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          (289002, 139)
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of y:
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'numpy.ndarray'>
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          model fitted ?
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          True
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          y is not None
 84%|████████▎ | 132/158 [1:20:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Model used for fitting:
 84%|████████▎ | 132/158 [1:20:44<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▎ | 132/158 [1:20:44<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          [16:46:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▎ | 132/158 [1:20:46<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          predict called
 84%|████████▎ | 132/158 [1:21:35<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of X:
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'pandas.core.frame.DataFrame'>
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Shape of X:
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          (323071, 139)
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          Type of y:
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          <class 'numpy.ndarray'>
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          model fitted ?
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          True
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705]                                                                                          y is not None
 84%|████████▎ | 132/158 [1:21:39<7:50:37, 1086.06s/trial, best loss: -3449.9525508693705] 84%|████████▍ | 133/158 [1:21:39<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                          New call of f
 84%|████████▍ | 133/158 [1:21:39<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 84%|████████▍ | 133/158 [1:21:39<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 84%|████████▍ | 133/158 [1:23:04<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 84%|████████▍ | 133/158 [1:23:04<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         [16:48:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 133/158 [1:23:06<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 84%|████████▍ | 133/158 [1:23:35<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         True
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 84%|████████▍ | 133/158 [1:23:38<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 84%|████████▍ | 133/158 [1:23:42<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 133/158 [1:23:42<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         [16:49:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 133/158 [1:23:43<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 84%|████████▍ | 133/158 [1:24:11<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         True
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 84%|████████▍ | 133/158 [1:24:13<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 84%|████████▍ | 133/158 [1:24:17<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 133/158 [1:24:17<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         [16:50:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 133/158 [1:24:18<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 84%|████████▍ | 133/158 [1:24:46<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         True
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 84%|████████▍ | 133/158 [1:24:48<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 84%|████████▍ | 133/158 [1:24:53<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 133/158 [1:24:53<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         [16:50:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 133/158 [1:24:54<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 84%|████████▍ | 133/158 [1:25:22<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         True
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 84%|████████▍ | 133/158 [1:25:25<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 84%|████████▍ | 133/158 [1:25:29<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 133/158 [1:25:29<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         [16:51:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 133/158 [1:25:31<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 84%|████████▍ | 133/158 [1:26:00<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         True
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 84%|████████▍ | 133/158 [1:26:02<6:03:00, 871.22s/trial, best loss: -3449.9525508693705] 85%|████████▍ | 134/158 [1:26:03<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 85%|████████▍ | 134/158 [1:26:03<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 85%|████████▍ | 134/158 [1:26:03<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▍ | 134/158 [1:27:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 85%|████████▍ | 134/158 [1:27:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         [16:53:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▍ | 134/158 [1:27:30<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▍ | 134/158 [1:31:30<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▍ | 134/158 [1:31:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▍ | 134/158 [1:31:42<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▍ | 134/158 [1:31:42<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         [16:57:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▍ | 134/158 [1:31:44<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▍ | 134/158 [1:35:20<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▍ | 134/158 [1:35:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▍ | 134/158 [1:35:32<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▍ | 134/158 [1:35:32<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         [17:01:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▍ | 134/158 [1:35:34<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▍ | 134/158 [1:39:13<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▍ | 134/158 [1:39:22<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▍ | 134/158 [1:39:26<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▍ | 134/158 [1:39:26<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         [17:05:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▍ | 134/158 [1:39:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▍ | 134/158 [1:43:18<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▍ | 134/158 [1:43:27<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▍ | 134/158 [1:43:32<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▍ | 134/158 [1:43:32<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         [17:09:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▍ | 134/158 [1:43:33<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▍ | 134/158 [1:47:28<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▍ | 134/158 [1:47:38<4:35:35, 688.98s/trial, best loss: -3449.9525508693705] 85%|████████▌ | 135/158 [1:47:39<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 85%|████████▌ | 135/158 [1:47:39<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 85%|████████▌ | 135/158 [1:47:39<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▌ | 135/158 [1:49:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 85%|████████▌ | 135/158 [1:49:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         [17:14:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▌ | 135/158 [1:49:06<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▌ | 135/158 [1:49:31<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▌ | 135/158 [1:49:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▌ | 135/158 [1:49:37<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▌ | 135/158 [1:49:37<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         [17:15:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▌ | 135/158 [1:49:39<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▌ | 135/158 [1:50:02<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▌ | 135/158 [1:50:04<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▌ | 135/158 [1:50:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▌ | 135/158 [1:50:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         [17:15:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▌ | 135/158 [1:50:10<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▌ | 135/158 [1:50:33<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▌ | 135/158 [1:50:36<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▌ | 135/158 [1:50:40<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▌ | 135/158 [1:50:40<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         [17:16:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▌ | 135/158 [1:50:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▌ | 135/158 [1:51:06<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▌ | 135/158 [1:51:08<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 85%|████████▌ | 135/158 [1:51:13<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▌ | 135/158 [1:51:13<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         [17:17:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▌ | 135/158 [1:51:14<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 85%|████████▌ | 135/158 [1:51:39<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         True
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 85%|████████▌ | 135/158 [1:51:42<5:33:55, 871.13s/trial, best loss: -3449.9525508693705] 86%|████████▌ | 136/158 [1:51:42<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 86%|████████▌ | 136/158 [1:51:42<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 86%|████████▌ | 136/158 [1:51:42<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 86%|████████▌ | 136/158 [1:53:08<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 86%|████████▌ | 136/158 [1:53:08<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         [17:18:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 136/158 [1:53:10<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 86%|████████▌ | 136/158 [1:56:33<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         True
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 86%|████████▌ | 136/158 [1:56:39<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 86%|████████▌ | 136/158 [1:56:43<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 136/158 [1:56:43<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         [17:22:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 136/158 [1:56:45<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 86%|████████▌ | 136/158 [1:59:44<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         True
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 86%|████████▌ | 136/158 [1:59:49<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 86%|████████▌ | 136/158 [1:59:53<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 136/158 [1:59:53<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         [17:25:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 136/158 [1:59:56<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 86%|████████▌ | 136/158 [2:02:57<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         True
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 86%|████████▌ | 136/158 [2:03:03<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 86%|████████▌ | 136/158 [2:03:07<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 136/158 [2:03:07<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         [17:28:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 136/158 [2:03:10<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 86%|████████▌ | 136/158 [2:06:23<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         True
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 86%|████████▌ | 136/158 [2:06:29<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 86%|████████▌ | 136/158 [2:06:34<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 86%|████████▌ | 136/158 [2:06:34<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         [17:32:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 86%|████████▌ | 136/158 [2:06:36<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 86%|████████▌ | 136/158 [2:09:55<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         True
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 86%|████████▌ | 136/158 [2:10:02<4:10:21, 682.79s/trial, best loss: -3449.9525508693705] 87%|████████▋ | 137/158 [2:10:03<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 87%|████████▋ | 137/158 [2:10:03<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 87%|████████▋ | 137/158 [2:10:03<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 137/158 [2:11:28<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.05, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 87%|████████▋ | 137/158 [2:11:28<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         [17:37:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 137/158 [2:11:29<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 137/158 [2:11:32<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 137/158 [2:11:33<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 137/158 [2:11:37<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 137/158 [2:11:37<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         [17:37:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 137/158 [2:11:38<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 137/158 [2:11:41<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 137/158 [2:11:45<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 137/158 [2:11:45<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         [17:37:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 137/158 [2:11:47<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 137/158 [2:11:49<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 137/158 [2:11:50<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 137/158 [2:11:54<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 137/158 [2:11:54<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         [17:37:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 137/158 [2:11:56<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 137/158 [2:11:58<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 137/158 [2:11:59<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 137/158 [2:12:04<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 137/158 [2:12:04<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         [17:37:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 137/158 [2:12:05<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 137/158 [2:12:08<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 137/158 [2:12:09<4:42:51, 808.16s/trial, best loss: -3449.9525508693705] 87%|████████▋ | 138/158 [2:12:09<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 87%|████████▋ | 138/158 [2:12:09<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 87%|████████▋ | 138/158 [2:12:09<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 138/158 [2:13:34<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 87%|████████▋ | 138/158 [2:13:34<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         [17:39:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 138/158 [2:13:35<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 138/158 [2:14:04<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 138/158 [2:14:07<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 138/158 [2:14:10<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 138/158 [2:14:10<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         [17:39:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 138/158 [2:14:12<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 138/158 [2:14:40<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 138/158 [2:14:42<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 138/158 [2:14:46<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 138/158 [2:14:46<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         [17:40:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 138/158 [2:14:47<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 138/158 [2:15:15<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 138/158 [2:15:18<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 138/158 [2:15:22<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 138/158 [2:15:22<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         [17:41:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 138/158 [2:15:24<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 138/158 [2:15:52<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 138/158 [2:15:55<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 87%|████████▋ | 138/158 [2:15:59<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.2, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 87%|████████▋ | 138/158 [2:15:59<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         [17:41:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 87%|████████▋ | 138/158 [2:16:01<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 87%|████████▋ | 138/158 [2:16:29<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         True
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 87%|████████▋ | 138/158 [2:16:32<3:21:11, 603.57s/trial, best loss: -3449.9525508693705] 88%|████████▊ | 139/158 [2:16:33<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 88%|████████▊ | 139/158 [2:16:33<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 88%|████████▊ | 139/158 [2:16:33<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 88%|████████▊ | 139/158 [2:17:58<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.3, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 88%|████████▊ | 139/158 [2:17:58<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         [17:43:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 139/158 [2:17:59<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 88%|████████▊ | 139/158 [2:18:51<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         True
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 88%|████████▊ | 139/158 [2:18:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 88%|████████▊ | 139/158 [2:18:58<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 139/158 [2:18:58<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         [17:44:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 139/158 [2:18:59<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 88%|████████▊ | 139/158 [2:19:48<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         True
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 88%|████████▊ | 139/158 [2:19:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 88%|████████▊ | 139/158 [2:19:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 139/158 [2:19:54<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         [17:45:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 139/158 [2:19:56<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 88%|████████▊ | 139/158 [2:20:44<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         True
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 88%|████████▊ | 139/158 [2:20:47<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 88%|████████▊ | 139/158 [2:20:51<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 139/158 [2:20:51<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         [17:46:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 139/158 [2:20:53<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 88%|████████▊ | 139/158 [2:21:43<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         True
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 88%|████████▊ | 139/158 [2:21:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 88%|████████▊ | 139/158 [2:21:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.3, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 88%|████████▊ | 139/158 [2:21:50<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         [17:47:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 88%|████████▊ | 139/158 [2:21:52<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 88%|████████▊ | 139/158 [2:22:43<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         True
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 88%|████████▊ | 139/158 [2:22:46<2:38:49, 501.53s/trial, best loss: -3449.9525508693705] 89%|████████▊ | 140/158 [2:22:47<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 89%|████████▊ | 140/158 [2:22:47<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 89%|████████▊ | 140/158 [2:22:47<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▊ | 140/158 [2:24:12<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.35, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 89%|████████▊ | 140/158 [2:24:12<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         [17:49:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▊ | 140/158 [2:24:13<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▊ | 140/158 [2:25:17<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▊ | 140/158 [2:25:21<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▊ | 140/158 [2:25:25<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▊ | 140/158 [2:25:25<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         [17:51:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▊ | 140/158 [2:25:27<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▊ | 140/158 [2:26:26<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▊ | 140/158 [2:26:31<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▊ | 140/158 [2:26:35<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▊ | 140/158 [2:26:35<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         [17:52:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▊ | 140/158 [2:26:36<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▊ | 140/158 [2:27:36<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▊ | 140/158 [2:27:41<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▊ | 140/158 [2:27:45<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▊ | 140/158 [2:27:45<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         [17:53:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▊ | 140/158 [2:27:47<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▊ | 140/158 [2:28:49<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▊ | 140/158 [2:28:53<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▊ | 140/158 [2:28:58<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.35, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▊ | 140/158 [2:28:58<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         [17:54:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▊ | 140/158 [2:29:00<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▊ | 140/158 [2:30:02<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▊ | 140/158 [2:30:07<2:18:59, 463.29s/trial, best loss: -3449.9525508693705] 89%|████████▉ | 141/158 [2:30:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 89%|████████▉ | 141/158 [2:30:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 89%|████████▉ | 141/158 [2:30:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▉ | 141/158 [2:31:33<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=15,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 89%|████████▉ | 141/158 [2:31:33<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         [17:57:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 141/158 [2:31:35<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▉ | 141/158 [2:34:06<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▉ | 141/158 [2:34:10<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▉ | 141/158 [2:34:14<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 141/158 [2:34:14<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         [18:00:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 141/158 [2:34:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▉ | 141/158 [2:36:31<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▉ | 141/158 [2:36:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▉ | 141/158 [2:36:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 141/158 [2:36:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         [18:02:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 141/158 [2:36:41<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▉ | 141/158 [2:39:03<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▉ | 141/158 [2:39:08<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▉ | 141/158 [2:39:12<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 141/158 [2:39:12<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         [18:05:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 141/158 [2:39:14<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▉ | 141/158 [2:41:36<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▉ | 141/158 [2:41:40<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 89%|████████▉ | 141/158 [2:41:45<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 89%|████████▉ | 141/158 [2:41:45<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         [18:07:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 89%|████████▉ | 141/158 [2:41:46<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 89%|████████▉ | 141/158 [2:44:11<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         True
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 89%|████████▉ | 141/158 [2:44:16<2:09:24, 456.71s/trial, best loss: -3449.9525508693705] 90%|████████▉ | 142/158 [2:44:17<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 90%|████████▉ | 142/158 [2:44:17<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 90%|████████▉ | 142/158 [2:44:17<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 90%|████████▉ | 142/158 [2:45:42<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.1, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.45, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 90%|████████▉ | 142/158 [2:45:42<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         [18:11:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|████████▉ | 142/158 [2:45:44<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 90%|████████▉ | 142/158 [2:46:06<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         True
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 90%|████████▉ | 142/158 [2:46:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 90%|████████▉ | 142/158 [2:46:12<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|████████▉ | 142/158 [2:46:12<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         [18:11:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|████████▉ | 142/158 [2:46:13<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 90%|████████▉ | 142/158 [2:46:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         True
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 90%|████████▉ | 142/158 [2:46:36<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 90%|████████▉ | 142/158 [2:46:40<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|████████▉ | 142/158 [2:46:40<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         [18:12:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|████████▉ | 142/158 [2:46:41<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 90%|████████▉ | 142/158 [2:47:02<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         True
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 90%|████████▉ | 142/158 [2:47:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 90%|████████▉ | 142/158 [2:47:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|████████▉ | 142/158 [2:47:08<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         [18:12:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|████████▉ | 142/158 [2:47:10<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 90%|████████▉ | 142/158 [2:47:31<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         True
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 90%|████████▉ | 142/158 [2:47:34<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 90%|████████▉ | 142/158 [2:47:38<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.1, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.45, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.3,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 90%|████████▉ | 142/158 [2:47:38<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         [18:13:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 90%|████████▉ | 142/158 [2:47:40<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 90%|████████▉ | 142/158 [2:48:02<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         True
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 90%|████████▉ | 142/158 [2:48:04<2:33:08, 574.31s/trial, best loss: -3449.9525508693705] 91%|█████████ | 143/158 [2:48:05<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 91%|█████████ | 143/158 [2:48:05<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 91%|█████████ | 143/158 [2:48:05<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 143/158 [2:49:30<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.01, max_delta_step=None, max_depth=20,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=50, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 91%|█████████ | 143/158 [2:49:30<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         [18:15:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 143/158 [2:49:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 143/158 [2:50:45<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 143/158 [2:50:46<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 143/158 [2:50:50<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 143/158 [2:50:50<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         [18:16:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 143/158 [2:50:52<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 143/158 [2:51:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 143/158 [2:51:35<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 143/158 [2:51:38<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 143/158 [2:51:38<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         [18:17:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 143/158 [2:51:41<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 143/158 [2:52:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 143/158 [2:52:33<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 143/158 [2:52:37<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 143/158 [2:52:37<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         [18:18:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 143/158 [2:52:40<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 143/158 [2:53:31<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 143/158 [2:53:32<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 143/158 [2:53:36<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.01, max_delta_step=0, max_depth=20,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=50, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.9,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 143/158 [2:53:36<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         [18:19:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 143/158 [2:53:39<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 143/158 [2:54:22<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 143/158 [2:54:23<1:57:36, 470.44s/trial, best loss: -3449.9525508693705] 91%|█████████ | 144/158 [2:54:23<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 91%|█████████ | 144/158 [2:54:23<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 91%|█████████ | 144/158 [2:54:23<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 144/158 [2:55:48<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.9, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 91%|█████████ | 144/158 [2:55:48<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         [18:21:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 144/158 [2:55:50<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 144/158 [2:56:18<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 144/158 [2:56:20<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 144/158 [2:56:24<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 144/158 [2:56:24<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         [18:22:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 144/158 [2:56:25<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 144/158 [2:56:51<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 144/158 [2:56:53<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 144/158 [2:56:57<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 144/158 [2:56:57<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         [18:22:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 144/158 [2:56:59<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 144/158 [2:57:25<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 144/158 [2:57:27<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 144/158 [2:57:32<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 144/158 [2:57:32<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         [18:23:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 144/158 [2:57:33<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 144/158 [2:58:00<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 144/158 [2:58:02<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 91%|█████████ | 144/158 [2:58:07<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.7,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 91%|█████████ | 144/158 [2:58:07<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         [18:23:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 91%|█████████ | 144/158 [2:58:08<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 91%|█████████ | 144/158 [2:58:35<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         True
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 91%|█████████ | 144/158 [2:58:38<1:43:20, 442.88s/trial, best loss: -3449.9525508693705] 92%|█████████▏| 145/158 [2:58:39<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 92%|█████████▏| 145/158 [2:58:39<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 92%|█████████▏| 145/158 [2:58:39<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 145/158 [3:00:03<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 92%|█████████▏| 145/158 [3:00:03<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         [18:25:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 145/158 [3:00:05<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 145/158 [3:00:31<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 145/158 [3:00:33<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 145/158 [3:00:37<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 145/158 [3:00:37<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         [18:26:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 145/158 [3:00:38<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 145/158 [3:01:02<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 145/158 [3:01:04<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 145/158 [3:01:08<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 145/158 [3:01:08<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         [18:26:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 145/158 [3:01:10<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 145/158 [3:01:34<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 145/158 [3:01:36<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 145/158 [3:01:41<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 145/158 [3:01:41<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         [18:27:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 145/158 [3:01:42<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 145/158 [3:02:07<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 145/158 [3:02:09<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 145/158 [3:02:14<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 145/158 [3:02:14<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         [18:28:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 145/158 [3:02:15<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 145/158 [3:02:41<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 145/158 [3:02:43<1:23:45, 386.59s/trial, best loss: -3449.9525508693705] 92%|█████████▏| 146/158 [3:02:44<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 92%|█████████▏| 146/158 [3:02:44<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         New call of hyperopt_train_test
 92%|█████████▏| 146/158 [3:02:44<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 146/158 [3:04:08<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 92%|█████████▏| 146/158 [3:04:08<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         [18:29:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 146/158 [3:04:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 146/158 [3:04:36<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         (259530, 139)
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 146/158 [3:04:38<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 146/158 [3:04:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 146/158 [3:04:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         [18:30:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 146/158 [3:04:43<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 146/158 [3:05:07<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         (271631, 139)
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 146/158 [3:05:10<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 146/158 [3:05:14<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 146/158 [3:05:14<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         [18:31:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 146/158 [3:05:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 146/158 [3:05:39<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         (286101, 139)
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 146/158 [3:05:42<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 146/158 [3:05:46<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 146/158 [3:05:46<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         [18:31:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 146/158 [3:05:48<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 146/158 [3:06:12<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         (289002, 139)
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 146/158 [3:06:15<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Model used for fitting:
 92%|█████████▏| 146/158 [3:06:19<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 92%|█████████▏| 146/158 [3:06:19<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         [18:32:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 92%|█████████▏| 146/158 [3:06:21<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         predict called
 92%|█████████▏| 146/158 [3:06:46<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of X:
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'pandas.core.frame.DataFrame'>
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Shape of X:
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         (323071, 139)
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         Type of y:
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         <class 'numpy.ndarray'>
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         model fitted ?
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         True
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705]                                                                                         y is not None
 92%|█████████▏| 146/158 [3:06:49<1:08:50, 344.17s/trial, best loss: -3449.9525508693705] 93%|█████████▎| 147/158 [3:06:49<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                         New call of f
 93%|█████████▎| 147/158 [3:06:49<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 93%|█████████▎| 147/158 [3:06:49<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 93%|█████████▎| 147/158 [3:08:14<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 93%|█████████▎| 147/158 [3:08:14<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       [18:34:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 147/158 [3:08:16<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 93%|█████████▎| 147/158 [3:08:42<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       True
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 93%|█████████▎| 147/158 [3:08:44<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 93%|█████████▎| 147/158 [3:08:48<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 147/158 [3:08:48<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       [18:34:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 147/158 [3:08:49<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 93%|█████████▎| 147/158 [3:09:13<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       True
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 93%|█████████▎| 147/158 [3:09:15<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 93%|█████████▎| 147/158 [3:09:19<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 147/158 [3:09:19<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       [18:35:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 147/158 [3:09:21<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 93%|█████████▎| 147/158 [3:09:45<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       True
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 93%|█████████▎| 147/158 [3:09:47<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 93%|█████████▎| 147/158 [3:09:52<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 147/158 [3:09:52<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       [18:35:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 147/158 [3:09:53<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 93%|█████████▎| 147/158 [3:10:18<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       True
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 93%|█████████▎| 147/158 [3:10:20<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 93%|█████████▎| 147/158 [3:10:25<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 93%|█████████▎| 147/158 [3:10:25<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       [18:36:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 93%|█████████▎| 147/158 [3:10:26<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 93%|█████████▎| 147/158 [3:10:51<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       True
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 93%|█████████▎| 147/158 [3:10:54<57:39, 314.51s/trial, best loss: -3449.9525508693705] 94%|█████████▎| 148/158 [3:10:55<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 94%|█████████▎| 148/158 [3:10:55<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 94%|█████████▎| 148/158 [3:10:55<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▎| 148/158 [3:12:20<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 94%|█████████▎| 148/158 [3:12:20<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:38:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▎| 148/158 [3:12:22<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▎| 148/158 [3:12:48<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▎| 148/158 [3:12:50<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▎| 148/158 [3:12:54<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▎| 148/158 [3:12:54<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:38:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▎| 148/158 [3:12:55<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▎| 148/158 [3:13:19<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▎| 148/158 [3:13:21<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▎| 148/158 [3:13:25<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▎| 148/158 [3:13:25<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:39:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▎| 148/158 [3:13:27<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▎| 148/158 [3:13:51<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▎| 148/158 [3:13:53<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▎| 148/158 [3:13:58<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▎| 148/158 [3:13:58<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:39:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▎| 148/158 [3:13:59<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▎| 148/158 [3:14:24<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▎| 148/158 [3:14:26<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▎| 148/158 [3:14:31<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▎| 148/158 [3:14:31<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:40:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▎| 148/158 [3:14:33<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▎| 148/158 [3:14:58<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▎| 148/158 [3:15:00<48:58, 293.84s/trial, best loss: -3449.9525508693705] 94%|█████████▍| 149/158 [3:15:01<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 94%|█████████▍| 149/158 [3:15:01<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 94%|█████████▍| 149/158 [3:15:01<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▍| 149/158 [3:16:26<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 94%|█████████▍| 149/158 [3:16:26<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       [18:42:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 149/158 [3:16:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▍| 149/158 [3:16:53<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▍| 149/158 [3:16:56<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▍| 149/158 [3:16:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 149/158 [3:16:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       [18:42:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 149/158 [3:17:01<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▍| 149/158 [3:17:25<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▍| 149/158 [3:17:27<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▍| 149/158 [3:17:31<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 149/158 [3:17:31<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       [18:43:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 149/158 [3:17:33<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▍| 149/158 [3:17:57<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▍| 149/158 [3:17:59<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▍| 149/158 [3:18:04<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 149/158 [3:18:04<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       [18:43:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 149/158 [3:18:05<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▍| 149/158 [3:18:30<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▍| 149/158 [3:18:32<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 94%|█████████▍| 149/158 [3:18:37<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 94%|█████████▍| 149/158 [3:18:37<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       [18:44:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 94%|█████████▍| 149/158 [3:18:38<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 94%|█████████▍| 149/158 [3:19:03<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       True
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 94%|█████████▍| 149/158 [3:19:06<41:56, 279.56s/trial, best loss: -3449.9525508693705] 95%|█████████▍| 150/158 [3:19:07<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 95%|█████████▍| 150/158 [3:19:07<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 95%|█████████▍| 150/158 [3:19:07<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 95%|█████████▍| 150/158 [3:20:31<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 95%|█████████▍| 150/158 [3:20:31<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       [18:46:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 150/158 [3:20:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 95%|█████████▍| 150/158 [3:20:59<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       True
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 95%|█████████▍| 150/158 [3:21:01<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 95%|█████████▍| 150/158 [3:21:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 150/158 [3:21:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       [18:46:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 150/158 [3:21:06<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 95%|█████████▍| 150/158 [3:21:30<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       True
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 95%|█████████▍| 150/158 [3:21:33<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 95%|█████████▍| 150/158 [3:21:36<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 150/158 [3:21:36<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       [18:47:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 150/158 [3:21:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 95%|█████████▍| 150/158 [3:22:02<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       True
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 95%|█████████▍| 150/158 [3:22:05<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 95%|█████████▍| 150/158 [3:22:09<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 150/158 [3:22:09<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       [18:47:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 150/158 [3:22:10<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 95%|█████████▍| 150/158 [3:22:35<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       True
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 95%|█████████▍| 150/158 [3:22:38<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 95%|█████████▍| 150/158 [3:22:42<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 95%|█████████▍| 150/158 [3:22:42<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       [18:48:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 95%|█████████▍| 150/158 [3:22:44<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 95%|█████████▍| 150/158 [3:23:09<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       True
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 95%|█████████▍| 150/158 [3:23:12<35:55, 269.42s/trial, best loss: -3449.9525508693705] 96%|█████████▌| 151/158 [3:23:12<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 96%|█████████▌| 151/158 [3:23:12<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 96%|█████████▌| 151/158 [3:23:12<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 151/158 [3:24:36<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▌| 151/158 [3:24:36<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       [18:50:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 151/158 [3:24:38<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 151/158 [3:25:04<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 151/158 [3:25:06<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 151/158 [3:25:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 151/158 [3:25:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       [18:50:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 151/158 [3:25:11<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 151/158 [3:25:35<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 151/158 [3:25:37<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 151/158 [3:25:41<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 151/158 [3:25:41<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       [18:51:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 151/158 [3:25:43<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 151/158 [3:26:07<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 151/158 [3:26:10<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 151/158 [3:26:14<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 151/158 [3:26:14<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       [18:52:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 151/158 [3:26:15<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 151/158 [3:26:40<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 151/158 [3:26:42<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 151/158 [3:26:47<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 151/158 [3:26:47<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       [18:52:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 151/158 [3:26:49<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 151/158 [3:27:14<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 151/158 [3:27:16<30:35, 262.19s/trial, best loss: -3449.9525508693705] 96%|█████████▌| 152/158 [3:27:17<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 96%|█████████▌| 152/158 [3:27:17<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 96%|█████████▌| 152/158 [3:27:17<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 152/158 [3:28:41<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.03, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 96%|█████████▌| 152/158 [3:28:41<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       [18:54:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 152/158 [3:28:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 152/158 [3:29:09<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 152/158 [3:29:11<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 152/158 [3:29:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 152/158 [3:29:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       [18:55:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 152/158 [3:29:17<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 152/158 [3:29:41<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 152/158 [3:29:43<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 152/158 [3:29:47<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 152/158 [3:29:47<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       [18:55:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 152/158 [3:29:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 152/158 [3:30:13<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 152/158 [3:30:15<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 152/158 [3:30:20<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 152/158 [3:30:20<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       [18:56:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 152/158 [3:30:21<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 152/158 [3:30:46<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 152/158 [3:30:49<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 96%|█████████▌| 152/158 [3:30:53<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.03, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 96%|█████████▌| 152/158 [3:30:53<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       [18:56:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 96%|█████████▌| 152/158 [3:30:55<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 96%|█████████▌| 152/158 [3:31:20<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       True
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 96%|█████████▌| 152/158 [3:31:23<25:42, 257.01s/trial, best loss: -3449.9525508693705] 97%|█████████▋| 153/158 [3:31:23<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 97%|█████████▋| 153/158 [3:31:24<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 97%|█████████▋| 153/158 [3:31:24<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 153/158 [3:32:48<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.5, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 153/158 [3:32:48<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:58:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 153/158 [3:32:50<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 153/158 [3:33:18<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 153/158 [3:33:21<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 153/158 [3:33:25<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 153/158 [3:33:25<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:59:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 153/158 [3:33:26<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 153/158 [3:33:53<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 153/158 [3:33:55<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 153/158 [3:33:59<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 153/158 [3:33:59<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [18:59:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 153/158 [3:34:01<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 153/158 [3:34:28<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 153/158 [3:34:30<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 153/158 [3:34:34<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 153/158 [3:34:34<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [19:00:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 153/158 [3:34:36<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 153/158 [3:35:03<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 153/158 [3:35:06<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 153/158 [3:35:10<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.5, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 153/158 [3:35:10<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [19:00:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 153/158 [3:35:12<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 153/158 [3:35:40<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 153/158 [3:35:43<21:09, 253.84s/trial, best loss: -3449.9525508693705] 97%|█████████▋| 154/158 [3:35:43<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 97%|█████████▋| 154/158 [3:35:43<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 97%|█████████▋| 154/158 [3:35:43<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 154/158 [3:37:08<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.1, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 97%|█████████▋| 154/158 [3:37:08<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       [19:02:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 154/158 [3:37:10<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 154/158 [3:37:36<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 154/158 [3:37:38<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 154/158 [3:37:42<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 154/158 [3:37:42<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       [19:03:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 154/158 [3:37:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 154/158 [3:38:09<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 154/158 [3:38:11<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 154/158 [3:38:15<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 154/158 [3:38:15<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       [19:04:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 154/158 [3:38:16<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 154/158 [3:38:41<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 154/158 [3:38:44<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 154/158 [3:38:48<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 154/158 [3:38:48<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       [19:04:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 154/158 [3:38:50<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 154/158 [3:39:15<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 154/158 [3:39:17<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 97%|█████████▋| 154/158 [3:39:22<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.1, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.2,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 97%|█████████▋| 154/158 [3:39:22<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       [19:05:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 97%|█████████▋| 154/158 [3:39:24<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 97%|█████████▋| 154/158 [3:39:50<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       True
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 97%|█████████▋| 154/158 [3:39:52<17:02, 255.64s/trial, best loss: -3449.9525508693705] 98%|█████████▊| 155/158 [3:39:53<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 98%|█████████▊| 155/158 [3:39:53<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 98%|█████████▊| 155/158 [3:39:53<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 98%|█████████▊| 155/158 [3:41:18<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.6, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.15, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 98%|█████████▊| 155/158 [3:41:18<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [19:07:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 155/158 [3:41:20<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 98%|█████████▊| 155/158 [3:41:47<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 98%|█████████▊| 155/158 [3:41:49<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 98%|█████████▊| 155/158 [3:41:53<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 155/158 [3:41:53<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [19:07:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 155/158 [3:41:55<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 98%|█████████▊| 155/158 [3:42:20<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 98%|█████████▊| 155/158 [3:42:23<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 98%|█████████▊| 155/158 [3:42:27<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 155/158 [3:42:27<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [19:08:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 155/158 [3:42:28<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 98%|█████████▊| 155/158 [3:42:54<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 98%|█████████▊| 155/158 [3:42:57<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 98%|█████████▊| 155/158 [3:43:01<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 155/158 [3:43:01<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [19:08:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 155/158 [3:43:02<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 98%|█████████▊| 155/158 [3:43:29<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 98%|█████████▊| 155/158 [3:43:31<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 98%|█████████▊| 155/158 [3:43:36<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.15, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.4,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 98%|█████████▊| 155/158 [3:43:36<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       [19:09:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 98%|█████████▊| 155/158 [3:43:37<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 98%|█████████▊| 155/158 [3:44:04<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       True
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 98%|█████████▊| 155/158 [3:44:07<12:41, 253.84s/trial, best loss: -3449.9525508693705] 99%|█████████▊| 156/158 [3:44:07<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 99%|█████████▊| 156/158 [3:44:07<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 99%|█████████▊| 156/158 [3:44:07<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▊| 156/158 [3:45:32<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.5, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.4, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▊| 156/158 [3:45:32<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       [19:11:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 156/158 [3:45:34<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▊| 156/158 [3:46:01<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▊| 156/158 [3:46:03<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▊| 156/158 [3:46:07<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 156/158 [3:46:07<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       [19:11:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 156/158 [3:46:09<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▊| 156/158 [3:46:34<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▊| 156/158 [3:46:36<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▊| 156/158 [3:46:40<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 156/158 [3:46:40<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       [19:12:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 156/158 [3:46:42<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▊| 156/158 [3:47:08<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▊| 156/158 [3:47:10<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▊| 156/158 [3:47:14<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 156/158 [3:47:14<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       [19:13:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 156/158 [3:47:16<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▊| 156/158 [3:47:42<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▊| 156/158 [3:47:45<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▊| 156/158 [3:47:49<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.4, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▊| 156/158 [3:47:49<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       [19:13:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▊| 156/158 [3:47:51<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▊| 156/158 [3:48:18<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▊| 156/158 [3:48:21<08:28, 254.03s/trial, best loss: -3449.9525508693705] 99%|█████████▉| 157/158 [3:48:21<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 99%|█████████▉| 157/158 [3:48:21<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 99%|█████████▉| 157/158 [3:48:21<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▉| 157/158 [3:49:46<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.8, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 99%|█████████▉| 157/158 [3:49:46<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       [19:15:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 157/158 [3:49:48<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▉| 157/158 [3:50:15<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▉| 157/158 [3:50:17<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▉| 157/158 [3:50:21<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 157/158 [3:50:21<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       [19:16:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 157/158 [3:50:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▉| 157/158 [3:50:48<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       (271631, 139)
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▉| 157/158 [3:50:50<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▉| 157/158 [3:50:54<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 157/158 [3:50:54<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       [19:16:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 157/158 [3:50:55<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▉| 157/158 [3:51:21<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       (286101, 139)
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▉| 157/158 [3:51:23<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▉| 157/158 [3:51:28<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 157/158 [3:51:28<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       [19:17:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 157/158 [3:51:29<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▉| 157/158 [3:51:55<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       (289002, 139)
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▉| 157/158 [3:51:57<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 99%|█████████▉| 157/158 [3:52:02<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 99%|█████████▉| 157/158 [3:52:02<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       [19:17:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 99%|█████████▉| 157/158 [3:52:03<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 99%|█████████▉| 157/158 [3:52:30<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       (323071, 139)
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       True
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 99%|█████████▉| 157/158 [3:52:32<04:13, 253.93s/trial, best loss: -3449.9525508693705]100%|██████████| 158/158 [3:52:33<00:00, 253.29s/trial, best loss: -3449.9525508693705]100%|██████████| 158/158 [3:52:33<00:00, 88.31s/trial, best loss: -3449.9525508693705] 
Best model so far :
{'colsample_bytree': 5, 'features': 0, 'learning_rate': 1, 'max_depth': 1, 'n_estimators': 2, 'random_state': 0, 'subsample': 3, 'tree_method': 0}

OPTIMIZATION STEP COMPLETE.

Run 1 since program start
2020-12-20 19:18:19.253341
Found saved Trials! Loading...
Rerunning from 158 trials to add another one.
 84%|████████▍ | 158/188 [00:00<?, ?trial/s, best loss=?]                                                         New call of f
 84%|████████▍ | 158/188 [00:00<00:00, 6856.56trial/s, best loss=?]                                                                   New call of hyperopt_train_test
 84%|████████▍ | 158/188 [00:00<00:00, 6803.20trial/s, best loss=?]                                                                   Model used for fitting:
 84%|████████▍ | 158/188 [01:26<00:16,  1.84trial/s, best loss=?]                                                                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.4, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.25, max_delta_step=None, max_depth=12,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=500, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 84%|████████▍ | 158/188 [01:26<00:16,  1.84trial/s, best loss=?]                                                                 [19:19:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 158/188 [01:27<00:16,  1.80trial/s, best loss=?]                                                                 predict called
 84%|████████▍ | 158/188 [02:26<00:27,  1.08trial/s, best loss=?]                                                                 Type of X:
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 Shape of X:
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 (259530, 139)
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 Type of y:
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 <class 'numpy.ndarray'>
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 model fitted ?
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 True
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 y is not None
 84%|████████▍ | 158/188 [02:29<00:28,  1.06trial/s, best loss=?]                                                                 Model used for fitting:
 84%|████████▍ | 158/188 [02:33<00:29,  1.03trial/s, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 158/188 [02:33<00:29,  1.03trial/s, best loss=?]                                                                 [19:20:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 158/188 [02:34<00:29,  1.02trial/s, best loss=?]                                                                 predict called
 84%|████████▍ | 158/188 [03:29<00:39,  1.33s/trial, best loss=?]                                                                 Type of X:
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 Shape of X:
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 (271631, 139)
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 Type of y:
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 model fitted ?
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 True
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 y is not None
 84%|████████▍ | 158/188 [03:32<00:40,  1.34s/trial, best loss=?]                                                                 Model used for fitting:
 84%|████████▍ | 158/188 [03:36<00:41,  1.37s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 158/188 [03:36<00:41,  1.37s/trial, best loss=?]                                                                 [19:21:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 158/188 [03:37<00:41,  1.38s/trial, best loss=?]                                                                 predict called
 84%|████████▍ | 158/188 [04:33<00:51,  1.73s/trial, best loss=?]                                                                 Type of X:
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 Shape of X:
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 (286101, 139)
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 Type of y:
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 model fitted ?
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 True
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 y is not None
 84%|████████▍ | 158/188 [04:36<00:52,  1.75s/trial, best loss=?]                                                                 Model used for fitting:
 84%|████████▍ | 158/188 [04:40<00:53,  1.77s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 158/188 [04:40<00:53,  1.77s/trial, best loss=?]                                                                 [19:23:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 158/188 [04:42<00:53,  1.79s/trial, best loss=?]                                                                 predict called
 84%|████████▍ | 158/188 [05:38<01:04,  2.14s/trial, best loss=?]                                                                 Type of X:
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 Shape of X:
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 (289002, 139)
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 Type of y:
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 model fitted ?
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 True
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 y is not None
 84%|████████▍ | 158/188 [05:41<01:04,  2.16s/trial, best loss=?]                                                                 Model used for fitting:
 84%|████████▍ | 158/188 [05:46<01:05,  2.19s/trial, best loss=?]                                                                 XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.25, max_delta_step=0, max_depth=12,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=500, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.5,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 84%|████████▍ | 158/188 [05:46<01:05,  2.19s/trial, best loss=?]                                                                 [19:24:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 84%|████████▍ | 158/188 [05:47<01:06,  2.20s/trial, best loss=?]                                                                 predict called
 84%|████████▍ | 158/188 [06:45<01:16,  2.56s/trial, best loss=?]                                                                 Type of X:
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 <class 'pandas.core.frame.DataFrame'>
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 Shape of X:
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 (323071, 139)
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 Type of y:
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 <class 'numpy.ndarray'>
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 model fitted ?
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 True
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?]                                                                 y is not None
 84%|████████▍ | 158/188 [06:48<01:17,  2.59s/trial, best loss=?] 85%|████████▍ | 159/188 [06:49<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       New call of f
 85%|████████▍ | 159/188 [06:49<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       New call of hyperopt_train_test
 85%|████████▍ | 159/188 [06:49<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 85%|████████▍ | 159/188 [08:15<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.2, gamma=None,
              gpu_id=None, importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=None, max_depth=10,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=None, num_parallel_tree=None,
              objective='binary:logistic', random_state=42, reg_alpha=None,
              reg_lambda=None, scale_pos_weight=None, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=None, verbosity=None)
 85%|████████▍ | 159/188 [08:15<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       [19:26:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▍ | 159/188 [08:17<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       predict called
 85%|████████▍ | 159/188 [09:03<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       Type of X:
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       <class 'pandas.core.frame.DataFrame'>
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       Shape of X:
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       (259530, 139)
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       Type of y:
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       <class 'numpy.ndarray'>
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       model fitted ?
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       True
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       y is not None
 85%|████████▍ | 159/188 [09:06<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       Model used for fitting:
 85%|████████▍ | 159/188 [09:10<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=1000, n_jobs=8, num_parallel_tree=1,
              objective='binary:logistic', random_state=42, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, subsample=0.6,
              tree_method='gpu_hist', use_label_encoder=True,
              validate_parameters=1, verbosity=None)
 85%|████████▍ | 159/188 [09:10<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]                                                                                       [19:27:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
 85%|████████▍ | 159/188 [09:12<3:17:48, 409.25s/trial, best loss: -3449.9525508693705]