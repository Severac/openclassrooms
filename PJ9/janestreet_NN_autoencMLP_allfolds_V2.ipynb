{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch classifier notebook\n",
    "\n",
    "V1 : only 1 split. First implementation  \n",
    "All folds V1 : with all folds  \n",
    "All folds V2 : add activation stats plot  \n",
    "All folds V2.1 : back to  best MLP found so far, and backport fix of activation layers stats. Add weight decay and scheduler (fit one cycle) code\n",
    "\n",
    "All folds autoencoder MLP V1  \n",
    "All folds autoencoder MLP V2 : with weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torch.optim as optim\n",
    "import torch_optimizer as optim  # Custom optimizers (not officially pytorch) : to use RAdam https://pypi.org/project/torch-optimizer/#radam\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import PIL.Image\n",
    "\n",
    "import datetime\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET_INPUT_FILE = 'train.csv'\n",
    "\n",
    "#FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)] + ['cross_41_42_43', 'cross_1_2']\n",
    "FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)]\n",
    "\n",
    "# For custom non-overlaped folds generation\n",
    "TRAIN_PERCENT = 0.70  \n",
    "TEST_PERCENT = 0.30\n",
    "\n",
    "# If subsplit of training set : percentage of second training set  \n",
    "TRAIN1_PERCENT = 0.20  \n",
    "\n",
    "ACT_N = False  # Add N previous predictions to input of MLP <= Does not work, logic is not right\n",
    "ACT_N_SIZE = 5\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Behavior\n",
    "seed = 42\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Torch RNG\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)\n",
    "# CuDA Determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_SWEEP = True\n",
    "DO_SINGLE_TRAIN = False\n",
    "#BATCH_SIZE = 50000\n",
    "#BATCH_SIZE = 4096 # Gave once better results than 50000\n",
    "#BATCH_SIZE = 2048\n",
    "\n",
    "#BATCH_SIZE = 300000\n",
    "\n",
    "#BATCH_SIZE = 4096\n",
    "#BATCH_SIZE = 8192\n",
    "#BATCH_SIZE = 32768\n",
    "BATCH_SIZE = 8192\n",
    "WEIGHT_DECAY = 1e-4 # Remettre à 1e-5\n",
    "LEARNING_RATE = 1e-4\n",
    "DROPOUT = 0.7\n",
    "\n",
    "EARLY_STOPPING = True\n",
    "\n",
    "NUM_EPOCHS = 1000\n",
    "#NUM_EPOCHS = 36\n",
    "\n",
    "MODEL_FILE = f'model_NN_allfolds_V1.pt'\n",
    "\n",
    "BATCH_SIZE_AE = 40960\n",
    "NUM_EPOCHS_AE = 1000\n",
    "LEARNING_RATE_AE = 1e-3\n",
    "WEIGHT_DECAY_AE = 1e-4\n",
    "MODEL_FILE_AE = f'model_NN_AE_allfolds_V1.pt'\n",
    "\n",
    "RETRAIN_MODEl_AE = False\n",
    "\n",
    "MODEL_COMMENT_AE = f'All folds MLP autoenc, 2 layers 64 32, good model reloaded, batch size {BATCH_SIZE_AE}, lr={LEARNING_RATE_AE}, patience 5, standard scale, weight decay {WEIGHT_DECAY_AE}, dropout 0.5, with cross features, no scheduler, no std scale'\n",
    "MODEL_COMMENT = f'All folds MLP with autoenc (noise 0.01)), 3 layers 130, 200 and 100, good model reloaded, batch size {BATCH_SIZE}, lr={LEARNING_RATE}, patience 5, standard scale, weight decay {WEIGHT_DECAY}, 0.7 dropout, without cross features, no scheduler, no std scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'Best utility',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [524288, 262144, 131072, 65536, 32768, 16384, 8192, 4096, 2048, 1024, 512]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            #'values': [1e-2, 1e-3, 1e-4, 3e-4, 1e-5]\n",
    "            #'values': [1e-2, 1e-3, 1e-4]\n",
    "            'values': [1e-2, 1e-3]\n",
    "        },\n",
    "\n",
    "        'weight_decay': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        },\n",
    "    \n",
    "        'use_autoenc': {\n",
    "            'values': ['encoder-decoder', 'encoder', 'encoder-only' 'None']\n",
    "            #'values': ['encoder-decoder', 'None']\n",
    "        },\n",
    "        \n",
    "        'activation_function': {\n",
    "            'values': ['relu', 'leakyrelu']\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyStandardScale(tensor, mean, std):\n",
    "    return((tensor - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# this is code slightly modified from the sklearn docs here:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cv_indices_custom(cv_custom, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv_custom):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accounts for variable instance counts in each split by dividing utility_pi by number of instances (but this has been removed)\n",
    "# It also does some copy of dataframe to prevent memory overwrite\n",
    "def utility_function(df_test, df_test_predictions):\n",
    "    df_test_copy = df_test.copy(deep=True)\n",
    "    df_test_copy.loc[:, 'utility_pj'] = df_test_copy['weight'] * df_test_copy['resp'] * df_test_predictions\n",
    "    #df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum() / df_test_copy.groupby('date')['utility_pj'].count()\n",
    "    df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum()\n",
    "\n",
    "    nb_unique_dates = df_test_utility_pi.shape[0]\n",
    "\n",
    "    if (np.sqrt(df_test_utility_pi.pow(2).sum()) == 0):\n",
    "        t = 0\n",
    "\n",
    "    else:\n",
    "        t = (df_test_utility_pi.sum() / np.sqrt(df_test_utility_pi.pow(2).sum())) * (np.sqrt(250 / np.abs(nb_unique_dates)))\n",
    "\n",
    "    u = min(max(t, 0), 6) * df_test_utility_pi.sum()\n",
    "    del df_test_copy\n",
    "    \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "# The aim of this function is to return closest date from an index\n",
    "# So that split indices correspond to start or end of a new day\n",
    "# myList contains list of instances that correspond to start of a new da\n",
    "\n",
    "def take_closest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutputActivationStats:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        #self.outputs.append(module_out)\n",
    "        #print('Save output callback :')\n",
    "        #print(module)\n",
    "        #print({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        self.outputs.append({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#\n",
    "#plot_cv_indices(cv, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.device"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 3090'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "    \n",
    "df = pd.read_csv(DATASET_INPUT_FILE)\n",
    "df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_41_42_43'] = df['feature_41'] + df['feature_42'] + df['feature_43']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_1_2'] = df['feature_1'] / (df['feature_2'] + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non overlap fold generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_indexes_list = df.groupby('date')['ts_id'].first().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_split_size = int((df.shape[0] // 5) * TRAIN_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_split_size = int((df.shape[0] // 5) * TEST_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 477711, 958233, 1435933, 1913985]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_start_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have 5 folds of 3 subsets each (2 training sets and 1 test set per fold)\n",
    "# (1st training set of each fold will be used for 1st model, ie auto encoder)\n",
    "\n",
    "NB_FOLDS = 5\n",
    "last_index = df.shape[0] - 1\n",
    "\n",
    "cv_table = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_train_start_index = train_split_start_indexes[fold_indice]\n",
    "    \n",
    "    if (fold_indice == NB_FOLDS - 1):    \n",
    "        nextfold_train_start_index = last_index\n",
    "        \n",
    "    else:\n",
    "        nextfold_train_start_index = train_split_start_indexes[fold_indice + 1]\n",
    "    \n",
    "    fold_test_start_index = take_closest(date_indexes_list, int(TRAIN_PERCENT * (nextfold_train_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    fold_train2_start_index = take_closest(date_indexes_list, int(TRAIN1_PERCENT * (fold_test_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    \n",
    "    cv_table.append(fold_train_start_index)\n",
    "    cv_table.append(fold_train2_start_index)\n",
    "    cv_table.append(fold_test_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_table.append(last_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples = []\n",
    "\n",
    "for i in range(0, NB_FOLDS*3, 3):\n",
    "    cv_tuples.append([df.loc[cv_table[i]:cv_table[i+1]-1, :].index.to_list(), df.loc[cv_table[i+1]:cv_table[i+2]-1, :].index.to_list(),\n",
    "                      df.loc[cv_table[i+2]:cv_table[i+3]-1, :].index.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141102"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_tuples[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#plot_cv_indices_custom(cv_tuples_generator, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20); \n",
    "\n",
    "#cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of training set :\n",
    "#train_sets_table =  [cv_tuples[i][0] for i in range(5)]\n",
    "#sum([len(train_set_table) for train_set_table in train_sets_table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our old time series split (with overlap : required 1 neural network trained per split)\n",
    "# But in this script it's not needed because we're training 1 unique network, with a different fold strategy (non overlaped)\n",
    "#cv = PurgedGroupTimeSeriesSplit(\n",
    "#    n_splits=5,\n",
    "#    max_train_group_size=180,\n",
    "#    group_gap=20,\n",
    "#    max_test_group_size=60\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index, test_index = next(cv.split(df, (df['resp'] > 0)*1, df['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.loc[train_index, 'resp'] > 0).astype(np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(f_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Sum of model parameters:')\n",
    "#[print(p.sum()) for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter()\n",
    "\n",
    "#writer.add_text('test', 'test:'  + str(model).replace('\\n', '<BR>'))\n",
    "\n",
    "#writer.flush()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list = []\n",
    "\n",
    "for fold, (train1_index, train2_index, test_index) in enumerate(cv_tuples_generator):\n",
    "    folds_list.append((train1_index, train2_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_train1 = [folds_list[i][0] for i in range(5)]\n",
    "folds_list_train1_flat = [folds_list_train1_item for sublist in folds_list_train1 for folds_list_train1_item in sublist]\n",
    "folds_list_train1_unique = list(set(folds_list_train1_flat))\n",
    "\n",
    "folds_list_train2 = [folds_list[i][1] for i in range(5)]\n",
    "folds_list_train2_flat = [folds_list_train2_item for sublist in folds_list_train2 for folds_list_train2_item in sublist]\n",
    "folds_list_train2_unique = list(set(folds_list_train2_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train2_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train1_item) for folds_list_train1_item in folds_list_train1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train2_item) for folds_list_train2_item in folds_list_train2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_test = [folds_list[i][2] for i in range(5)]\n",
    "folds_list_test_flat = [folds_list_test_item for sublist in folds_list_test for folds_list_test_item in sublist]\n",
    "folds_list_test_unique = set(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_test_item) for folds_list_test_item in folds_list_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat) + len(folds_list_train2_flat) + len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141980, 130)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[folds_list_test[4], FEATURES_LIST_TOTRAIN].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00880718,  0.39574469,  0.33059838,  0.00919269,  0.00341737,\n",
       "       -0.00498373, -0.01455459,  0.05534631,  0.02511896,  0.2646538 ,\n",
       "        0.16705702,  0.09489698,  0.04450428,  0.15251293,  0.07996651,\n",
       "        0.22166532,  0.12827658,  0.12181565,  0.10958852,  0.29772963,\n",
       "        0.26463247,  0.1881408 ,  0.17251055,  0.25474009,  0.23267903,\n",
       "        0.29794049,  0.2685417 ,  0.13985131,  0.16285107,  0.33060734,\n",
       "        0.34385913,  0.22684687,  0.25190658,  0.31637359,  0.3359838 ,\n",
       "        0.35284181,  0.36773315,  0.02650339,  0.0186391 ,  0.04320553,\n",
       "        0.05298663,  0.45417433,  0.37762691,  0.41617323,  0.43927675,\n",
       "        0.48651095,  0.49207956,  0.36839975,  0.50144387,  0.54379067,\n",
       "        0.53074971,  0.45673965,  0.05646874,  0.38900233,  0.37690587,\n",
       "        0.77549302,  0.92466193,  0.78590429,  0.80847667,  0.89895923,\n",
       "        0.55335406,  0.55554392,  0.55922873,  0.56139559,  0.44231975,\n",
       "        0.61884351,  0.61715568,  0.59770334,  0.59814018,  0.37738388,\n",
       "        0.23893403,  0.30802914,  0.00410365, -0.03220141, -0.00163732,\n",
       "       -0.01991575, -0.03158872, -0.0931838 , -0.00806526, -0.03578937,\n",
       "       -0.00251814, -0.01489434, -0.03498338, -0.10154564,  0.39337805,\n",
       "        0.54162178,  0.39241949,  0.42814332,  0.49755557,  0.39935045,\n",
       "        0.43319566,  0.52353302,  0.42238168,  0.42206715,  0.43484953,\n",
       "        0.4547188 ,  0.39837193,  0.5421566 ,  0.39730999,  0.42589424,\n",
       "        0.48653787,  0.41054099,  0.43399339,  0.48391166,  0.41683186,\n",
       "        0.41979739,  0.46144612,  0.455339  ,  0.39499643,  0.38242161,\n",
       "        0.39000896,  0.391784  ,  0.38065447,  0.40349802,  0.43203717,\n",
       "        0.37668634,  0.42790068,  0.42074866,  0.39520648,  0.44651147,\n",
       "        0.36161378,  0.29991827,  0.37089359,  0.30351037,  0.36007002,\n",
       "        0.27782367,  0.3742626 ,  0.25712366,  0.37390404,  0.268233  ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(folds_list_train1_unique + folds_list_train2_unique), FEATURES_LIST_TOTRAIN].to_numpy().mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677155"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(folds_list_train1_unique + folds_list_train2_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0088,  0.3957,  0.3306,  0.0092,  0.0034, -0.0050, -0.0146,  0.0553,\n",
       "         0.0251,  0.2647,  0.1671,  0.0949,  0.0445,  0.1525,  0.0800,  0.2217,\n",
       "         0.1283,  0.1218,  0.1096,  0.2977,  0.2646,  0.1881,  0.1725,  0.2547,\n",
       "         0.2327,  0.2979,  0.2685,  0.1399,  0.1629,  0.3306,  0.3439,  0.2268,\n",
       "         0.2519,  0.3164,  0.3360,  0.3528,  0.3677,  0.0265,  0.0186,  0.0432,\n",
       "         0.0530,  0.4542,  0.3776,  0.4162,  0.4393,  0.4865,  0.4921,  0.3684,\n",
       "         0.5014,  0.5438,  0.5307,  0.4567,  0.0565,  0.3890,  0.3769,  0.7755,\n",
       "         0.9247,  0.7859,  0.8085,  0.8990,  0.5534,  0.5555,  0.5592,  0.5614,\n",
       "         0.4423,  0.6188,  0.6172,  0.5977,  0.5981,  0.3774,  0.2389,  0.3080,\n",
       "         0.0041, -0.0322, -0.0016, -0.0199, -0.0316, -0.0932, -0.0081, -0.0358,\n",
       "        -0.0025, -0.0149, -0.0350, -0.1015,  0.3934,  0.5416,  0.3924,  0.4281,\n",
       "         0.4976,  0.3994,  0.4332,  0.5235,  0.4224,  0.4221,  0.4348,  0.4547,\n",
       "         0.3984,  0.5422,  0.3973,  0.4259,  0.4865,  0.4105,  0.4340,  0.4839,\n",
       "         0.4168,  0.4198,  0.4614,  0.4553,  0.3950,  0.3824,  0.3900,  0.3918,\n",
       "         0.3807,  0.4035,  0.4320,  0.3767,  0.4279,  0.4207,  0.3952,  0.4465,\n",
       "         0.3616,  0.2999,  0.3709,  0.3035,  0.3601,  0.2778,  0.3743,  0.2571,\n",
       "         0.3739,  0.2682], dtype=torch.float64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor(df.loc[(folds_list_train1_unique + folds_list_train2_unique), FEATURES_LIST_TOTRAIN].to_numpy(), device='cpu'), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.8386e-03,  3.8558e-01,  3.5769e-01,  8.9192e-03,  4.1501e-03,\n",
       "        -3.7146e-03, -1.2589e-02,  5.1777e-02,  2.6828e-02,  2.4881e-01,\n",
       "         1.8235e-01,  8.9122e-02,  4.9486e-02,  1.4311e-01,  8.9027e-02,\n",
       "         2.1168e-01,  1.4630e-01,  1.2122e-01,  1.1358e-01,  2.9381e-01,\n",
       "         2.6877e-01,  1.8691e-01,  1.7698e-01,  2.5244e-01,  2.3856e-01,\n",
       "         2.9407e-01,  2.7318e-01,  1.3548e-01,  1.6088e-01,  3.2189e-01,\n",
       "         3.4253e-01,  2.2056e-01,  2.5013e-01,  3.0822e-01,  3.3535e-01,\n",
       "         3.4145e-01,  3.6583e-01,  2.9320e-02,  2.2892e-02,  4.0022e-02,\n",
       "         5.0750e-02,  4.4505e-01,  3.6018e-01,  3.4603e-01,  4.1153e-01,\n",
       "         4.3803e-01,  4.7612e-01,  3.4787e-01,  4.9963e-01,  5.6400e-01,\n",
       "         5.1226e-01,  4.5739e-01,  4.5744e-02,  3.6270e-01,  3.5887e-01,\n",
       "         6.5260e-01,  8.0495e-01,  6.6135e-01,  6.7981e-01,  7.6259e-01,\n",
       "         5.5640e-01,  5.5817e-01,  5.4554e-01,  5.4678e-01,  4.3506e-01,\n",
       "         6.0757e-01,  6.0850e-01,  5.9519e-01,  5.9594e-01,  3.6954e-01,\n",
       "         2.4337e-01,  3.3227e-01,  5.3933e-03, -3.2868e-02, -2.0445e-04,\n",
       "        -1.9092e-02, -3.1898e-02, -7.6800e-02, -6.0595e-03, -3.5435e-02,\n",
       "        -2.0995e-03, -1.4418e-02, -3.4615e-02, -8.0085e-02,  3.9822e-01,\n",
       "         5.5782e-01,  4.0240e-01,  4.4445e-01,  5.1409e-01,  4.0052e-01,\n",
       "         4.1025e-01,  5.2051e-01,  4.0508e-01,  4.0883e-01,  4.2889e-01,\n",
       "         4.1763e-01,  4.0227e-01,  5.5909e-01,  4.0711e-01,  4.3686e-01,\n",
       "         5.0012e-01,  4.0856e-01,  4.0506e-01,  4.8141e-01,  4.0166e-01,\n",
       "         4.0706e-01,  4.5305e-01,  4.1501e-01,  3.9999e-01,  4.1654e-01,\n",
       "         4.0074e-01,  4.0688e-01,  4.1228e-01,  4.0264e-01,  4.0711e-01,\n",
       "         3.7342e-01,  4.0443e-01,  4.0103e-01,  3.8582e-01,  4.1560e-01,\n",
       "         3.3513e-01,  2.6878e-01,  3.4355e-01,  2.8000e-01,  3.3515e-01,\n",
       "         2.4488e-01,  3.3918e-01,  2.3238e-01,  3.4256e-01,  2.4562e-01],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(f_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Epoch(0) - Training Loss: 8.4486\n",
      "Epoch(0) - Fold 0 - Validation Loss : 7.4532\n",
      "Epoch(0) - Fold 1 - Validation Loss : 5.9195\n",
      "Epoch(0) - Fold 2 - Validation Loss : 5.1400\n",
      "Epoch(0) - Fold 3 - Validation Loss : 5.0390\n",
      "Epoch(0) - Fold 4 - Validation Loss : 4.6468\n",
      "Epoch(0) - GLOBAL - Validation Loss: 5.6397\n",
      "Saving model corresponding to last_loss == 5.639703469680469\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 8.1090\n",
      "Epoch(1) - Fold 0 - Validation Loss : 6.8808\n",
      "Epoch(1) - Fold 1 - Validation Loss : 5.5018\n",
      "Epoch(1) - Fold 2 - Validation Loss : 4.7283\n",
      "Epoch(1) - Fold 3 - Validation Loss : 4.6391\n",
      "Epoch(1) - Fold 4 - Validation Loss : 4.3017\n",
      "Epoch(1) - GLOBAL - Validation Loss: 5.2103\n",
      "Saving model corresponding to last_loss == 5.210343085586485\n",
      "\n",
      "\n",
      "Epoch(2) - Training Loss: 7.3955\n",
      "Epoch(2) - Fold 0 - Validation Loss : 6.1263\n",
      "Epoch(2) - Fold 1 - Validation Loss : 4.9431\n",
      "Epoch(2) - Fold 2 - Validation Loss : 4.2284\n",
      "Epoch(2) - Fold 3 - Validation Loss : 4.1663\n",
      "Epoch(2) - Fold 4 - Validation Loss : 3.8621\n",
      "Epoch(2) - GLOBAL - Validation Loss: 4.6653\n",
      "Saving model corresponding to last_loss == 4.665255414571185\n",
      "\n",
      "\n",
      "Epoch(3) - Training Loss: 6.3322\n",
      "Epoch(3) - Fold 0 - Validation Loss : 5.4120\n",
      "Epoch(3) - Fold 1 - Validation Loss : 4.2889\n",
      "Epoch(3) - Fold 2 - Validation Loss : 3.8926\n",
      "Epoch(3) - Fold 3 - Validation Loss : 3.8083\n",
      "Epoch(3) - Fold 4 - Validation Loss : 3.4632\n",
      "Epoch(3) - GLOBAL - Validation Loss: 4.1730\n",
      "Saving model corresponding to last_loss == 4.172988464747431\n",
      "\n",
      "\n",
      "Epoch(4) - Training Loss: 5.0077\n",
      "Epoch(4) - Fold 0 - Validation Loss : 4.7809\n",
      "Epoch(4) - Fold 1 - Validation Loss : 3.6487\n",
      "Epoch(4) - Fold 2 - Validation Loss : 3.6623\n",
      "Epoch(4) - Fold 3 - Validation Loss : 3.5649\n",
      "Epoch(4) - Fold 4 - Validation Loss : 3.1397\n",
      "Epoch(4) - GLOBAL - Validation Loss: 3.7593\n",
      "Saving model corresponding to last_loss == 3.759316822271482\n",
      "\n",
      "\n",
      "Epoch(5) - Training Loss: 3.9866\n",
      "Epoch(5) - Fold 0 - Validation Loss : 4.1381\n",
      "Epoch(5) - Fold 1 - Validation Loss : 3.1874\n",
      "Epoch(5) - Fold 2 - Validation Loss : 3.1850\n",
      "Epoch(5) - Fold 3 - Validation Loss : 3.1209\n",
      "Epoch(5) - Fold 4 - Validation Loss : 2.7231\n",
      "Epoch(5) - GLOBAL - Validation Loss: 3.2709\n",
      "Saving model corresponding to last_loss == 3.2708991366231146\n",
      "\n",
      "\n",
      "Epoch(6) - Training Loss: 3.5440\n",
      "Epoch(6) - Fold 0 - Validation Loss : 3.6806\n",
      "Epoch(6) - Fold 1 - Validation Loss : 2.8611\n",
      "Epoch(6) - Fold 2 - Validation Loss : 2.8202\n",
      "Epoch(6) - Fold 3 - Validation Loss : 2.7838\n",
      "Epoch(6) - Fold 4 - Validation Loss : 2.4258\n",
      "Epoch(6) - GLOBAL - Validation Loss: 2.9143\n",
      "Saving model corresponding to last_loss == 2.9143287914827747\n",
      "\n",
      "\n",
      "Epoch(7) - Training Loss: 3.2077\n",
      "Epoch(7) - Fold 0 - Validation Loss : 3.2968\n",
      "Epoch(7) - Fold 1 - Validation Loss : 2.5999\n",
      "Epoch(7) - Fold 2 - Validation Loss : 2.5394\n",
      "Epoch(7) - Fold 3 - Validation Loss : 2.5293\n",
      "Epoch(7) - Fold 4 - Validation Loss : 2.1924\n",
      "Epoch(7) - GLOBAL - Validation Loss: 2.6316\n",
      "Saving model corresponding to last_loss == 2.6315618119184525\n",
      "\n",
      "\n",
      "Epoch(8) - Training Loss: 2.9253\n",
      "Epoch(8) - Fold 0 - Validation Loss : 2.9428\n",
      "Epoch(8) - Fold 1 - Validation Loss : 2.3621\n",
      "Epoch(8) - Fold 2 - Validation Loss : 2.2886\n",
      "Epoch(8) - Fold 3 - Validation Loss : 2.2918\n",
      "Epoch(8) - Fold 4 - Validation Loss : 1.9815\n",
      "Epoch(8) - GLOBAL - Validation Loss: 2.3734\n",
      "Saving model corresponding to last_loss == 2.3733609336501376\n",
      "\n",
      "\n",
      "Epoch(9) - Training Loss: 2.6533\n",
      "Epoch(9) - Fold 0 - Validation Loss : 2.6173\n",
      "Epoch(9) - Fold 1 - Validation Loss : 2.1362\n",
      "Epoch(9) - Fold 2 - Validation Loss : 2.0764\n",
      "Epoch(9) - Fold 3 - Validation Loss : 2.0878\n",
      "Epoch(9) - Fold 4 - Validation Loss : 1.7947\n",
      "Epoch(9) - GLOBAL - Validation Loss: 2.1425\n",
      "Saving model corresponding to last_loss == 2.1424791700711308\n",
      "\n",
      "\n",
      "Epoch(10) - Training Loss: 2.4106\n",
      "Epoch(10) - Fold 0 - Validation Loss : 2.3497\n",
      "Epoch(10) - Fold 1 - Validation Loss : 1.9445\n",
      "Epoch(10) - Fold 2 - Validation Loss : 1.8985\n",
      "Epoch(10) - Fold 3 - Validation Loss : 1.9125\n",
      "Epoch(10) - Fold 4 - Validation Loss : 1.6387\n",
      "Epoch(10) - GLOBAL - Validation Loss: 1.9488\n",
      "Saving model corresponding to last_loss == 1.9488063180305797\n",
      "\n",
      "\n",
      "Epoch(11) - Training Loss: 2.1852\n",
      "Epoch(11) - Fold 0 - Validation Loss : 2.1381\n",
      "Epoch(11) - Fold 1 - Validation Loss : 1.7676\n",
      "Epoch(11) - Fold 2 - Validation Loss : 1.7593\n",
      "Epoch(11) - Fold 3 - Validation Loss : 1.7724\n",
      "Epoch(11) - Fold 4 - Validation Loss : 1.5084\n",
      "Epoch(11) - GLOBAL - Validation Loss: 1.7892\n",
      "Saving model corresponding to last_loss == 1.7891860574162155\n",
      "\n",
      "\n",
      "Epoch(12) - Training Loss: 1.9735\n",
      "Epoch(12) - Fold 0 - Validation Loss : 1.9625\n",
      "Epoch(12) - Fold 1 - Validation Loss : 1.6177\n",
      "Epoch(12) - Fold 2 - Validation Loss : 1.6484\n",
      "Epoch(12) - Fold 3 - Validation Loss : 1.6638\n",
      "Epoch(12) - Fold 4 - Validation Loss : 1.4023\n",
      "Epoch(12) - GLOBAL - Validation Loss: 1.6589\n",
      "Saving model corresponding to last_loss == 1.6589464933629123\n",
      "\n",
      "\n",
      "Epoch(13) - Training Loss: 1.7888\n",
      "Epoch(13) - Fold 0 - Validation Loss : 1.8315\n",
      "Epoch(13) - Fold 1 - Validation Loss : 1.4974\n",
      "Epoch(13) - Fold 2 - Validation Loss : 1.5611\n",
      "Epoch(13) - Fold 3 - Validation Loss : 1.5753\n",
      "Epoch(13) - Fold 4 - Validation Loss : 1.3186\n",
      "Epoch(13) - GLOBAL - Validation Loss: 1.5568\n",
      "Saving model corresponding to last_loss == 1.5567860455012348\n",
      "\n",
      "\n",
      "Epoch(14) - Training Loss: 1.6526\n",
      "Epoch(14) - Fold 0 - Validation Loss : 1.7266\n",
      "Epoch(14) - Fold 1 - Validation Loss : 1.4129\n",
      "Epoch(14) - Fold 2 - Validation Loss : 1.4872\n",
      "Epoch(14) - Fold 3 - Validation Loss : 1.4988\n",
      "Epoch(14) - Fold 4 - Validation Loss : 1.2514\n",
      "Epoch(14) - GLOBAL - Validation Loss: 1.4754\n",
      "Saving model corresponding to last_loss == 1.475370597339449\n",
      "\n",
      "\n",
      "Epoch(15) - Training Loss: 1.5568\n",
      "Epoch(15) - Fold 0 - Validation Loss : 1.6444\n",
      "Epoch(15) - Fold 1 - Validation Loss : 1.3390\n",
      "Epoch(15) - Fold 2 - Validation Loss : 1.4194\n",
      "Epoch(15) - Fold 3 - Validation Loss : 1.4281\n",
      "Epoch(15) - Fold 4 - Validation Loss : 1.1908\n",
      "Epoch(15) - GLOBAL - Validation Loss: 1.4043\n",
      "Saving model corresponding to last_loss == 1.4043391610486269\n",
      "\n",
      "\n",
      "Epoch(16) - Training Loss: 1.4786\n",
      "Epoch(16) - Fold 0 - Validation Loss : 1.5691\n",
      "Epoch(16) - Fold 1 - Validation Loss : 1.2856\n",
      "Epoch(16) - Fold 2 - Validation Loss : 1.3633\n",
      "Epoch(16) - Fold 3 - Validation Loss : 1.3715\n",
      "Epoch(16) - Fold 4 - Validation Loss : 1.1426\n",
      "Epoch(16) - GLOBAL - Validation Loss: 1.3464\n",
      "Saving model corresponding to last_loss == 1.346426609503412\n",
      "\n",
      "\n",
      "Epoch(17) - Training Loss: 1.4146\n",
      "Epoch(17) - Fold 0 - Validation Loss : 1.5109\n",
      "Epoch(17) - Fold 1 - Validation Loss : 1.2362\n",
      "Epoch(17) - Fold 2 - Validation Loss : 1.3163\n",
      "Epoch(17) - Fold 3 - Validation Loss : 1.3224\n",
      "Epoch(17) - Fold 4 - Validation Loss : 1.1028\n",
      "Epoch(17) - GLOBAL - Validation Loss: 1.2977\n",
      "Saving model corresponding to last_loss == 1.297716746586828\n",
      "\n",
      "\n",
      "Epoch(18) - Training Loss: 1.3620\n",
      "Epoch(18) - Fold 0 - Validation Loss : 1.4751\n",
      "Epoch(18) - Fold 1 - Validation Loss : 1.1944\n",
      "Epoch(18) - Fold 2 - Validation Loss : 1.2810\n",
      "Epoch(18) - Fold 3 - Validation Loss : 1.2854\n",
      "Epoch(18) - Fold 4 - Validation Loss : 1.0697\n",
      "Epoch(18) - GLOBAL - Validation Loss: 1.2611\n",
      "Saving model corresponding to last_loss == 1.2611276049603526\n",
      "\n",
      "\n",
      "Epoch(19) - Training Loss: 1.3099\n",
      "Epoch(19) - Fold 0 - Validation Loss : 1.4191\n",
      "Epoch(19) - Fold 1 - Validation Loss : 1.1584\n",
      "Epoch(19) - Fold 2 - Validation Loss : 1.2419\n",
      "Epoch(19) - Fold 3 - Validation Loss : 1.2478\n",
      "Epoch(19) - Fold 4 - Validation Loss : 1.0381\n",
      "Epoch(19) - GLOBAL - Validation Loss: 1.2211\n",
      "Saving model corresponding to last_loss == 1.2210642327346537\n",
      "\n",
      "\n",
      "Epoch(20) - Training Loss: 1.2624\n",
      "Epoch(20) - Fold 0 - Validation Loss : 1.3801\n",
      "Epoch(20) - Fold 1 - Validation Loss : 1.1242\n",
      "Epoch(20) - Fold 2 - Validation Loss : 1.2117\n",
      "Epoch(20) - Fold 3 - Validation Loss : 1.2167\n",
      "Epoch(20) - Fold 4 - Validation Loss : 1.0125\n",
      "Epoch(20) - GLOBAL - Validation Loss: 1.1890\n",
      "Saving model corresponding to last_loss == 1.189040083811103\n",
      "\n",
      "\n",
      "Epoch(21) - Training Loss: 1.2189\n",
      "Epoch(21) - Fold 0 - Validation Loss : 1.3402\n",
      "Epoch(21) - Fold 1 - Validation Loss : 1.0925\n",
      "Epoch(21) - Fold 2 - Validation Loss : 1.1817\n",
      "Epoch(21) - Fold 3 - Validation Loss : 1.1881\n",
      "Epoch(21) - Fold 4 - Validation Loss : 0.9877\n",
      "Epoch(21) - GLOBAL - Validation Loss: 1.1580\n",
      "Saving model corresponding to last_loss == 1.1580470393669469\n",
      "\n",
      "\n",
      "Epoch(22) - Training Loss: 1.1788\n",
      "Epoch(22) - Fold 0 - Validation Loss : 1.3149\n",
      "Epoch(22) - Fold 1 - Validation Loss : 1.0671\n",
      "Epoch(22) - Fold 2 - Validation Loss : 1.1573\n",
      "Epoch(22) - Fold 3 - Validation Loss : 1.1660\n",
      "Epoch(22) - Fold 4 - Validation Loss : 0.9665\n",
      "Epoch(22) - GLOBAL - Validation Loss: 1.1344\n",
      "Saving model corresponding to last_loss == 1.13435355562573\n",
      "\n",
      "\n",
      "Epoch(23) - Training Loss: 1.1474\n",
      "Epoch(23) - Fold 0 - Validation Loss : 1.2803\n",
      "Epoch(23) - Fold 1 - Validation Loss : 1.0391\n",
      "Epoch(23) - Fold 2 - Validation Loss : 1.1305\n",
      "Epoch(23) - Fold 3 - Validation Loss : 1.1376\n",
      "Epoch(23) - Fold 4 - Validation Loss : 0.9442\n",
      "Epoch(23) - GLOBAL - Validation Loss: 1.1064\n",
      "Saving model corresponding to last_loss == 1.1063596444728108\n",
      "\n",
      "\n",
      "Epoch(24) - Training Loss: 1.1127\n",
      "Epoch(24) - Fold 0 - Validation Loss : 1.2518\n",
      "Epoch(24) - Fold 1 - Validation Loss : 1.0145\n",
      "Epoch(24) - Fold 2 - Validation Loss : 1.1052\n",
      "Epoch(24) - Fold 3 - Validation Loss : 1.1135\n",
      "Epoch(24) - Fold 4 - Validation Loss : 0.9228\n",
      "Epoch(24) - GLOBAL - Validation Loss: 1.0815\n",
      "Saving model corresponding to last_loss == 1.0815409633947835\n",
      "\n",
      "\n",
      "Epoch(25) - Training Loss: 1.0834\n",
      "Epoch(25) - Fold 0 - Validation Loss : 1.2225\n",
      "Epoch(25) - Fold 1 - Validation Loss : 0.9908\n",
      "Epoch(25) - Fold 2 - Validation Loss : 1.0798\n",
      "Epoch(25) - Fold 3 - Validation Loss : 1.0898\n",
      "Epoch(25) - Fold 4 - Validation Loss : 0.9026\n",
      "Epoch(25) - GLOBAL - Validation Loss: 1.0571\n",
      "Saving model corresponding to last_loss == 1.057087027442416\n",
      "\n",
      "\n",
      "Epoch(26) - Training Loss: 1.0577\n",
      "Epoch(26) - Fold 0 - Validation Loss : 1.1972\n",
      "Epoch(26) - Fold 1 - Validation Loss : 0.9705\n",
      "Epoch(26) - Fold 2 - Validation Loss : 1.0577\n",
      "Epoch(26) - Fold 3 - Validation Loss : 1.0679\n",
      "Epoch(26) - Fold 4 - Validation Loss : 0.8849\n",
      "Epoch(26) - GLOBAL - Validation Loss: 1.0356\n",
      "Saving model corresponding to last_loss == 1.0356351794561687\n",
      "\n",
      "\n",
      "Epoch(27) - Training Loss: 1.0352\n",
      "Epoch(27) - Fold 0 - Validation Loss : 1.1704\n",
      "Epoch(27) - Fold 1 - Validation Loss : 0.9491\n",
      "Epoch(27) - Fold 2 - Validation Loss : 1.0337\n",
      "Epoch(27) - Fold 3 - Validation Loss : 1.0450\n",
      "Epoch(27) - Fold 4 - Validation Loss : 0.8650\n",
      "Epoch(27) - GLOBAL - Validation Loss: 1.0126\n",
      "Saving model corresponding to last_loss == 1.0126452382544706\n",
      "\n",
      "\n",
      "Epoch(28) - Training Loss: 1.0137\n",
      "Epoch(28) - Fold 0 - Validation Loss : 1.1508\n",
      "Epoch(28) - Fold 1 - Validation Loss : 0.9319\n",
      "Epoch(28) - Fold 2 - Validation Loss : 1.0134\n",
      "Epoch(28) - Fold 3 - Validation Loss : 1.0257\n",
      "Epoch(28) - Fold 4 - Validation Loss : 0.8496\n",
      "Epoch(28) - GLOBAL - Validation Loss: 0.9943\n",
      "Saving model corresponding to last_loss == 0.9942761631412571\n",
      "\n",
      "\n",
      "Epoch(29) - Training Loss: 0.9937\n",
      "Epoch(29) - Fold 0 - Validation Loss : 1.1272\n",
      "Epoch(29) - Fold 1 - Validation Loss : 0.9154\n",
      "Epoch(29) - Fold 2 - Validation Loss : 0.9942\n",
      "Epoch(29) - Fold 3 - Validation Loss : 1.0076\n",
      "Epoch(29) - Fold 4 - Validation Loss : 0.8330\n",
      "Epoch(29) - GLOBAL - Validation Loss: 0.9755\n",
      "Saving model corresponding to last_loss == 0.9754879988222734\n",
      "\n",
      "\n",
      "Epoch(30) - Training Loss: 0.9776\n",
      "Epoch(30) - Fold 0 - Validation Loss : 1.1087\n",
      "Epoch(30) - Fold 1 - Validation Loss : 0.9008\n",
      "Epoch(30) - Fold 2 - Validation Loss : 0.9778\n",
      "Epoch(30) - Fold 3 - Validation Loss : 0.9901\n",
      "Epoch(30) - Fold 4 - Validation Loss : 0.8184\n",
      "Epoch(30) - GLOBAL - Validation Loss: 0.9592\n",
      "Saving model corresponding to last_loss == 0.9591728678132307\n",
      "\n",
      "\n",
      "Epoch(31) - Training Loss: 0.9590\n",
      "Epoch(31) - Fold 0 - Validation Loss : 1.0895\n",
      "Epoch(31) - Fold 1 - Validation Loss : 0.8863\n",
      "Epoch(31) - Fold 2 - Validation Loss : 0.9620\n",
      "Epoch(31) - Fold 3 - Validation Loss : 0.9746\n",
      "Epoch(31) - Fold 4 - Validation Loss : 0.8064\n",
      "Epoch(31) - GLOBAL - Validation Loss: 0.9438\n",
      "Saving model corresponding to last_loss == 0.9437682590792367\n",
      "\n",
      "\n",
      "Epoch(32) - Training Loss: 0.9430\n",
      "Epoch(32) - Fold 0 - Validation Loss : 1.0684\n",
      "Epoch(32) - Fold 1 - Validation Loss : 0.8715\n",
      "Epoch(32) - Fold 2 - Validation Loss : 0.9451\n",
      "Epoch(32) - Fold 3 - Validation Loss : 0.9573\n",
      "Epoch(32) - Fold 4 - Validation Loss : 0.7922\n",
      "Epoch(32) - GLOBAL - Validation Loss: 0.9269\n",
      "Saving model corresponding to last_loss == 0.9268870246695814\n",
      "\n",
      "\n",
      "Epoch(33) - Training Loss: 0.9282\n",
      "Epoch(33) - Fold 0 - Validation Loss : 1.0532\n",
      "Epoch(33) - Fold 1 - Validation Loss : 0.8584\n",
      "Epoch(33) - Fold 2 - Validation Loss : 0.9296\n",
      "Epoch(33) - Fold 3 - Validation Loss : 0.9431\n",
      "Epoch(33) - Fold 4 - Validation Loss : 0.7800\n",
      "Epoch(33) - GLOBAL - Validation Loss: 0.9129\n",
      "Saving model corresponding to last_loss == 0.9128522234179725\n",
      "\n",
      "\n",
      "Epoch(34) - Training Loss: 0.9146\n",
      "Epoch(34) - Fold 0 - Validation Loss : 1.0372\n",
      "Epoch(34) - Fold 1 - Validation Loss : 0.8457\n",
      "Epoch(34) - Fold 2 - Validation Loss : 0.9149\n",
      "Epoch(34) - Fold 3 - Validation Loss : 0.9292\n",
      "Epoch(34) - Fold 4 - Validation Loss : 0.7683\n",
      "Epoch(34) - GLOBAL - Validation Loss: 0.8990\n",
      "Saving model corresponding to last_loss == 0.899043308878262\n",
      "\n",
      "\n",
      "Epoch(35) - Training Loss: 0.9012\n",
      "Epoch(35) - Fold 0 - Validation Loss : 1.0230\n",
      "Epoch(35) - Fold 1 - Validation Loss : 0.8349\n",
      "Epoch(35) - Fold 2 - Validation Loss : 0.9044\n",
      "Epoch(35) - Fold 3 - Validation Loss : 0.9162\n",
      "Epoch(35) - Fold 4 - Validation Loss : 0.7581\n",
      "Epoch(35) - GLOBAL - Validation Loss: 0.8873\n",
      "Saving model corresponding to last_loss == 0.8873235029570395\n",
      "\n",
      "\n",
      "Epoch(36) - Training Loss: 0.8892\n",
      "Epoch(36) - Fold 0 - Validation Loss : 1.0080\n",
      "Epoch(36) - Fold 1 - Validation Loss : 0.8225\n",
      "Epoch(36) - Fold 2 - Validation Loss : 0.8893\n",
      "Epoch(36) - Fold 3 - Validation Loss : 0.9025\n",
      "Epoch(36) - Fold 4 - Validation Loss : 0.7462\n",
      "Epoch(36) - GLOBAL - Validation Loss: 0.8737\n",
      "Saving model corresponding to last_loss == 0.8736915347800773\n",
      "\n",
      "\n",
      "Epoch(37) - Training Loss: 0.8792\n",
      "Epoch(37) - Fold 0 - Validation Loss : 0.9940\n",
      "Epoch(37) - Fold 1 - Validation Loss : 0.8127\n",
      "Epoch(37) - Fold 2 - Validation Loss : 0.8782\n",
      "Epoch(37) - Fold 3 - Validation Loss : 0.8914\n",
      "Epoch(37) - Fold 4 - Validation Loss : 0.7355\n",
      "Epoch(37) - GLOBAL - Validation Loss: 0.8624\n",
      "Saving model corresponding to last_loss == 0.8623584735061393\n",
      "\n",
      "\n",
      "Epoch(38) - Training Loss: 0.8669\n",
      "Epoch(38) - Fold 0 - Validation Loss : 0.9849\n",
      "Epoch(38) - Fold 1 - Validation Loss : 0.8044\n",
      "Epoch(38) - Fold 2 - Validation Loss : 0.8695\n",
      "Epoch(38) - Fold 3 - Validation Loss : 0.8811\n",
      "Epoch(38) - Fold 4 - Validation Loss : 0.7275\n",
      "Epoch(38) - GLOBAL - Validation Loss: 0.8535\n",
      "Saving model corresponding to last_loss == 0.8534979962698186\n",
      "\n",
      "\n",
      "Epoch(39) - Training Loss: 0.8556\n",
      "Epoch(39) - Fold 0 - Validation Loss : 0.9687\n",
      "Epoch(39) - Fold 1 - Validation Loss : 0.7930\n",
      "Epoch(39) - Fold 2 - Validation Loss : 0.8578\n",
      "Epoch(39) - Fold 3 - Validation Loss : 0.8687\n",
      "Epoch(39) - Fold 4 - Validation Loss : 0.7179\n",
      "Epoch(39) - GLOBAL - Validation Loss: 0.8412\n",
      "Saving model corresponding to last_loss == 0.8412230318461063\n",
      "\n",
      "\n",
      "Epoch(40) - Training Loss: 0.8442\n",
      "Epoch(40) - Fold 0 - Validation Loss : 0.9557\n",
      "Epoch(40) - Fold 1 - Validation Loss : 0.7837\n",
      "Epoch(40) - Fold 2 - Validation Loss : 0.8474\n",
      "Epoch(40) - Fold 3 - Validation Loss : 0.8576\n",
      "Epoch(40) - Fold 4 - Validation Loss : 0.7069\n",
      "Epoch(40) - GLOBAL - Validation Loss: 0.8303\n",
      "Saving model corresponding to last_loss == 0.8302628840542248\n",
      "\n",
      "\n",
      "Epoch(41) - Training Loss: 0.8341\n",
      "Epoch(41) - Fold 0 - Validation Loss : 0.9437\n",
      "Epoch(41) - Fold 1 - Validation Loss : 0.7737\n",
      "Epoch(41) - Fold 2 - Validation Loss : 0.8358\n",
      "Epoch(41) - Fold 3 - Validation Loss : 0.8465\n",
      "Epoch(41) - Fold 4 - Validation Loss : 0.6992\n",
      "Epoch(41) - GLOBAL - Validation Loss: 0.8198\n",
      "Saving model corresponding to last_loss == 0.8197677093565867\n",
      "\n",
      "\n",
      "Epoch(42) - Training Loss: 0.8244\n",
      "Epoch(42) - Fold 0 - Validation Loss : 0.9283\n",
      "Epoch(42) - Fold 1 - Validation Loss : 0.7633\n",
      "Epoch(42) - Fold 2 - Validation Loss : 0.8256\n",
      "Epoch(42) - Fold 3 - Validation Loss : 0.8345\n",
      "Epoch(42) - Fold 4 - Validation Loss : 0.6886\n",
      "Epoch(42) - GLOBAL - Validation Loss: 0.8081\n",
      "Saving model corresponding to last_loss == 0.8080665660965127\n",
      "\n",
      "\n",
      "Epoch(43) - Training Loss: 0.8129\n",
      "Epoch(43) - Fold 0 - Validation Loss : 0.9175\n",
      "Epoch(43) - Fold 1 - Validation Loss : 0.7547\n",
      "Epoch(43) - Fold 2 - Validation Loss : 0.8167\n",
      "Epoch(43) - Fold 3 - Validation Loss : 0.8249\n",
      "Epoch(43) - Fold 4 - Validation Loss : 0.6794\n",
      "Epoch(43) - GLOBAL - Validation Loss: 0.7986\n",
      "Saving model corresponding to last_loss == 0.7986482327988293\n",
      "\n",
      "\n",
      "Epoch(44) - Training Loss: 0.8024\n",
      "Epoch(44) - Fold 0 - Validation Loss : 0.9047\n",
      "Epoch(44) - Fold 1 - Validation Loss : 0.7440\n",
      "Epoch(44) - Fold 2 - Validation Loss : 0.8043\n",
      "Epoch(44) - Fold 3 - Validation Loss : 0.8139\n",
      "Epoch(44) - Fold 4 - Validation Loss : 0.6693\n",
      "Epoch(44) - GLOBAL - Validation Loss: 0.7872\n",
      "Saving model corresponding to last_loss == 0.7872323973167956\n",
      "\n",
      "\n",
      "Epoch(45) - Training Loss: 0.7920\n",
      "Epoch(45) - Fold 0 - Validation Loss : 0.8938\n",
      "Epoch(45) - Fold 1 - Validation Loss : 0.7353\n",
      "Epoch(45) - Fold 2 - Validation Loss : 0.7939\n",
      "Epoch(45) - Fold 3 - Validation Loss : 0.8041\n",
      "Epoch(45) - Fold 4 - Validation Loss : 0.6615\n",
      "Epoch(45) - GLOBAL - Validation Loss: 0.7777\n",
      "Saving model corresponding to last_loss == 0.7776973356192335\n",
      "\n",
      "\n",
      "Epoch(46) - Training Loss: 0.7828\n",
      "Epoch(46) - Fold 0 - Validation Loss : 0.8810\n",
      "Epoch(46) - Fold 1 - Validation Loss : 0.7249\n",
      "Epoch(46) - Fold 2 - Validation Loss : 0.7831\n",
      "Epoch(46) - Fold 3 - Validation Loss : 0.7926\n",
      "Epoch(46) - Fold 4 - Validation Loss : 0.6512\n",
      "Epoch(46) - GLOBAL - Validation Loss: 0.7666\n",
      "Saving model corresponding to last_loss == 0.7665616267542055\n",
      "\n",
      "\n",
      "Epoch(47) - Training Loss: 0.7729\n",
      "Epoch(47) - Fold 0 - Validation Loss : 0.8688\n",
      "Epoch(47) - Fold 1 - Validation Loss : 0.7150\n",
      "Epoch(47) - Fold 2 - Validation Loss : 0.7713\n",
      "Epoch(47) - Fold 3 - Validation Loss : 0.7819\n",
      "Epoch(47) - Fold 4 - Validation Loss : 0.6403\n",
      "Epoch(47) - GLOBAL - Validation Loss: 0.7555\n",
      "Saving model corresponding to last_loss == 0.7554541100409193\n",
      "\n",
      "\n",
      "Epoch(48) - Training Loss: 0.7619\n",
      "Epoch(48) - Fold 0 - Validation Loss : 0.8592\n",
      "Epoch(48) - Fold 1 - Validation Loss : 0.7074\n",
      "Epoch(48) - Fold 2 - Validation Loss : 0.7628\n",
      "Epoch(48) - Fold 3 - Validation Loss : 0.7733\n",
      "Epoch(48) - Fold 4 - Validation Loss : 0.6324\n",
      "Epoch(48) - GLOBAL - Validation Loss: 0.7470\n",
      "Saving model corresponding to last_loss == 0.7470318740176519\n",
      "\n",
      "\n",
      "Epoch(49) - Training Loss: 0.7528\n",
      "Epoch(49) - Fold 0 - Validation Loss : 0.8479\n",
      "Epoch(49) - Fold 1 - Validation Loss : 0.6967\n",
      "Epoch(49) - Fold 2 - Validation Loss : 0.7509\n",
      "Epoch(49) - Fold 3 - Validation Loss : 0.7635\n",
      "Epoch(49) - Fold 4 - Validation Loss : 0.6238\n",
      "Epoch(49) - GLOBAL - Validation Loss: 0.7365\n",
      "Saving model corresponding to last_loss == 0.7365413395696022\n",
      "\n",
      "\n",
      "Epoch(50) - Training Loss: 0.7432\n",
      "Epoch(50) - Fold 0 - Validation Loss : 0.8386\n",
      "Epoch(50) - Fold 1 - Validation Loss : 0.6897\n",
      "Epoch(50) - Fold 2 - Validation Loss : 0.7436\n",
      "Epoch(50) - Fold 3 - Validation Loss : 0.7541\n",
      "Epoch(50) - Fold 4 - Validation Loss : 0.6165\n",
      "Epoch(50) - GLOBAL - Validation Loss: 0.7285\n",
      "Saving model corresponding to last_loss == 0.7285106833712183\n",
      "\n",
      "\n",
      "Epoch(51) - Training Loss: 0.7339\n",
      "Epoch(51) - Fold 0 - Validation Loss : 0.8268\n",
      "Epoch(51) - Fold 1 - Validation Loss : 0.6803\n",
      "Epoch(51) - Fold 2 - Validation Loss : 0.7320\n",
      "Epoch(51) - Fold 3 - Validation Loss : 0.7449\n",
      "Epoch(51) - Fold 4 - Validation Loss : 0.6063\n",
      "Epoch(51) - GLOBAL - Validation Loss: 0.7181\n",
      "Saving model corresponding to last_loss == 0.7180669436006907\n",
      "\n",
      "\n",
      "Epoch(52) - Training Loss: 0.7243\n",
      "Epoch(52) - Fold 0 - Validation Loss : 0.8162\n",
      "Epoch(52) - Fold 1 - Validation Loss : 0.6723\n",
      "Epoch(52) - Fold 2 - Validation Loss : 0.7231\n",
      "Epoch(52) - Fold 3 - Validation Loss : 0.7353\n",
      "Epoch(52) - Fold 4 - Validation Loss : 0.5980\n",
      "Epoch(52) - GLOBAL - Validation Loss: 0.7090\n",
      "Saving model corresponding to last_loss == 0.7089962513757798\n",
      "\n",
      "\n",
      "Epoch(53) - Training Loss: 0.7161\n",
      "Epoch(53) - Fold 0 - Validation Loss : 0.8065\n",
      "Epoch(53) - Fold 1 - Validation Loss : 0.6648\n",
      "Epoch(53) - Fold 2 - Validation Loss : 0.7150\n",
      "Epoch(53) - Fold 3 - Validation Loss : 0.7274\n",
      "Epoch(53) - Fold 4 - Validation Loss : 0.5914\n",
      "Epoch(53) - GLOBAL - Validation Loss: 0.7010\n",
      "Saving model corresponding to last_loss == 0.701029775661813\n",
      "\n",
      "\n",
      "Epoch(54) - Training Loss: 0.7077\n",
      "Epoch(54) - Fold 0 - Validation Loss : 0.7986\n",
      "Epoch(54) - Fold 1 - Validation Loss : 0.6578\n",
      "Epoch(54) - Fold 2 - Validation Loss : 0.7074\n",
      "Epoch(54) - Fold 3 - Validation Loss : 0.7198\n",
      "Epoch(54) - Fold 4 - Validation Loss : 0.5846\n",
      "Epoch(54) - GLOBAL - Validation Loss: 0.6936\n",
      "Saving model corresponding to last_loss == 0.6936386159189352\n",
      "\n",
      "\n",
      "Epoch(55) - Training Loss: 0.7000\n",
      "Epoch(55) - Fold 0 - Validation Loss : 0.7907\n",
      "Epoch(55) - Fold 1 - Validation Loss : 0.6510\n",
      "Epoch(55) - Fold 2 - Validation Loss : 0.6999\n",
      "Epoch(55) - Fold 3 - Validation Loss : 0.7126\n",
      "Epoch(55) - Fold 4 - Validation Loss : 0.5781\n",
      "Epoch(55) - GLOBAL - Validation Loss: 0.6865\n",
      "Saving model corresponding to last_loss == 0.6864563440998658\n",
      "\n",
      "\n",
      "Epoch(56) - Training Loss: 0.6925\n",
      "Epoch(56) - Fold 0 - Validation Loss : 0.7809\n",
      "Epoch(56) - Fold 1 - Validation Loss : 0.6437\n",
      "Epoch(56) - Fold 2 - Validation Loss : 0.6914\n",
      "Epoch(56) - Fold 3 - Validation Loss : 0.7039\n",
      "Epoch(56) - Fold 4 - Validation Loss : 0.5708\n",
      "Epoch(56) - GLOBAL - Validation Loss: 0.6781\n",
      "Saving model corresponding to last_loss == 0.6781299028316814\n",
      "\n",
      "\n",
      "Epoch(57) - Training Loss: 0.6862\n",
      "Epoch(57) - Fold 0 - Validation Loss : 0.7752\n",
      "Epoch(57) - Fold 1 - Validation Loss : 0.6385\n",
      "Epoch(57) - Fold 2 - Validation Loss : 0.6854\n",
      "Epoch(57) - Fold 3 - Validation Loss : 0.6976\n",
      "Epoch(57) - Fold 4 - Validation Loss : 0.5657\n",
      "Epoch(57) - GLOBAL - Validation Loss: 0.6725\n",
      "Saving model corresponding to last_loss == 0.6724650192532657\n",
      "\n",
      "\n",
      "Epoch(58) - Training Loss: 0.6778\n",
      "Epoch(58) - Fold 0 - Validation Loss : 0.7693\n",
      "Epoch(58) - Fold 1 - Validation Loss : 0.6332\n",
      "Epoch(58) - Fold 2 - Validation Loss : 0.6801\n",
      "Epoch(58) - Fold 3 - Validation Loss : 0.6923\n",
      "Epoch(58) - Fold 4 - Validation Loss : 0.5611\n",
      "Epoch(58) - GLOBAL - Validation Loss: 0.6672\n",
      "Saving model corresponding to last_loss == 0.667217590226095\n",
      "\n",
      "\n",
      "Epoch(59) - Training Loss: 0.6729\n",
      "Epoch(59) - Fold 0 - Validation Loss : 0.7586\n",
      "Epoch(59) - Fold 1 - Validation Loss : 0.6256\n",
      "Epoch(59) - Fold 2 - Validation Loss : 0.6716\n",
      "Epoch(59) - Fold 3 - Validation Loss : 0.6838\n",
      "Epoch(59) - Fold 4 - Validation Loss : 0.5539\n",
      "Epoch(59) - GLOBAL - Validation Loss: 0.6587\n",
      "Saving model corresponding to last_loss == 0.6586852517583145\n",
      "\n",
      "\n",
      "Epoch(60) - Training Loss: 0.6661\n",
      "Epoch(60) - Fold 0 - Validation Loss : 0.7520\n",
      "Epoch(60) - Fold 1 - Validation Loss : 0.6202\n",
      "Epoch(60) - Fold 2 - Validation Loss : 0.6650\n",
      "Epoch(60) - Fold 3 - Validation Loss : 0.6791\n",
      "Epoch(60) - Fold 4 - Validation Loss : 0.5488\n",
      "Epoch(60) - GLOBAL - Validation Loss: 0.6530\n",
      "Saving model corresponding to last_loss == 0.6530343329729159\n",
      "\n",
      "\n",
      "Epoch(61) - Training Loss: 0.6574\n",
      "Epoch(61) - Fold 0 - Validation Loss : 0.7436\n",
      "Epoch(61) - Fold 1 - Validation Loss : 0.6131\n",
      "Epoch(61) - Fold 2 - Validation Loss : 0.6568\n",
      "Epoch(61) - Fold 3 - Validation Loss : 0.6720\n",
      "Epoch(61) - Fold 4 - Validation Loss : 0.5430\n",
      "Epoch(61) - GLOBAL - Validation Loss: 0.6457\n",
      "Saving model corresponding to last_loss == 0.6457045836782489\n",
      "\n",
      "\n",
      "Epoch(62) - Training Loss: 0.6504\n",
      "Epoch(62) - Fold 0 - Validation Loss : 0.7369\n",
      "Epoch(62) - Fold 1 - Validation Loss : 0.6070\n",
      "Epoch(62) - Fold 2 - Validation Loss : 0.6503\n",
      "Epoch(62) - Fold 3 - Validation Loss : 0.6655\n",
      "Epoch(62) - Fold 4 - Validation Loss : 0.5377\n",
      "Epoch(62) - GLOBAL - Validation Loss: 0.6395\n",
      "Saving model corresponding to last_loss == 0.6394802005297241\n",
      "\n",
      "\n",
      "Epoch(63) - Training Loss: 0.6419\n",
      "Epoch(63) - Fold 0 - Validation Loss : 0.7281\n",
      "Epoch(63) - Fold 1 - Validation Loss : 0.6005\n",
      "Epoch(63) - Fold 2 - Validation Loss : 0.6428\n",
      "Epoch(63) - Fold 3 - Validation Loss : 0.6584\n",
      "Epoch(63) - Fold 4 - Validation Loss : 0.5315\n",
      "Epoch(63) - GLOBAL - Validation Loss: 0.6323\n",
      "Saving model corresponding to last_loss == 0.632265910273784\n",
      "\n",
      "\n",
      "Epoch(64) - Training Loss: 0.6343\n",
      "Epoch(64) - Fold 0 - Validation Loss : 0.7220\n",
      "Epoch(64) - Fold 1 - Validation Loss : 0.5951\n",
      "Epoch(64) - Fold 2 - Validation Loss : 0.6369\n",
      "Epoch(64) - Fold 3 - Validation Loss : 0.6536\n",
      "Epoch(64) - Fold 4 - Validation Loss : 0.5268\n",
      "Epoch(64) - GLOBAL - Validation Loss: 0.6269\n",
      "Saving model corresponding to last_loss == 0.6268703071535496\n",
      "\n",
      "\n",
      "Epoch(65) - Training Loss: 0.6282\n",
      "Epoch(65) - Fold 0 - Validation Loss : 0.7146\n",
      "Epoch(65) - Fold 1 - Validation Loss : 0.5893\n",
      "Epoch(65) - Fold 2 - Validation Loss : 0.6299\n",
      "Epoch(65) - Fold 3 - Validation Loss : 0.6467\n",
      "Epoch(65) - Fold 4 - Validation Loss : 0.5206\n",
      "Epoch(65) - GLOBAL - Validation Loss: 0.6202\n",
      "Saving model corresponding to last_loss == 0.6202126110649229\n",
      "\n",
      "\n",
      "Epoch(66) - Training Loss: 0.6217\n",
      "Epoch(66) - Fold 0 - Validation Loss : 0.7092\n",
      "Epoch(66) - Fold 1 - Validation Loss : 0.5850\n",
      "Epoch(66) - Fold 2 - Validation Loss : 0.6255\n",
      "Epoch(66) - Fold 3 - Validation Loss : 0.6428\n",
      "Epoch(66) - Fold 4 - Validation Loss : 0.5172\n",
      "Epoch(66) - GLOBAL - Validation Loss: 0.6159\n",
      "Saving model corresponding to last_loss == 0.6159320073419263\n",
      "\n",
      "\n",
      "Epoch(67) - Training Loss: 0.6147\n",
      "Epoch(67) - Fold 0 - Validation Loss : 0.7014\n",
      "Epoch(67) - Fold 1 - Validation Loss : 0.5791\n",
      "Epoch(67) - Fold 2 - Validation Loss : 0.6196\n",
      "Epoch(67) - Fold 3 - Validation Loss : 0.6380\n",
      "Epoch(67) - Fold 4 - Validation Loss : 0.5130\n",
      "Epoch(67) - GLOBAL - Validation Loss: 0.6102\n",
      "Saving model corresponding to last_loss == 0.6102205934410276\n",
      "\n",
      "\n",
      "Epoch(68) - Training Loss: 0.6071\n",
      "Epoch(68) - Fold 0 - Validation Loss : 0.6938\n",
      "Epoch(68) - Fold 1 - Validation Loss : 0.5726\n",
      "Epoch(68) - Fold 2 - Validation Loss : 0.6118\n",
      "Epoch(68) - Fold 3 - Validation Loss : 0.6306\n",
      "Epoch(68) - Fold 4 - Validation Loss : 0.5069\n",
      "Epoch(68) - GLOBAL - Validation Loss: 0.6031\n",
      "Saving model corresponding to last_loss == 0.6031208584485716\n",
      "\n",
      "\n",
      "Epoch(69) - Training Loss: 0.5995\n",
      "Epoch(69) - Fold 0 - Validation Loss : 0.6884\n",
      "Epoch(69) - Fold 1 - Validation Loss : 0.5683\n",
      "Epoch(69) - Fold 2 - Validation Loss : 0.6078\n",
      "Epoch(69) - Fold 3 - Validation Loss : 0.6263\n",
      "Epoch(69) - Fold 4 - Validation Loss : 0.5027\n",
      "Epoch(69) - GLOBAL - Validation Loss: 0.5987\n",
      "Saving model corresponding to last_loss == 0.5987019380749123\n",
      "\n",
      "\n",
      "Epoch(70) - Training Loss: 0.5929\n",
      "Epoch(70) - Fold 0 - Validation Loss : 0.6822\n",
      "Epoch(70) - Fold 1 - Validation Loss : 0.5631\n",
      "Epoch(70) - Fold 2 - Validation Loss : 0.6023\n",
      "Epoch(70) - Fold 3 - Validation Loss : 0.6217\n",
      "Epoch(70) - Fold 4 - Validation Loss : 0.4987\n",
      "Epoch(70) - GLOBAL - Validation Loss: 0.5936\n",
      "Saving model corresponding to last_loss == 0.5936123155407987\n",
      "\n",
      "\n",
      "Epoch(71) - Training Loss: 0.5855\n",
      "Epoch(71) - Fold 0 - Validation Loss : 0.6770\n",
      "Epoch(71) - Fold 1 - Validation Loss : 0.5586\n",
      "Epoch(71) - Fold 2 - Validation Loss : 0.5985\n",
      "Epoch(71) - Fold 3 - Validation Loss : 0.6174\n",
      "Epoch(71) - Fold 4 - Validation Loss : 0.4950\n",
      "Epoch(71) - GLOBAL - Validation Loss: 0.5893\n",
      "Saving model corresponding to last_loss == 0.5893026360770998\n",
      "\n",
      "\n",
      "Epoch(72) - Training Loss: 0.5812\n",
      "Epoch(72) - Fold 0 - Validation Loss : 0.6695\n",
      "Epoch(72) - Fold 1 - Validation Loss : 0.5522\n",
      "Epoch(72) - Fold 2 - Validation Loss : 0.5915\n",
      "Epoch(72) - Fold 3 - Validation Loss : 0.6115\n",
      "Epoch(72) - Fold 4 - Validation Loss : 0.4895\n",
      "Epoch(72) - GLOBAL - Validation Loss: 0.5828\n",
      "Saving model corresponding to last_loss == 0.5828167650052252\n",
      "\n",
      "\n",
      "Epoch(73) - Training Loss: 0.5727\n",
      "Epoch(73) - Fold 0 - Validation Loss : 0.6672\n",
      "Epoch(73) - Fold 1 - Validation Loss : 0.5497\n",
      "Epoch(73) - Fold 2 - Validation Loss : 0.5900\n",
      "Epoch(73) - Fold 3 - Validation Loss : 0.6109\n",
      "Epoch(73) - Fold 4 - Validation Loss : 0.4882\n",
      "Epoch(73) - GLOBAL - Validation Loss: 0.5812\n",
      "Saving model corresponding to last_loss == 0.5811946827201753\n",
      "\n",
      "\n",
      "Epoch(74) - Training Loss: 0.5671\n",
      "Epoch(74) - Fold 0 - Validation Loss : 0.6606\n",
      "Epoch(74) - Fold 1 - Validation Loss : 0.5432\n",
      "Epoch(74) - Fold 2 - Validation Loss : 0.5823\n",
      "Epoch(74) - Fold 3 - Validation Loss : 0.6035\n",
      "Epoch(74) - Fold 4 - Validation Loss : 0.4821\n",
      "Epoch(74) - GLOBAL - Validation Loss: 0.5743\n",
      "Saving model corresponding to last_loss == 0.5743408654515444\n",
      "\n",
      "\n",
      "Epoch(75) - Training Loss: 0.5604\n",
      "Epoch(75) - Fold 0 - Validation Loss : 0.6592\n",
      "Epoch(75) - Fold 1 - Validation Loss : 0.5421\n",
      "Epoch(75) - Fold 2 - Validation Loss : 0.5804\n",
      "Epoch(75) - Fold 3 - Validation Loss : 0.6009\n",
      "Epoch(75) - Fold 4 - Validation Loss : 0.4808\n",
      "Epoch(75) - GLOBAL - Validation Loss: 0.5727\n",
      "Saving model corresponding to last_loss == 0.5726683215976992\n",
      "\n",
      "\n",
      "Epoch(76) - Training Loss: 0.5558\n",
      "Epoch(76) - Fold 0 - Validation Loss : 0.6524\n",
      "Epoch(76) - Fold 1 - Validation Loss : 0.5370\n",
      "Epoch(76) - Fold 2 - Validation Loss : 0.5770\n",
      "Epoch(76) - Fold 3 - Validation Loss : 0.5972\n",
      "Epoch(76) - Fold 4 - Validation Loss : 0.4773\n",
      "Epoch(76) - GLOBAL - Validation Loss: 0.5682\n",
      "Saving model corresponding to last_loss == 0.5681860658380826\n",
      "\n",
      "\n",
      "Epoch(77) - Training Loss: 0.5486\n",
      "Epoch(77) - Fold 0 - Validation Loss : 0.6432\n",
      "Epoch(77) - Fold 1 - Validation Loss : 0.5288\n",
      "Epoch(77) - Fold 2 - Validation Loss : 0.5681\n",
      "Epoch(77) - Fold 3 - Validation Loss : 0.5905\n",
      "Epoch(77) - Fold 4 - Validation Loss : 0.4704\n",
      "Epoch(77) - GLOBAL - Validation Loss: 0.5602\n",
      "Saving model corresponding to last_loss == 0.5602012308547494\n",
      "\n",
      "\n",
      "Epoch(78) - Training Loss: 0.5420\n",
      "Epoch(78) - Fold 0 - Validation Loss : 0.6372\n",
      "Epoch(78) - Fold 1 - Validation Loss : 0.5244\n",
      "Epoch(78) - Fold 2 - Validation Loss : 0.5638\n",
      "Epoch(78) - Fold 3 - Validation Loss : 0.5850\n",
      "Epoch(78) - Fold 4 - Validation Loss : 0.4661\n",
      "Epoch(78) - GLOBAL - Validation Loss: 0.5553\n",
      "Saving model corresponding to last_loss == 0.5552974228975989\n",
      "\n",
      "\n",
      "Epoch(79) - Training Loss: 0.5360\n",
      "Epoch(79) - Fold 0 - Validation Loss : 0.6318\n",
      "Epoch(79) - Fold 1 - Validation Loss : 0.5206\n",
      "Epoch(79) - Fold 2 - Validation Loss : 0.5604\n",
      "Epoch(79) - Fold 3 - Validation Loss : 0.5809\n",
      "Epoch(79) - Fold 4 - Validation Loss : 0.4627\n",
      "Epoch(79) - GLOBAL - Validation Loss: 0.5513\n",
      "Saving model corresponding to last_loss == 0.5512842197338816\n",
      "\n",
      "\n",
      "Epoch(80) - Training Loss: 0.5307\n",
      "Epoch(80) - Fold 0 - Validation Loss : 0.6278\n",
      "Epoch(80) - Fold 1 - Validation Loss : 0.5169\n",
      "Epoch(80) - Fold 2 - Validation Loss : 0.5560\n",
      "Epoch(80) - Fold 3 - Validation Loss : 0.5767\n",
      "Epoch(80) - Fold 4 - Validation Loss : 0.4598\n",
      "Epoch(80) - GLOBAL - Validation Loss: 0.5474\n",
      "Saving model corresponding to last_loss == 0.5474319264895643\n",
      "\n",
      "\n",
      "Epoch(81) - Training Loss: 0.5261\n",
      "Epoch(81) - Fold 0 - Validation Loss : 0.6221\n",
      "Epoch(81) - Fold 1 - Validation Loss : 0.5124\n",
      "Epoch(81) - Fold 2 - Validation Loss : 0.5518\n",
      "Epoch(81) - Fold 3 - Validation Loss : 0.5716\n",
      "Epoch(81) - Fold 4 - Validation Loss : 0.4549\n",
      "Epoch(81) - GLOBAL - Validation Loss: 0.5426\n",
      "Saving model corresponding to last_loss == 0.5425662013118077\n",
      "\n",
      "\n",
      "Epoch(82) - Training Loss: 0.5213\n",
      "Epoch(82) - Fold 0 - Validation Loss : 0.6171\n",
      "Epoch(82) - Fold 1 - Validation Loss : 0.5079\n",
      "Epoch(82) - Fold 2 - Validation Loss : 0.5468\n",
      "Epoch(82) - Fold 3 - Validation Loss : 0.5676\n",
      "Epoch(82) - Fold 4 - Validation Loss : 0.4518\n",
      "Epoch(82) - GLOBAL - Validation Loss: 0.5382\n",
      "Saving model corresponding to last_loss == 0.5382402951435702\n",
      "\n",
      "\n",
      "Epoch(83) - Training Loss: 0.5185\n",
      "Epoch(83) - Fold 0 - Validation Loss : 0.6130\n",
      "Epoch(83) - Fold 1 - Validation Loss : 0.5048\n",
      "Epoch(83) - Fold 2 - Validation Loss : 0.5435\n",
      "Epoch(83) - Fold 3 - Validation Loss : 0.5643\n",
      "Epoch(83) - Fold 4 - Validation Loss : 0.4487\n",
      "Epoch(83) - GLOBAL - Validation Loss: 0.5348\n",
      "Saving model corresponding to last_loss == 0.534847069798582\n",
      "\n",
      "\n",
      "Epoch(84) - Training Loss: 0.5134\n",
      "Epoch(84) - Fold 0 - Validation Loss : 0.6086\n",
      "Epoch(84) - Fold 1 - Validation Loss : 0.5007\n",
      "Epoch(84) - Fold 2 - Validation Loss : 0.5393\n",
      "Epoch(84) - Fold 3 - Validation Loss : 0.5597\n",
      "Epoch(84) - Fold 4 - Validation Loss : 0.4451\n",
      "Epoch(84) - GLOBAL - Validation Loss: 0.5307\n",
      "Saving model corresponding to last_loss == 0.5306833494079316\n",
      "\n",
      "\n",
      "Epoch(85) - Training Loss: 0.5104\n",
      "Epoch(85) - Fold 0 - Validation Loss : 0.6054\n",
      "Epoch(85) - Fold 1 - Validation Loss : 0.4978\n",
      "Epoch(85) - Fold 2 - Validation Loss : 0.5358\n",
      "Epoch(85) - Fold 3 - Validation Loss : 0.5560\n",
      "Epoch(85) - Fold 4 - Validation Loss : 0.4423\n",
      "Epoch(85) - GLOBAL - Validation Loss: 0.5275\n",
      "Saving model corresponding to last_loss == 0.5274665382458573\n",
      "\n",
      "\n",
      "Epoch(86) - Training Loss: 0.5062\n",
      "Epoch(86) - Fold 0 - Validation Loss : 0.5995\n",
      "Epoch(86) - Fold 1 - Validation Loss : 0.4933\n",
      "Epoch(86) - Fold 2 - Validation Loss : 0.5304\n",
      "Epoch(86) - Fold 3 - Validation Loss : 0.5502\n",
      "Epoch(86) - Fold 4 - Validation Loss : 0.4374\n",
      "Epoch(86) - GLOBAL - Validation Loss: 0.5222\n",
      "Saving model corresponding to last_loss == 0.5221883420910642\n",
      "\n",
      "\n",
      "Epoch(87) - Training Loss: 0.5061\n",
      "Epoch(87) - Fold 0 - Validation Loss : 0.5968\n",
      "Epoch(87) - Fold 1 - Validation Loss : 0.4908\n",
      "Epoch(87) - Fold 2 - Validation Loss : 0.5278\n",
      "Epoch(87) - Fold 3 - Validation Loss : 0.5471\n",
      "Epoch(87) - Fold 4 - Validation Loss : 0.4350\n",
      "Epoch(87) - GLOBAL - Validation Loss: 0.5195\n",
      "Saving model corresponding to last_loss == 0.5194934324469003\n",
      "\n",
      "\n",
      "Epoch(88) - Training Loss: 0.5112\n",
      "Epoch(88) - Fold 0 - Validation Loss : 0.5944\n",
      "Epoch(88) - Fold 1 - Validation Loss : 0.4891\n",
      "Epoch(88) - Fold 2 - Validation Loss : 0.5263\n",
      "Epoch(88) - Fold 3 - Validation Loss : 0.5458\n",
      "Epoch(88) - Fold 4 - Validation Loss : 0.4335\n",
      "Epoch(88) - GLOBAL - Validation Loss: 0.5178\n",
      "Saving model corresponding to last_loss == 0.5178070548295943\n",
      "\n",
      "\n",
      "Epoch(89) - Training Loss: 0.4976\n",
      "Epoch(89) - Fold 0 - Validation Loss : 0.5876\n",
      "Epoch(89) - Fold 1 - Validation Loss : 0.4836\n",
      "Epoch(89) - Fold 2 - Validation Loss : 0.5205\n",
      "Epoch(89) - Fold 3 - Validation Loss : 0.5405\n",
      "Epoch(89) - Fold 4 - Validation Loss : 0.4291\n",
      "Epoch(89) - GLOBAL - Validation Loss: 0.5122\n",
      "Saving model corresponding to last_loss == 0.5122446071895936\n",
      "\n",
      "\n",
      "Epoch(90) - Training Loss: 0.4926\n",
      "Epoch(90) - Fold 0 - Validation Loss : 0.5841\n",
      "Epoch(90) - Fold 1 - Validation Loss : 0.4808\n",
      "Epoch(90) - Fold 2 - Validation Loss : 0.5171\n",
      "Epoch(90) - Fold 3 - Validation Loss : 0.5365\n",
      "Epoch(90) - Fold 4 - Validation Loss : 0.4262\n",
      "Epoch(90) - GLOBAL - Validation Loss: 0.5090\n",
      "Saving model corresponding to last_loss == 0.5089555156686077\n",
      "\n",
      "\n",
      "Epoch(91) - Training Loss: 0.4893\n",
      "Epoch(91) - Fold 0 - Validation Loss : 0.5810\n",
      "Epoch(91) - Fold 1 - Validation Loss : 0.4777\n",
      "Epoch(91) - Fold 2 - Validation Loss : 0.5132\n",
      "Epoch(91) - Fold 3 - Validation Loss : 0.5326\n",
      "Epoch(91) - Fold 4 - Validation Loss : 0.4227\n",
      "Epoch(91) - GLOBAL - Validation Loss: 0.5054\n",
      "Saving model corresponding to last_loss == 0.5054258486172937\n",
      "\n",
      "\n",
      "Epoch(92) - Training Loss: 0.4863\n",
      "Epoch(92) - Fold 0 - Validation Loss : 0.5779\n",
      "Epoch(92) - Fold 1 - Validation Loss : 0.4752\n",
      "Epoch(92) - Fold 2 - Validation Loss : 0.5110\n",
      "Epoch(92) - Fold 3 - Validation Loss : 0.5301\n",
      "Epoch(92) - Fold 4 - Validation Loss : 0.4205\n",
      "Epoch(92) - GLOBAL - Validation Loss: 0.5030\n",
      "Saving model corresponding to last_loss == 0.5029584367112291\n",
      "\n",
      "\n",
      "Epoch(93) - Training Loss: 0.4835\n",
      "Epoch(93) - Fold 0 - Validation Loss : 0.5756\n",
      "Epoch(93) - Fold 1 - Validation Loss : 0.4732\n",
      "Epoch(93) - Fold 2 - Validation Loss : 0.5092\n",
      "Epoch(93) - Fold 3 - Validation Loss : 0.5275\n",
      "Epoch(93) - Fold 4 - Validation Loss : 0.4188\n",
      "Epoch(93) - GLOBAL - Validation Loss: 0.5009\n",
      "Saving model corresponding to last_loss == 0.5008552307127091\n",
      "\n",
      "\n",
      "Epoch(94) - Training Loss: 0.4808\n",
      "Epoch(94) - Fold 0 - Validation Loss : 0.5723\n",
      "Epoch(94) - Fold 1 - Validation Loss : 0.4705\n",
      "Epoch(94) - Fold 2 - Validation Loss : 0.5062\n",
      "Epoch(94) - Fold 3 - Validation Loss : 0.5252\n",
      "Epoch(94) - Fold 4 - Validation Loss : 0.4169\n",
      "Epoch(94) - GLOBAL - Validation Loss: 0.4982\n",
      "Saving model corresponding to last_loss == 0.49822071119509925\n",
      "\n",
      "\n",
      "Epoch(95) - Training Loss: 0.4780\n",
      "Epoch(95) - Fold 0 - Validation Loss : 0.5679\n",
      "Epoch(95) - Fold 1 - Validation Loss : 0.4672\n",
      "Epoch(95) - Fold 2 - Validation Loss : 0.5017\n",
      "Epoch(95) - Fold 3 - Validation Loss : 0.5206\n",
      "Epoch(95) - Fold 4 - Validation Loss : 0.4128\n",
      "Epoch(95) - GLOBAL - Validation Loss: 0.4940\n",
      "Saving model corresponding to last_loss == 0.49403679611268425\n",
      "\n",
      "\n",
      "Epoch(96) - Training Loss: 0.4755\n",
      "Epoch(96) - Fold 0 - Validation Loss : 0.5650\n",
      "Epoch(96) - Fold 1 - Validation Loss : 0.4645\n",
      "Epoch(96) - Fold 2 - Validation Loss : 0.4992\n",
      "Epoch(96) - Fold 3 - Validation Loss : 0.5182\n",
      "Epoch(96) - Fold 4 - Validation Loss : 0.4103\n",
      "Epoch(96) - GLOBAL - Validation Loss: 0.4914\n",
      "Saving model corresponding to last_loss == 0.4914387775025696\n",
      "\n",
      "\n",
      "Epoch(97) - Training Loss: 0.4730\n",
      "Epoch(97) - Fold 0 - Validation Loss : 0.5614\n",
      "Epoch(97) - Fold 1 - Validation Loss : 0.4619\n",
      "Epoch(97) - Fold 2 - Validation Loss : 0.4960\n",
      "Epoch(97) - Fold 3 - Validation Loss : 0.5150\n",
      "Epoch(97) - Fold 4 - Validation Loss : 0.4081\n",
      "Epoch(97) - GLOBAL - Validation Loss: 0.4885\n",
      "Saving model corresponding to last_loss == 0.48849914049659027\n",
      "\n",
      "\n",
      "Epoch(98) - Training Loss: 0.4704\n",
      "Epoch(98) - Fold 0 - Validation Loss : 0.5591\n",
      "Epoch(98) - Fold 1 - Validation Loss : 0.4595\n",
      "Epoch(98) - Fold 2 - Validation Loss : 0.4939\n",
      "Epoch(98) - Fold 3 - Validation Loss : 0.5129\n",
      "Epoch(98) - Fold 4 - Validation Loss : 0.4062\n",
      "Epoch(98) - GLOBAL - Validation Loss: 0.4863\n",
      "Saving model corresponding to last_loss == 0.4863095806402673\n",
      "\n",
      "\n",
      "Epoch(99) - Training Loss: 0.4677\n",
      "Epoch(99) - Fold 0 - Validation Loss : 0.5562\n",
      "Epoch(99) - Fold 1 - Validation Loss : 0.4574\n",
      "Epoch(99) - Fold 2 - Validation Loss : 0.4916\n",
      "Epoch(99) - Fold 3 - Validation Loss : 0.5095\n",
      "Epoch(99) - Fold 4 - Validation Loss : 0.4038\n",
      "Epoch(99) - GLOBAL - Validation Loss: 0.4837\n",
      "Saving model corresponding to last_loss == 0.4837245056053609\n",
      "\n",
      "\n",
      "Epoch(100) - Training Loss: 0.4654\n",
      "Epoch(100) - Fold 0 - Validation Loss : 0.5540\n",
      "Epoch(100) - Fold 1 - Validation Loss : 0.4558\n",
      "Epoch(100) - Fold 2 - Validation Loss : 0.4895\n",
      "Epoch(100) - Fold 3 - Validation Loss : 0.5071\n",
      "Epoch(100) - Fold 4 - Validation Loss : 0.4022\n",
      "Epoch(100) - GLOBAL - Validation Loss: 0.4817\n",
      "Saving model corresponding to last_loss == 0.4817222788300513\n",
      "\n",
      "\n",
      "Epoch(101) - Training Loss: 0.4643\n",
      "Epoch(101) - Fold 0 - Validation Loss : 0.5548\n",
      "Epoch(101) - Fold 1 - Validation Loss : 0.4558\n",
      "Epoch(101) - Fold 2 - Validation Loss : 0.4892\n",
      "Epoch(101) - Fold 3 - Validation Loss : 0.5076\n",
      "Epoch(101) - Fold 4 - Validation Loss : 0.4026\n",
      "Epoch(101) - GLOBAL - Validation Loss: 0.4820\n",
      "Intermediate early stopping : vepoch_loss = 0.4820, the_last_loss=0.4817\n",
      "\n",
      "\n",
      "Epoch(102) - Training Loss: 0.4652\n",
      "Epoch(102) - Fold 0 - Validation Loss : 0.5500\n",
      "Epoch(102) - Fold 1 - Validation Loss : 0.4522\n",
      "Epoch(102) - Fold 2 - Validation Loss : 0.4850\n",
      "Epoch(102) - Fold 3 - Validation Loss : 0.5030\n",
      "Epoch(102) - Fold 4 - Validation Loss : 0.3987\n",
      "Epoch(102) - GLOBAL - Validation Loss: 0.4778\n",
      "Saving model corresponding to last_loss == 0.47780216612371634\n",
      "\n",
      "\n",
      "Epoch(103) - Training Loss: 0.4600\n",
      "Epoch(103) - Fold 0 - Validation Loss : 0.5467\n",
      "Epoch(103) - Fold 1 - Validation Loss : 0.4494\n",
      "Epoch(103) - Fold 2 - Validation Loss : 0.4814\n",
      "Epoch(103) - Fold 3 - Validation Loss : 0.4999\n",
      "Epoch(103) - Fold 4 - Validation Loss : 0.3957\n",
      "Epoch(103) - GLOBAL - Validation Loss: 0.4746\n",
      "Saving model corresponding to last_loss == 0.47462880059095713\n",
      "\n",
      "\n",
      "Epoch(104) - Training Loss: 0.4582\n",
      "Epoch(104) - Fold 0 - Validation Loss : 0.5445\n",
      "Epoch(104) - Fold 1 - Validation Loss : 0.4473\n",
      "Epoch(104) - Fold 2 - Validation Loss : 0.4796\n",
      "Epoch(104) - Fold 3 - Validation Loss : 0.4973\n",
      "Epoch(104) - Fold 4 - Validation Loss : 0.3939\n",
      "Epoch(104) - GLOBAL - Validation Loss: 0.4725\n",
      "Saving model corresponding to last_loss == 0.4725417402054141\n",
      "\n",
      "\n",
      "Epoch(105) - Training Loss: 0.4556\n",
      "Epoch(105) - Fold 0 - Validation Loss : 0.5419\n",
      "Epoch(105) - Fold 1 - Validation Loss : 0.4454\n",
      "Epoch(105) - Fold 2 - Validation Loss : 0.4777\n",
      "Epoch(105) - Fold 3 - Validation Loss : 0.4954\n",
      "Epoch(105) - Fold 4 - Validation Loss : 0.3924\n",
      "Epoch(105) - GLOBAL - Validation Loss: 0.4705\n",
      "Saving model corresponding to last_loss == 0.47054247930423243\n",
      "\n",
      "\n",
      "Epoch(106) - Training Loss: 0.4545\n",
      "Epoch(106) - Fold 0 - Validation Loss : 0.5410\n",
      "Epoch(106) - Fold 1 - Validation Loss : 0.4440\n",
      "Epoch(106) - Fold 2 - Validation Loss : 0.4758\n",
      "Epoch(106) - Fold 3 - Validation Loss : 0.4943\n",
      "Epoch(106) - Fold 4 - Validation Loss : 0.3909\n",
      "Epoch(106) - GLOBAL - Validation Loss: 0.4692\n",
      "Saving model corresponding to last_loss == 0.4692047432747982\n",
      "\n",
      "\n",
      "Epoch(107) - Training Loss: 0.4539\n",
      "Epoch(107) - Fold 0 - Validation Loss : 0.5389\n",
      "Epoch(107) - Fold 1 - Validation Loss : 0.4428\n",
      "Epoch(107) - Fold 2 - Validation Loss : 0.4746\n",
      "Epoch(107) - Fold 3 - Validation Loss : 0.4925\n",
      "Epoch(107) - Fold 4 - Validation Loss : 0.3894\n",
      "Epoch(107) - GLOBAL - Validation Loss: 0.4676\n",
      "Saving model corresponding to last_loss == 0.46761175153408113\n",
      "\n",
      "\n",
      "Epoch(108) - Training Loss: 0.4519\n",
      "Epoch(108) - Fold 0 - Validation Loss : 0.5359\n",
      "Epoch(108) - Fold 1 - Validation Loss : 0.4409\n",
      "Epoch(108) - Fold 2 - Validation Loss : 0.4722\n",
      "Epoch(108) - Fold 3 - Validation Loss : 0.4899\n",
      "Epoch(108) - Fold 4 - Validation Loss : 0.3878\n",
      "Epoch(108) - GLOBAL - Validation Loss: 0.4653\n",
      "Saving model corresponding to last_loss == 0.46534296644822126\n",
      "\n",
      "\n",
      "Epoch(109) - Training Loss: 0.4494\n",
      "Epoch(109) - Fold 0 - Validation Loss : 0.5343\n",
      "Epoch(109) - Fold 1 - Validation Loss : 0.4388\n",
      "Epoch(109) - Fold 2 - Validation Loss : 0.4703\n",
      "Epoch(109) - Fold 3 - Validation Loss : 0.4882\n",
      "Epoch(109) - Fold 4 - Validation Loss : 0.3858\n",
      "Epoch(109) - GLOBAL - Validation Loss: 0.4635\n",
      "Saving model corresponding to last_loss == 0.4634663503726964\n",
      "\n",
      "\n",
      "Epoch(110) - Training Loss: 0.4479\n",
      "Epoch(110) - Fold 0 - Validation Loss : 0.5323\n",
      "Epoch(110) - Fold 1 - Validation Loss : 0.4374\n",
      "Epoch(110) - Fold 2 - Validation Loss : 0.4687\n",
      "Epoch(110) - Fold 3 - Validation Loss : 0.4863\n",
      "Epoch(110) - Fold 4 - Validation Loss : 0.3848\n",
      "Epoch(110) - GLOBAL - Validation Loss: 0.4619\n",
      "Saving model corresponding to last_loss == 0.4619002930733197\n",
      "\n",
      "\n",
      "Epoch(111) - Training Loss: 0.4464\n",
      "Epoch(111) - Fold 0 - Validation Loss : 0.5354\n",
      "Epoch(111) - Fold 1 - Validation Loss : 0.4394\n",
      "Epoch(111) - Fold 2 - Validation Loss : 0.4702\n",
      "Epoch(111) - Fold 3 - Validation Loss : 0.4877\n",
      "Epoch(111) - Fold 4 - Validation Loss : 0.3863\n",
      "Epoch(111) - GLOBAL - Validation Loss: 0.4638\n",
      "Intermediate early stopping : vepoch_loss = 0.4638, the_last_loss=0.4619\n",
      "\n",
      "\n",
      "Epoch(112) - Training Loss: 0.4477\n",
      "Epoch(112) - Fold 0 - Validation Loss : 0.5326\n",
      "Epoch(112) - Fold 1 - Validation Loss : 0.4373\n",
      "Epoch(112) - Fold 2 - Validation Loss : 0.4689\n",
      "Epoch(112) - Fold 3 - Validation Loss : 0.4859\n",
      "Epoch(112) - Fold 4 - Validation Loss : 0.3849\n",
      "Epoch(112) - GLOBAL - Validation Loss: 0.4619\n",
      "Intermediate early stopping : vepoch_loss = 0.4619, the_last_loss=0.4619\n",
      "\n",
      "\n",
      "Epoch(113) - Training Loss: 0.4473\n",
      "Epoch(113) - Fold 0 - Validation Loss : 0.5288\n",
      "Epoch(113) - Fold 1 - Validation Loss : 0.4347\n",
      "Epoch(113) - Fold 2 - Validation Loss : 0.4656\n",
      "Epoch(113) - Fold 3 - Validation Loss : 0.4827\n",
      "Epoch(113) - Fold 4 - Validation Loss : 0.3818\n",
      "Epoch(113) - GLOBAL - Validation Loss: 0.4587\n",
      "Saving model corresponding to last_loss == 0.45872297600211087\n",
      "\n",
      "\n",
      "Epoch(114) - Training Loss: 0.4441\n",
      "Epoch(114) - Fold 0 - Validation Loss : 0.5278\n",
      "Epoch(114) - Fold 1 - Validation Loss : 0.4336\n",
      "Epoch(114) - Fold 2 - Validation Loss : 0.4643\n",
      "Epoch(114) - Fold 3 - Validation Loss : 0.4816\n",
      "Epoch(114) - Fold 4 - Validation Loss : 0.3808\n",
      "Epoch(114) - GLOBAL - Validation Loss: 0.4576\n",
      "Saving model corresponding to last_loss == 0.4576026605727329\n",
      "\n",
      "\n",
      "Epoch(115) - Training Loss: 0.4442\n",
      "Epoch(115) - Fold 0 - Validation Loss : 0.5292\n",
      "Epoch(115) - Fold 1 - Validation Loss : 0.4345\n",
      "Epoch(115) - Fold 2 - Validation Loss : 0.4653\n",
      "Epoch(115) - Fold 3 - Validation Loss : 0.4825\n",
      "Epoch(115) - Fold 4 - Validation Loss : 0.3818\n",
      "Epoch(115) - GLOBAL - Validation Loss: 0.4586\n",
      "Intermediate early stopping : vepoch_loss = 0.4586, the_last_loss=0.4576\n",
      "\n",
      "\n",
      "Epoch(116) - Training Loss: 0.4444\n",
      "Epoch(116) - Fold 0 - Validation Loss : 0.5239\n",
      "Epoch(116) - Fold 1 - Validation Loss : 0.4307\n",
      "Epoch(116) - Fold 2 - Validation Loss : 0.4613\n",
      "Epoch(116) - Fold 3 - Validation Loss : 0.4782\n",
      "Epoch(116) - Fold 4 - Validation Loss : 0.3779\n",
      "Epoch(116) - GLOBAL - Validation Loss: 0.4544\n",
      "Saving model corresponding to last_loss == 0.45438356813002595\n",
      "\n",
      "\n",
      "Epoch(117) - Training Loss: 0.4434\n",
      "Epoch(117) - Fold 0 - Validation Loss : 0.5238\n",
      "Epoch(117) - Fold 1 - Validation Loss : 0.4302\n",
      "Epoch(117) - Fold 2 - Validation Loss : 0.4603\n",
      "Epoch(117) - Fold 3 - Validation Loss : 0.4773\n",
      "Epoch(117) - Fold 4 - Validation Loss : 0.3775\n",
      "Epoch(117) - GLOBAL - Validation Loss: 0.4538\n",
      "Saving model corresponding to last_loss == 0.4538114139904586\n",
      "\n",
      "\n",
      "Epoch(118) - Training Loss: 0.4417\n",
      "Epoch(118) - Fold 0 - Validation Loss : 0.5221\n",
      "Epoch(118) - Fold 1 - Validation Loss : 0.4285\n",
      "Epoch(118) - Fold 2 - Validation Loss : 0.4587\n",
      "Epoch(118) - Fold 3 - Validation Loss : 0.4760\n",
      "Epoch(118) - Fold 4 - Validation Loss : 0.3759\n",
      "Epoch(118) - GLOBAL - Validation Loss: 0.4522\n",
      "Saving model corresponding to last_loss == 0.45222969602076174\n",
      "\n",
      "\n",
      "Epoch(119) - Training Loss: 0.4387\n",
      "Epoch(119) - Fold 0 - Validation Loss : 0.5201\n",
      "Epoch(119) - Fold 1 - Validation Loss : 0.4270\n",
      "Epoch(119) - Fold 2 - Validation Loss : 0.4572\n",
      "Epoch(119) - Fold 3 - Validation Loss : 0.4744\n",
      "Epoch(119) - Fold 4 - Validation Loss : 0.3747\n",
      "Epoch(119) - GLOBAL - Validation Loss: 0.4507\n",
      "Saving model corresponding to last_loss == 0.45068457666215356\n",
      "\n",
      "\n",
      "Epoch(120) - Training Loss: 0.4361\n",
      "Epoch(120) - Fold 0 - Validation Loss : 0.5188\n",
      "Epoch(120) - Fold 1 - Validation Loss : 0.4262\n",
      "Epoch(120) - Fold 2 - Validation Loss : 0.4560\n",
      "Epoch(120) - Fold 3 - Validation Loss : 0.4728\n",
      "Epoch(120) - Fold 4 - Validation Loss : 0.3733\n",
      "Epoch(120) - GLOBAL - Validation Loss: 0.4494\n",
      "Saving model corresponding to last_loss == 0.4494108738790299\n",
      "\n",
      "\n",
      "Epoch(121) - Training Loss: 0.4348\n",
      "Epoch(121) - Fold 0 - Validation Loss : 0.5179\n",
      "Epoch(121) - Fold 1 - Validation Loss : 0.4257\n",
      "Epoch(121) - Fold 2 - Validation Loss : 0.4553\n",
      "Epoch(121) - Fold 3 - Validation Loss : 0.4717\n",
      "Epoch(121) - Fold 4 - Validation Loss : 0.3726\n",
      "Epoch(121) - GLOBAL - Validation Loss: 0.4486\n",
      "Saving model corresponding to last_loss == 0.4486205740514363\n",
      "\n",
      "\n",
      "Epoch(122) - Training Loss: 0.4341\n",
      "Epoch(122) - Fold 0 - Validation Loss : 0.5167\n",
      "Epoch(122) - Fold 1 - Validation Loss : 0.4242\n",
      "Epoch(122) - Fold 2 - Validation Loss : 0.4539\n",
      "Epoch(122) - Fold 3 - Validation Loss : 0.4710\n",
      "Epoch(122) - Fold 4 - Validation Loss : 0.3719\n",
      "Epoch(122) - GLOBAL - Validation Loss: 0.4475\n",
      "Saving model corresponding to last_loss == 0.44754021671943056\n",
      "\n",
      "\n",
      "Epoch(123) - Training Loss: 0.4332\n",
      "Epoch(123) - Fold 0 - Validation Loss : 0.5158\n",
      "Epoch(123) - Fold 1 - Validation Loss : 0.4237\n",
      "Epoch(123) - Fold 2 - Validation Loss : 0.4532\n",
      "Epoch(123) - Fold 3 - Validation Loss : 0.4702\n",
      "Epoch(123) - Fold 4 - Validation Loss : 0.3713\n",
      "Epoch(123) - GLOBAL - Validation Loss: 0.4468\n",
      "Saving model corresponding to last_loss == 0.4468294680440595\n",
      "\n",
      "\n",
      "Epoch(124) - Training Loss: 0.4322\n",
      "Epoch(124) - Fold 0 - Validation Loss : 0.5151\n",
      "Epoch(124) - Fold 1 - Validation Loss : 0.4224\n",
      "Epoch(124) - Fold 2 - Validation Loss : 0.4516\n",
      "Epoch(124) - Fold 3 - Validation Loss : 0.4682\n",
      "Epoch(124) - Fold 4 - Validation Loss : 0.3695\n",
      "Epoch(124) - GLOBAL - Validation Loss: 0.4454\n",
      "Saving model corresponding to last_loss == 0.4453761129400557\n",
      "\n",
      "\n",
      "Epoch(125) - Training Loss: 0.4313\n",
      "Epoch(125) - Fold 0 - Validation Loss : 0.5148\n",
      "Epoch(125) - Fold 1 - Validation Loss : 0.4224\n",
      "Epoch(125) - Fold 2 - Validation Loss : 0.4515\n",
      "Epoch(125) - Fold 3 - Validation Loss : 0.4677\n",
      "Epoch(125) - Fold 4 - Validation Loss : 0.3692\n",
      "Epoch(125) - GLOBAL - Validation Loss: 0.4451\n",
      "Saving model corresponding to last_loss == 0.4451371928476737\n",
      "\n",
      "\n",
      "Epoch(126) - Training Loss: 0.4305\n",
      "Epoch(126) - Fold 0 - Validation Loss : 0.5126\n",
      "Epoch(126) - Fold 1 - Validation Loss : 0.4211\n",
      "Epoch(126) - Fold 2 - Validation Loss : 0.4502\n",
      "Epoch(126) - Fold 3 - Validation Loss : 0.4672\n",
      "Epoch(126) - Fold 4 - Validation Loss : 0.3686\n",
      "Epoch(126) - GLOBAL - Validation Loss: 0.4439\n",
      "Saving model corresponding to last_loss == 0.44394420771767296\n",
      "\n",
      "\n",
      "Epoch(127) - Training Loss: 0.4299\n",
      "Epoch(127) - Fold 0 - Validation Loss : 0.5118\n",
      "Epoch(127) - Fold 1 - Validation Loss : 0.4200\n",
      "Epoch(127) - Fold 2 - Validation Loss : 0.4488\n",
      "Epoch(127) - Fold 3 - Validation Loss : 0.4655\n",
      "Epoch(127) - Fold 4 - Validation Loss : 0.3675\n",
      "Epoch(127) - GLOBAL - Validation Loss: 0.4427\n",
      "Saving model corresponding to last_loss == 0.44271282802488787\n",
      "\n",
      "\n",
      "Epoch(128) - Training Loss: 0.4291\n",
      "Epoch(128) - Fold 0 - Validation Loss : 0.5134\n",
      "Epoch(128) - Fold 1 - Validation Loss : 0.4214\n",
      "Epoch(128) - Fold 2 - Validation Loss : 0.4497\n",
      "Epoch(128) - Fold 3 - Validation Loss : 0.4656\n",
      "Epoch(128) - Fold 4 - Validation Loss : 0.3674\n",
      "Epoch(128) - GLOBAL - Validation Loss: 0.4435\n",
      "Intermediate early stopping : vepoch_loss = 0.4435, the_last_loss=0.4427\n",
      "\n",
      "\n",
      "Epoch(129) - Training Loss: 0.4294\n",
      "Epoch(129) - Fold 0 - Validation Loss : 0.5115\n",
      "Epoch(129) - Fold 1 - Validation Loss : 0.4196\n",
      "Epoch(129) - Fold 2 - Validation Loss : 0.4484\n",
      "Epoch(129) - Fold 3 - Validation Loss : 0.4651\n",
      "Epoch(129) - Fold 4 - Validation Loss : 0.3669\n",
      "Epoch(129) - GLOBAL - Validation Loss: 0.4423\n",
      "Saving model corresponding to last_loss == 0.44229974487521834\n",
      "\n",
      "\n",
      "Epoch(130) - Training Loss: 0.4277\n",
      "Epoch(130) - Fold 0 - Validation Loss : 0.5100\n",
      "Epoch(130) - Fold 1 - Validation Loss : 0.4186\n",
      "Epoch(130) - Fold 2 - Validation Loss : 0.4472\n",
      "Epoch(130) - Fold 3 - Validation Loss : 0.4635\n",
      "Epoch(130) - Fold 4 - Validation Loss : 0.3656\n",
      "Epoch(130) - GLOBAL - Validation Loss: 0.4410\n",
      "Saving model corresponding to last_loss == 0.44098519698477123\n",
      "\n",
      "\n",
      "Epoch(131) - Training Loss: 0.4270\n",
      "Epoch(131) - Fold 0 - Validation Loss : 0.5091\n",
      "Epoch(131) - Fold 1 - Validation Loss : 0.4175\n",
      "Epoch(131) - Fold 2 - Validation Loss : 0.4461\n",
      "Epoch(131) - Fold 3 - Validation Loss : 0.4626\n",
      "Epoch(131) - Fold 4 - Validation Loss : 0.3647\n",
      "Epoch(131) - GLOBAL - Validation Loss: 0.4400\n",
      "Saving model corresponding to last_loss == 0.44001306414605096\n",
      "\n",
      "\n",
      "Epoch(132) - Training Loss: 0.4270\n",
      "Epoch(132) - Fold 0 - Validation Loss : 0.5109\n",
      "Epoch(132) - Fold 1 - Validation Loss : 0.4187\n",
      "Epoch(132) - Fold 2 - Validation Loss : 0.4472\n",
      "Epoch(132) - Fold 3 - Validation Loss : 0.4637\n",
      "Epoch(132) - Fold 4 - Validation Loss : 0.3657\n",
      "Epoch(132) - GLOBAL - Validation Loss: 0.4412\n",
      "Intermediate early stopping : vepoch_loss = 0.4412, the_last_loss=0.4400\n",
      "\n",
      "\n",
      "Epoch(133) - Training Loss: 0.4274\n",
      "Epoch(133) - Fold 0 - Validation Loss : 0.5075\n",
      "Epoch(133) - Fold 1 - Validation Loss : 0.4164\n",
      "Epoch(133) - Fold 2 - Validation Loss : 0.4440\n",
      "Epoch(133) - Fold 3 - Validation Loss : 0.4609\n",
      "Epoch(133) - Fold 4 - Validation Loss : 0.3631\n",
      "Epoch(133) - GLOBAL - Validation Loss: 0.4384\n",
      "Saving model corresponding to last_loss == 0.4383704813572898\n",
      "\n",
      "\n",
      "Epoch(134) - Training Loss: 0.4255\n",
      "Epoch(134) - Fold 0 - Validation Loss : 0.5068\n",
      "Epoch(134) - Fold 1 - Validation Loss : 0.4158\n",
      "Epoch(134) - Fold 2 - Validation Loss : 0.4441\n",
      "Epoch(134) - Fold 3 - Validation Loss : 0.4604\n",
      "Epoch(134) - Fold 4 - Validation Loss : 0.3629\n",
      "Epoch(134) - GLOBAL - Validation Loss: 0.4380\n",
      "Saving model corresponding to last_loss == 0.4380024693011098\n",
      "\n",
      "\n",
      "Epoch(135) - Training Loss: 0.4251\n",
      "Epoch(135) - Fold 0 - Validation Loss : 0.5057\n",
      "Epoch(135) - Fold 1 - Validation Loss : 0.4150\n",
      "Epoch(135) - Fold 2 - Validation Loss : 0.4431\n",
      "Epoch(135) - Fold 3 - Validation Loss : 0.4592\n",
      "Epoch(135) - Fold 4 - Validation Loss : 0.3623\n",
      "Epoch(135) - GLOBAL - Validation Loss: 0.4371\n",
      "Saving model corresponding to last_loss == 0.43705924546291036\n",
      "\n",
      "\n",
      "Epoch(136) - Training Loss: 0.4249\n",
      "Epoch(136) - Fold 0 - Validation Loss : 0.5062\n",
      "Epoch(136) - Fold 1 - Validation Loss : 0.4154\n",
      "Epoch(136) - Fold 2 - Validation Loss : 0.4431\n",
      "Epoch(136) - Fold 3 - Validation Loss : 0.4599\n",
      "Epoch(136) - Fold 4 - Validation Loss : 0.3626\n",
      "Epoch(136) - GLOBAL - Validation Loss: 0.4374\n",
      "Intermediate early stopping : vepoch_loss = 0.4374, the_last_loss=0.4371\n",
      "\n",
      "\n",
      "Epoch(137) - Training Loss: 0.4240\n",
      "Epoch(137) - Fold 0 - Validation Loss : 0.5053\n",
      "Epoch(137) - Fold 1 - Validation Loss : 0.4144\n",
      "Epoch(137) - Fold 2 - Validation Loss : 0.4430\n",
      "Epoch(137) - Fold 3 - Validation Loss : 0.4593\n",
      "Epoch(137) - Fold 4 - Validation Loss : 0.3621\n",
      "Epoch(137) - GLOBAL - Validation Loss: 0.4368\n",
      "Saving model corresponding to last_loss == 0.436824971986187\n",
      "\n",
      "\n",
      "Epoch(138) - Training Loss: 0.4232\n",
      "Epoch(138) - Fold 0 - Validation Loss : 0.5069\n",
      "Epoch(138) - Fold 1 - Validation Loss : 0.4160\n",
      "Epoch(138) - Fold 2 - Validation Loss : 0.4440\n",
      "Epoch(138) - Fold 3 - Validation Loss : 0.4598\n",
      "Epoch(138) - Fold 4 - Validation Loss : 0.3630\n",
      "Epoch(138) - GLOBAL - Validation Loss: 0.4379\n",
      "Intermediate early stopping : vepoch_loss = 0.4379, the_last_loss=0.4368\n",
      "\n",
      "\n",
      "Epoch(139) - Training Loss: 0.4236\n",
      "Epoch(139) - Fold 0 - Validation Loss : 0.5022\n",
      "Epoch(139) - Fold 1 - Validation Loss : 0.4122\n",
      "Epoch(139) - Fold 2 - Validation Loss : 0.4398\n",
      "Epoch(139) - Fold 3 - Validation Loss : 0.4560\n",
      "Epoch(139) - Fold 4 - Validation Loss : 0.3593\n",
      "Epoch(139) - GLOBAL - Validation Loss: 0.4339\n",
      "Saving model corresponding to last_loss == 0.43390478653930603\n",
      "\n",
      "\n",
      "Epoch(140) - Training Loss: 0.4217\n",
      "Epoch(140) - Fold 0 - Validation Loss : 0.5014\n",
      "Epoch(140) - Fold 1 - Validation Loss : 0.4114\n",
      "Epoch(140) - Fold 2 - Validation Loss : 0.4398\n",
      "Epoch(140) - Fold 3 - Validation Loss : 0.4559\n",
      "Epoch(140) - Fold 4 - Validation Loss : 0.3590\n",
      "Epoch(140) - GLOBAL - Validation Loss: 0.4335\n",
      "Saving model corresponding to last_loss == 0.43347832026348454\n",
      "\n",
      "\n",
      "Epoch(141) - Training Loss: 0.4214\n",
      "Epoch(141) - Fold 0 - Validation Loss : 0.5026\n",
      "Epoch(141) - Fold 1 - Validation Loss : 0.4120\n",
      "Epoch(141) - Fold 2 - Validation Loss : 0.4397\n",
      "Epoch(141) - Fold 3 - Validation Loss : 0.4562\n",
      "Epoch(141) - Fold 4 - Validation Loss : 0.3591\n",
      "Epoch(141) - GLOBAL - Validation Loss: 0.4339\n",
      "Intermediate early stopping : vepoch_loss = 0.4339, the_last_loss=0.4335\n",
      "\n",
      "\n",
      "Epoch(142) - Training Loss: 0.4203\n",
      "Epoch(142) - Fold 0 - Validation Loss : 0.4996\n",
      "Epoch(142) - Fold 1 - Validation Loss : 0.4101\n",
      "Epoch(142) - Fold 2 - Validation Loss : 0.4377\n",
      "Epoch(142) - Fold 3 - Validation Loss : 0.4541\n",
      "Epoch(142) - Fold 4 - Validation Loss : 0.3573\n",
      "Epoch(142) - GLOBAL - Validation Loss: 0.4318\n",
      "Saving model corresponding to last_loss == 0.431756965456895\n",
      "\n",
      "\n",
      "Epoch(143) - Training Loss: 0.4192\n",
      "Epoch(143) - Fold 0 - Validation Loss : 0.5002\n",
      "Epoch(143) - Fold 1 - Validation Loss : 0.4098\n",
      "Epoch(143) - Fold 2 - Validation Loss : 0.4375\n",
      "Epoch(143) - Fold 3 - Validation Loss : 0.4541\n",
      "Epoch(143) - Fold 4 - Validation Loss : 0.3576\n",
      "Epoch(143) - GLOBAL - Validation Loss: 0.4318\n",
      "Intermediate early stopping : vepoch_loss = 0.4318, the_last_loss=0.4318\n",
      "\n",
      "\n",
      "Epoch(144) - Training Loss: 0.4195\n",
      "Epoch(144) - Fold 0 - Validation Loss : 0.4996\n",
      "Epoch(144) - Fold 1 - Validation Loss : 0.4094\n",
      "Epoch(144) - Fold 2 - Validation Loss : 0.4368\n",
      "Epoch(144) - Fold 3 - Validation Loss : 0.4531\n",
      "Epoch(144) - Fold 4 - Validation Loss : 0.3567\n",
      "Epoch(144) - GLOBAL - Validation Loss: 0.4311\n",
      "Saving model corresponding to last_loss == 0.4311237953515509\n",
      "\n",
      "\n",
      "Epoch(145) - Training Loss: 0.4190\n",
      "Epoch(145) - Fold 0 - Validation Loss : 0.4985\n",
      "Epoch(145) - Fold 1 - Validation Loss : 0.4086\n",
      "Epoch(145) - Fold 2 - Validation Loss : 0.4361\n",
      "Epoch(145) - Fold 3 - Validation Loss : 0.4522\n",
      "Epoch(145) - Fold 4 - Validation Loss : 0.3561\n",
      "Epoch(145) - GLOBAL - Validation Loss: 0.4303\n",
      "Saving model corresponding to last_loss == 0.43029930125678606\n",
      "\n",
      "\n",
      "Epoch(146) - Training Loss: 0.4181\n",
      "Epoch(146) - Fold 0 - Validation Loss : 0.4975\n",
      "Epoch(146) - Fold 1 - Validation Loss : 0.4078\n",
      "Epoch(146) - Fold 2 - Validation Loss : 0.4352\n",
      "Epoch(146) - Fold 3 - Validation Loss : 0.4512\n",
      "Epoch(146) - Fold 4 - Validation Loss : 0.3550\n",
      "Epoch(146) - GLOBAL - Validation Loss: 0.4293\n",
      "Saving model corresponding to last_loss == 0.4293444334615929\n",
      "\n",
      "\n",
      "Epoch(147) - Training Loss: 0.4170\n",
      "Epoch(147) - Fold 0 - Validation Loss : 0.4977\n",
      "Epoch(147) - Fold 1 - Validation Loss : 0.4079\n",
      "Epoch(147) - Fold 2 - Validation Loss : 0.4355\n",
      "Epoch(147) - Fold 3 - Validation Loss : 0.4515\n",
      "Epoch(147) - Fold 4 - Validation Loss : 0.3556\n",
      "Epoch(147) - GLOBAL - Validation Loss: 0.4296\n",
      "Intermediate early stopping : vepoch_loss = 0.4296, the_last_loss=0.4293\n",
      "\n",
      "\n",
      "Epoch(148) - Training Loss: 0.4167\n",
      "Epoch(148) - Fold 0 - Validation Loss : 0.4987\n",
      "Epoch(148) - Fold 1 - Validation Loss : 0.4083\n",
      "Epoch(148) - Fold 2 - Validation Loss : 0.4354\n",
      "Epoch(148) - Fold 3 - Validation Loss : 0.4516\n",
      "Epoch(148) - Fold 4 - Validation Loss : 0.3556\n",
      "Epoch(148) - GLOBAL - Validation Loss: 0.4299\n",
      "Intermediate early stopping : vepoch_loss = 0.4299, the_last_loss=0.4293\n",
      "\n",
      "\n",
      "Epoch(149) - Training Loss: 0.4175\n",
      "Epoch(149) - Fold 0 - Validation Loss : 0.4968\n",
      "Epoch(149) - Fold 1 - Validation Loss : 0.4069\n",
      "Epoch(149) - Fold 2 - Validation Loss : 0.4341\n",
      "Epoch(149) - Fold 3 - Validation Loss : 0.4497\n",
      "Epoch(149) - Fold 4 - Validation Loss : 0.3541\n",
      "Epoch(149) - GLOBAL - Validation Loss: 0.4283\n",
      "Saving model corresponding to last_loss == 0.4283200403041495\n",
      "\n",
      "\n",
      "Epoch(150) - Training Loss: 0.4162\n",
      "Epoch(150) - Fold 0 - Validation Loss : 0.4987\n",
      "Epoch(150) - Fold 1 - Validation Loss : 0.4086\n",
      "Epoch(150) - Fold 2 - Validation Loss : 0.4357\n",
      "Epoch(150) - Fold 3 - Validation Loss : 0.4511\n",
      "Epoch(150) - Fold 4 - Validation Loss : 0.3555\n",
      "Epoch(150) - GLOBAL - Validation Loss: 0.4299\n",
      "Intermediate early stopping : vepoch_loss = 0.4299, the_last_loss=0.4283\n",
      "\n",
      "\n",
      "Epoch(151) - Training Loss: 0.4178\n",
      "Epoch(151) - Fold 0 - Validation Loss : 0.4946\n",
      "Epoch(151) - Fold 1 - Validation Loss : 0.4053\n",
      "Epoch(151) - Fold 2 - Validation Loss : 0.4324\n",
      "Epoch(151) - Fold 3 - Validation Loss : 0.4481\n",
      "Epoch(151) - Fold 4 - Validation Loss : 0.3529\n",
      "Epoch(151) - GLOBAL - Validation Loss: 0.4266\n",
      "Saving model corresponding to last_loss == 0.4266386155166522\n",
      "\n",
      "\n",
      "Epoch(152) - Training Loss: 0.4146\n",
      "Epoch(152) - Fold 0 - Validation Loss : 0.4943\n",
      "Epoch(152) - Fold 1 - Validation Loss : 0.4047\n",
      "Epoch(152) - Fold 2 - Validation Loss : 0.4318\n",
      "Epoch(152) - Fold 3 - Validation Loss : 0.4472\n",
      "Epoch(152) - Fold 4 - Validation Loss : 0.3520\n",
      "Epoch(152) - GLOBAL - Validation Loss: 0.4260\n",
      "Saving model corresponding to last_loss == 0.42598999235305657\n",
      "\n",
      "\n",
      "Epoch(153) - Training Loss: 0.4138\n",
      "Epoch(153) - Fold 0 - Validation Loss : 0.4923\n",
      "Epoch(153) - Fold 1 - Validation Loss : 0.4033\n",
      "Epoch(153) - Fold 2 - Validation Loss : 0.4305\n",
      "Epoch(153) - Fold 3 - Validation Loss : 0.4460\n",
      "Epoch(153) - Fold 4 - Validation Loss : 0.3508\n",
      "Epoch(153) - GLOBAL - Validation Loss: 0.4246\n",
      "Saving model corresponding to last_loss == 0.424583626916674\n",
      "\n",
      "\n",
      "Epoch(154) - Training Loss: 0.4124\n",
      "Epoch(154) - Fold 0 - Validation Loss : 0.4927\n",
      "Epoch(154) - Fold 1 - Validation Loss : 0.4036\n",
      "Epoch(154) - Fold 2 - Validation Loss : 0.4307\n",
      "Epoch(154) - Fold 3 - Validation Loss : 0.4464\n",
      "Epoch(154) - Fold 4 - Validation Loss : 0.3511\n",
      "Epoch(154) - GLOBAL - Validation Loss: 0.4249\n",
      "Intermediate early stopping : vepoch_loss = 0.4249, the_last_loss=0.4246\n",
      "\n",
      "\n",
      "Epoch(155) - Training Loss: 0.4134\n",
      "Epoch(155) - Fold 0 - Validation Loss : 0.4918\n",
      "Epoch(155) - Fold 1 - Validation Loss : 0.4030\n",
      "Epoch(155) - Fold 2 - Validation Loss : 0.4300\n",
      "Epoch(155) - Fold 3 - Validation Loss : 0.4451\n",
      "Epoch(155) - Fold 4 - Validation Loss : 0.3503\n",
      "Epoch(155) - GLOBAL - Validation Loss: 0.4240\n",
      "Saving model corresponding to last_loss == 0.42403031482813736\n",
      "\n",
      "\n",
      "Epoch(156) - Training Loss: 0.4120\n",
      "Epoch(156) - Fold 0 - Validation Loss : 0.4899\n",
      "Epoch(156) - Fold 1 - Validation Loss : 0.4014\n",
      "Epoch(156) - Fold 2 - Validation Loss : 0.4285\n",
      "Epoch(156) - Fold 3 - Validation Loss : 0.4440\n",
      "Epoch(156) - Fold 4 - Validation Loss : 0.3492\n",
      "Epoch(156) - GLOBAL - Validation Loss: 0.4226\n",
      "Saving model corresponding to last_loss == 0.4225815389420952\n",
      "\n",
      "\n",
      "Epoch(157) - Training Loss: 0.4116\n",
      "Epoch(157) - Fold 0 - Validation Loss : 0.4898\n",
      "Epoch(157) - Fold 1 - Validation Loss : 0.4015\n",
      "Epoch(157) - Fold 2 - Validation Loss : 0.4288\n",
      "Epoch(157) - Fold 3 - Validation Loss : 0.4437\n",
      "Epoch(157) - Fold 4 - Validation Loss : 0.3491\n",
      "Epoch(157) - GLOBAL - Validation Loss: 0.4226\n",
      "Saving model corresponding to last_loss == 0.42257375049327833\n",
      "\n",
      "\n",
      "Epoch(158) - Training Loss: 0.4111\n",
      "Epoch(158) - Fold 0 - Validation Loss : 0.4896\n",
      "Epoch(158) - Fold 1 - Validation Loss : 0.4011\n",
      "Epoch(158) - Fold 2 - Validation Loss : 0.4286\n",
      "Epoch(158) - Fold 3 - Validation Loss : 0.4435\n",
      "Epoch(158) - Fold 4 - Validation Loss : 0.3489\n",
      "Epoch(158) - GLOBAL - Validation Loss: 0.4223\n",
      "Saving model corresponding to last_loss == 0.4223449901791473\n",
      "\n",
      "\n",
      "Epoch(159) - Training Loss: 0.4109\n",
      "Epoch(159) - Fold 0 - Validation Loss : 0.4889\n",
      "Epoch(159) - Fold 1 - Validation Loss : 0.4006\n",
      "Epoch(159) - Fold 2 - Validation Loss : 0.4279\n",
      "Epoch(159) - Fold 3 - Validation Loss : 0.4432\n",
      "Epoch(159) - Fold 4 - Validation Loss : 0.3485\n",
      "Epoch(159) - GLOBAL - Validation Loss: 0.4218\n",
      "Saving model corresponding to last_loss == 0.4218280142330598\n",
      "\n",
      "\n",
      "Epoch(160) - Training Loss: 0.4094\n",
      "Epoch(160) - Fold 0 - Validation Loss : 0.4883\n",
      "Epoch(160) - Fold 1 - Validation Loss : 0.4002\n",
      "Epoch(160) - Fold 2 - Validation Loss : 0.4274\n",
      "Epoch(160) - Fold 3 - Validation Loss : 0.4418\n",
      "Epoch(160) - Fold 4 - Validation Loss : 0.3479\n",
      "Epoch(160) - GLOBAL - Validation Loss: 0.4211\n",
      "Saving model corresponding to last_loss == 0.4211393348014661\n",
      "\n",
      "\n",
      "Epoch(161) - Training Loss: 0.4089\n",
      "Epoch(161) - Fold 0 - Validation Loss : 0.4871\n",
      "Epoch(161) - Fold 1 - Validation Loss : 0.3991\n",
      "Epoch(161) - Fold 2 - Validation Loss : 0.4260\n",
      "Epoch(161) - Fold 3 - Validation Loss : 0.4407\n",
      "Epoch(161) - Fold 4 - Validation Loss : 0.3467\n",
      "Epoch(161) - GLOBAL - Validation Loss: 0.4199\n",
      "Saving model corresponding to last_loss == 0.41989522458702827\n",
      "\n",
      "\n",
      "Epoch(162) - Training Loss: 0.4087\n",
      "Epoch(162) - Fold 0 - Validation Loss : 0.4868\n",
      "Epoch(162) - Fold 1 - Validation Loss : 0.3988\n",
      "Epoch(162) - Fold 2 - Validation Loss : 0.4260\n",
      "Epoch(162) - Fold 3 - Validation Loss : 0.4408\n",
      "Epoch(162) - Fold 4 - Validation Loss : 0.3470\n",
      "Epoch(162) - GLOBAL - Validation Loss: 0.4199\n",
      "Saving model corresponding to last_loss == 0.41989161391228985\n",
      "\n",
      "\n",
      "Epoch(163) - Training Loss: 0.4081\n",
      "Epoch(163) - Fold 0 - Validation Loss : 0.4859\n",
      "Epoch(163) - Fold 1 - Validation Loss : 0.3980\n",
      "Epoch(163) - Fold 2 - Validation Loss : 0.4250\n",
      "Epoch(163) - Fold 3 - Validation Loss : 0.4396\n",
      "Epoch(163) - Fold 4 - Validation Loss : 0.3459\n",
      "Epoch(163) - GLOBAL - Validation Loss: 0.4189\n",
      "Saving model corresponding to last_loss == 0.41886067533229554\n",
      "\n",
      "\n",
      "Epoch(164) - Training Loss: 0.4069\n",
      "Epoch(164) - Fold 0 - Validation Loss : 0.4871\n",
      "Epoch(164) - Fold 1 - Validation Loss : 0.3981\n",
      "Epoch(164) - Fold 2 - Validation Loss : 0.4251\n",
      "Epoch(164) - Fold 3 - Validation Loss : 0.4398\n",
      "Epoch(164) - Fold 4 - Validation Loss : 0.3463\n",
      "Epoch(164) - GLOBAL - Validation Loss: 0.4193\n",
      "Intermediate early stopping : vepoch_loss = 0.4193, the_last_loss=0.4189\n",
      "\n",
      "\n",
      "Epoch(165) - Training Loss: 0.4104\n",
      "Epoch(165) - Fold 0 - Validation Loss : 0.4866\n",
      "Epoch(165) - Fold 1 - Validation Loss : 0.3976\n",
      "Epoch(165) - Fold 2 - Validation Loss : 0.4246\n",
      "Epoch(165) - Fold 3 - Validation Loss : 0.4394\n",
      "Epoch(165) - Fold 4 - Validation Loss : 0.3455\n",
      "Epoch(165) - GLOBAL - Validation Loss: 0.4187\n",
      "Saving model corresponding to last_loss == 0.4187361475344595\n",
      "\n",
      "\n",
      "Epoch(166) - Training Loss: 0.4105\n",
      "Epoch(166) - Fold 0 - Validation Loss : 0.5006\n",
      "Epoch(166) - Fold 1 - Validation Loss : 0.4081\n",
      "Epoch(166) - Fold 2 - Validation Loss : 0.4345\n",
      "Epoch(166) - Fold 3 - Validation Loss : 0.4479\n",
      "Epoch(166) - Fold 4 - Validation Loss : 0.3539\n",
      "Epoch(166) - GLOBAL - Validation Loss: 0.4290\n",
      "Intermediate early stopping : vepoch_loss = 0.4290, the_last_loss=0.4187\n",
      "\n",
      "\n",
      "Epoch(167) - Training Loss: 0.4156\n",
      "Epoch(167) - Fold 0 - Validation Loss : 0.4951\n",
      "Epoch(167) - Fold 1 - Validation Loss : 0.4041\n",
      "Epoch(167) - Fold 2 - Validation Loss : 0.4316\n",
      "Epoch(167) - Fold 3 - Validation Loss : 0.4454\n",
      "Epoch(167) - Fold 4 - Validation Loss : 0.3513\n",
      "Epoch(167) - GLOBAL - Validation Loss: 0.4255\n",
      "Intermediate early stopping : vepoch_loss = 0.4255, the_last_loss=0.4187\n",
      "\n",
      "\n",
      "Epoch(168) - Training Loss: 0.4086\n",
      "Epoch(168) - Fold 0 - Validation Loss : 0.4849\n",
      "Epoch(168) - Fold 1 - Validation Loss : 0.3966\n",
      "Epoch(168) - Fold 2 - Validation Loss : 0.4234\n",
      "Epoch(168) - Fold 3 - Validation Loss : 0.4381\n",
      "Epoch(168) - Fold 4 - Validation Loss : 0.3448\n",
      "Epoch(168) - GLOBAL - Validation Loss: 0.4175\n",
      "Saving model corresponding to last_loss == 0.4175431791609222\n",
      "\n",
      "\n",
      "Epoch(169) - Training Loss: 0.4053\n",
      "Epoch(169) - Fold 0 - Validation Loss : 0.4818\n",
      "Epoch(169) - Fold 1 - Validation Loss : 0.3942\n",
      "Epoch(169) - Fold 2 - Validation Loss : 0.4210\n",
      "Epoch(169) - Fold 3 - Validation Loss : 0.4355\n",
      "Epoch(169) - Fold 4 - Validation Loss : 0.3427\n",
      "Epoch(169) - GLOBAL - Validation Loss: 0.4150\n",
      "Saving model corresponding to last_loss == 0.4150216100300835\n",
      "\n",
      "\n",
      "Epoch(170) - Training Loss: 0.4042\n",
      "Epoch(170) - Fold 0 - Validation Loss : 0.4814\n",
      "Epoch(170) - Fold 1 - Validation Loss : 0.3943\n",
      "Epoch(170) - Fold 2 - Validation Loss : 0.4210\n",
      "Epoch(170) - Fold 3 - Validation Loss : 0.4351\n",
      "Epoch(170) - Fold 4 - Validation Loss : 0.3422\n",
      "Epoch(170) - GLOBAL - Validation Loss: 0.4148\n",
      "Saving model corresponding to last_loss == 0.4148102317811892\n",
      "\n",
      "\n",
      "Epoch(171) - Training Loss: 0.4038\n",
      "Epoch(171) - Fold 0 - Validation Loss : 0.4813\n",
      "Epoch(171) - Fold 1 - Validation Loss : 0.3934\n",
      "Epoch(171) - Fold 2 - Validation Loss : 0.4200\n",
      "Epoch(171) - Fold 3 - Validation Loss : 0.4352\n",
      "Epoch(171) - Fold 4 - Validation Loss : 0.3418\n",
      "Epoch(171) - GLOBAL - Validation Loss: 0.4143\n",
      "Saving model corresponding to last_loss == 0.41434478185693707\n",
      "\n",
      "\n",
      "Epoch(172) - Training Loss: 0.4033\n",
      "Epoch(172) - Fold 0 - Validation Loss : 0.4810\n",
      "Epoch(172) - Fold 1 - Validation Loss : 0.3930\n",
      "Epoch(172) - Fold 2 - Validation Loss : 0.4194\n",
      "Epoch(172) - Fold 3 - Validation Loss : 0.4339\n",
      "Epoch(172) - Fold 4 - Validation Loss : 0.3410\n",
      "Epoch(172) - GLOBAL - Validation Loss: 0.4136\n",
      "Saving model corresponding to last_loss == 0.41363124255113687\n",
      "\n",
      "\n",
      "Epoch(173) - Training Loss: 0.4027\n",
      "Epoch(173) - Fold 0 - Validation Loss : 0.4802\n",
      "Epoch(173) - Fold 1 - Validation Loss : 0.3924\n",
      "Epoch(173) - Fold 2 - Validation Loss : 0.4189\n",
      "Epoch(173) - Fold 3 - Validation Loss : 0.4336\n",
      "Epoch(173) - Fold 4 - Validation Loss : 0.3410\n",
      "Epoch(173) - GLOBAL - Validation Loss: 0.4132\n",
      "Saving model corresponding to last_loss == 0.41322045393924006\n",
      "\n",
      "\n",
      "Epoch(174) - Training Loss: 0.4021\n",
      "Epoch(174) - Fold 0 - Validation Loss : 0.4798\n",
      "Epoch(174) - Fold 1 - Validation Loss : 0.3920\n",
      "Epoch(174) - Fold 2 - Validation Loss : 0.4181\n",
      "Epoch(174) - Fold 3 - Validation Loss : 0.4328\n",
      "Epoch(174) - Fold 4 - Validation Loss : 0.3401\n",
      "Epoch(174) - GLOBAL - Validation Loss: 0.4126\n",
      "Saving model corresponding to last_loss == 0.41256322195784734\n",
      "\n",
      "\n",
      "Epoch(175) - Training Loss: 0.4015\n",
      "Epoch(175) - Fold 0 - Validation Loss : 0.4788\n",
      "Epoch(175) - Fold 1 - Validation Loss : 0.3910\n",
      "Epoch(175) - Fold 2 - Validation Loss : 0.4177\n",
      "Epoch(175) - Fold 3 - Validation Loss : 0.4321\n",
      "Epoch(175) - Fold 4 - Validation Loss : 0.3396\n",
      "Epoch(175) - GLOBAL - Validation Loss: 0.4118\n",
      "Saving model corresponding to last_loss == 0.41184431956735024\n",
      "\n",
      "\n",
      "Epoch(176) - Training Loss: 0.4009\n",
      "Epoch(176) - Fold 0 - Validation Loss : 0.4779\n",
      "Epoch(176) - Fold 1 - Validation Loss : 0.3905\n",
      "Epoch(176) - Fold 2 - Validation Loss : 0.4171\n",
      "Epoch(176) - Fold 3 - Validation Loss : 0.4316\n",
      "Epoch(176) - Fold 4 - Validation Loss : 0.3394\n",
      "Epoch(176) - GLOBAL - Validation Loss: 0.4113\n",
      "Saving model corresponding to last_loss == 0.4112656096877904\n",
      "\n",
      "\n",
      "Epoch(177) - Training Loss: 0.4007\n",
      "Epoch(177) - Fold 0 - Validation Loss : 0.4778\n",
      "Epoch(177) - Fold 1 - Validation Loss : 0.3905\n",
      "Epoch(177) - Fold 2 - Validation Loss : 0.4169\n",
      "Epoch(177) - Fold 3 - Validation Loss : 0.4305\n",
      "Epoch(177) - Fold 4 - Validation Loss : 0.3387\n",
      "Epoch(177) - GLOBAL - Validation Loss: 0.4109\n",
      "Saving model corresponding to last_loss == 0.41090238874534846\n",
      "\n",
      "\n",
      "Epoch(178) - Training Loss: 0.4000\n",
      "Epoch(178) - Fold 0 - Validation Loss : 0.4767\n",
      "Epoch(178) - Fold 1 - Validation Loss : 0.3891\n",
      "Epoch(178) - Fold 2 - Validation Loss : 0.4156\n",
      "Epoch(178) - Fold 3 - Validation Loss : 0.4302\n",
      "Epoch(178) - Fold 4 - Validation Loss : 0.3382\n",
      "Epoch(178) - GLOBAL - Validation Loss: 0.4100\n",
      "Saving model corresponding to last_loss == 0.409972464018664\n",
      "\n",
      "\n",
      "Epoch(179) - Training Loss: 0.3993\n",
      "Epoch(179) - Fold 0 - Validation Loss : 0.4774\n",
      "Epoch(179) - Fold 1 - Validation Loss : 0.3895\n",
      "Epoch(179) - Fold 2 - Validation Loss : 0.4158\n",
      "Epoch(179) - Fold 3 - Validation Loss : 0.4302\n",
      "Epoch(179) - Fold 4 - Validation Loss : 0.3380\n",
      "Epoch(179) - GLOBAL - Validation Loss: 0.4102\n",
      "Intermediate early stopping : vepoch_loss = 0.4102, the_last_loss=0.4100\n",
      "\n",
      "\n",
      "Epoch(180) - Training Loss: 0.3992\n",
      "Epoch(180) - Fold 0 - Validation Loss : 0.4759\n",
      "Epoch(180) - Fold 1 - Validation Loss : 0.3887\n",
      "Epoch(180) - Fold 2 - Validation Loss : 0.4151\n",
      "Epoch(180) - Fold 3 - Validation Loss : 0.4290\n",
      "Epoch(180) - Fold 4 - Validation Loss : 0.3372\n",
      "Epoch(180) - GLOBAL - Validation Loss: 0.4092\n",
      "Saving model corresponding to last_loss == 0.4091643691374517\n",
      "\n",
      "\n",
      "Epoch(181) - Training Loss: 0.3986\n",
      "Epoch(181) - Fold 0 - Validation Loss : 0.4747\n",
      "Epoch(181) - Fold 1 - Validation Loss : 0.3879\n",
      "Epoch(181) - Fold 2 - Validation Loss : 0.4144\n",
      "Epoch(181) - Fold 3 - Validation Loss : 0.4287\n",
      "Epoch(181) - Fold 4 - Validation Loss : 0.3366\n",
      "Epoch(181) - GLOBAL - Validation Loss: 0.4085\n",
      "Saving model corresponding to last_loss == 0.4084842900499804\n",
      "\n",
      "\n",
      "Epoch(182) - Training Loss: 0.3981\n",
      "Epoch(182) - Fold 0 - Validation Loss : 0.4759\n",
      "Epoch(182) - Fold 1 - Validation Loss : 0.3886\n",
      "Epoch(182) - Fold 2 - Validation Loss : 0.4149\n",
      "Epoch(182) - Fold 3 - Validation Loss : 0.4289\n",
      "Epoch(182) - Fold 4 - Validation Loss : 0.3374\n",
      "Epoch(182) - GLOBAL - Validation Loss: 0.4092\n",
      "Intermediate early stopping : vepoch_loss = 0.4092, the_last_loss=0.4085\n",
      "\n",
      "\n",
      "Epoch(183) - Training Loss: 0.3989\n",
      "Epoch(183) - Fold 0 - Validation Loss : 0.4758\n",
      "Epoch(183) - Fold 1 - Validation Loss : 0.3883\n",
      "Epoch(183) - Fold 2 - Validation Loss : 0.4148\n",
      "Epoch(183) - Fold 3 - Validation Loss : 0.4285\n",
      "Epoch(183) - Fold 4 - Validation Loss : 0.3368\n",
      "Epoch(183) - GLOBAL - Validation Loss: 0.4088\n",
      "Intermediate early stopping : vepoch_loss = 0.4088, the_last_loss=0.4085\n",
      "\n",
      "\n",
      "Epoch(184) - Training Loss: 0.3997\n",
      "Epoch(184) - Fold 0 - Validation Loss : 0.4740\n",
      "Epoch(184) - Fold 1 - Validation Loss : 0.3870\n",
      "Epoch(184) - Fold 2 - Validation Loss : 0.4136\n",
      "Epoch(184) - Fold 3 - Validation Loss : 0.4275\n",
      "Epoch(184) - Fold 4 - Validation Loss : 0.3360\n",
      "Epoch(184) - GLOBAL - Validation Loss: 0.4076\n",
      "Saving model corresponding to last_loss == 0.40760406994944975\n",
      "\n",
      "\n",
      "Epoch(185) - Training Loss: 0.3991\n",
      "Epoch(185) - Fold 0 - Validation Loss : 0.4775\n",
      "Epoch(185) - Fold 1 - Validation Loss : 0.3898\n",
      "Epoch(185) - Fold 2 - Validation Loss : 0.4165\n",
      "Epoch(185) - Fold 3 - Validation Loss : 0.4301\n",
      "Epoch(185) - Fold 4 - Validation Loss : 0.3385\n",
      "Epoch(185) - GLOBAL - Validation Loss: 0.4105\n",
      "Intermediate early stopping : vepoch_loss = 0.4105, the_last_loss=0.4076\n",
      "\n",
      "\n",
      "Epoch(186) - Training Loss: 0.3996\n",
      "Epoch(186) - Fold 0 - Validation Loss : 0.4800\n",
      "Epoch(186) - Fold 1 - Validation Loss : 0.3914\n",
      "Epoch(186) - Fold 2 - Validation Loss : 0.4176\n",
      "Epoch(186) - Fold 3 - Validation Loss : 0.4311\n",
      "Epoch(186) - Fold 4 - Validation Loss : 0.3398\n",
      "Epoch(186) - GLOBAL - Validation Loss: 0.4120\n",
      "Intermediate early stopping : vepoch_loss = 0.4120, the_last_loss=0.4076\n",
      "\n",
      "\n",
      "Epoch(187) - Training Loss: 0.4028\n",
      "Epoch(187) - Fold 0 - Validation Loss : 0.4764\n",
      "Epoch(187) - Fold 1 - Validation Loss : 0.3887\n",
      "Epoch(187) - Fold 2 - Validation Loss : 0.4152\n",
      "Epoch(187) - Fold 3 - Validation Loss : 0.4285\n",
      "Epoch(187) - Fold 4 - Validation Loss : 0.3373\n",
      "Epoch(187) - GLOBAL - Validation Loss: 0.4093\n",
      "Intermediate early stopping : vepoch_loss = 0.4093, the_last_loss=0.4076\n",
      "\n",
      "\n",
      "Epoch(188) - Training Loss: 0.3975\n",
      "Epoch(188) - Fold 0 - Validation Loss : 0.4736\n",
      "Epoch(188) - Fold 1 - Validation Loss : 0.3864\n",
      "Epoch(188) - Fold 2 - Validation Loss : 0.4131\n",
      "Epoch(188) - Fold 3 - Validation Loss : 0.4265\n",
      "Epoch(188) - Fold 4 - Validation Loss : 0.3356\n",
      "Epoch(188) - GLOBAL - Validation Loss: 0.4071\n",
      "Saving model corresponding to last_loss == 0.407052042381169\n",
      "\n",
      "\n",
      "Epoch(189) - Training Loss: 0.3963\n",
      "Epoch(189) - Fold 0 - Validation Loss : 0.4717\n",
      "Epoch(189) - Fold 1 - Validation Loss : 0.3847\n",
      "Epoch(189) - Fold 2 - Validation Loss : 0.4111\n",
      "Epoch(189) - Fold 3 - Validation Loss : 0.4252\n",
      "Epoch(189) - Fold 4 - Validation Loss : 0.3339\n",
      "Epoch(189) - GLOBAL - Validation Loss: 0.4053\n",
      "Saving model corresponding to last_loss == 0.4053103837216554\n",
      "\n",
      "\n",
      "Epoch(190) - Training Loss: 0.3953\n",
      "Epoch(190) - Fold 0 - Validation Loss : 0.4723\n",
      "Epoch(190) - Fold 1 - Validation Loss : 0.3850\n",
      "Epoch(190) - Fold 2 - Validation Loss : 0.4114\n",
      "Epoch(190) - Fold 3 - Validation Loss : 0.4252\n",
      "Epoch(190) - Fold 4 - Validation Loss : 0.3344\n",
      "Epoch(190) - GLOBAL - Validation Loss: 0.4057\n",
      "Intermediate early stopping : vepoch_loss = 0.4057, the_last_loss=0.4053\n",
      "\n",
      "\n",
      "Epoch(191) - Training Loss: 0.3957\n",
      "Epoch(191) - Fold 0 - Validation Loss : 0.4703\n",
      "Epoch(191) - Fold 1 - Validation Loss : 0.3837\n",
      "Epoch(191) - Fold 2 - Validation Loss : 0.4103\n",
      "Epoch(191) - Fold 3 - Validation Loss : 0.4241\n",
      "Epoch(191) - Fold 4 - Validation Loss : 0.3333\n",
      "Epoch(191) - GLOBAL - Validation Loss: 0.4043\n",
      "Saving model corresponding to last_loss == 0.4043459460334621\n",
      "\n",
      "\n",
      "Epoch(192) - Training Loss: 0.3948\n",
      "Epoch(192) - Fold 0 - Validation Loss : 0.4702\n",
      "Epoch(192) - Fold 1 - Validation Loss : 0.3832\n",
      "Epoch(192) - Fold 2 - Validation Loss : 0.4100\n",
      "Epoch(192) - Fold 3 - Validation Loss : 0.4241\n",
      "Epoch(192) - Fold 4 - Validation Loss : 0.3331\n",
      "Epoch(192) - GLOBAL - Validation Loss: 0.4041\n",
      "Saving model corresponding to last_loss == 0.4041168661997202\n",
      "\n",
      "\n",
      "Epoch(193) - Training Loss: 0.3939\n",
      "Epoch(193) - Fold 0 - Validation Loss : 0.4695\n",
      "Epoch(193) - Fold 1 - Validation Loss : 0.3830\n",
      "Epoch(193) - Fold 2 - Validation Loss : 0.4096\n",
      "Epoch(193) - Fold 3 - Validation Loss : 0.4233\n",
      "Epoch(193) - Fold 4 - Validation Loss : 0.3327\n",
      "Epoch(193) - GLOBAL - Validation Loss: 0.4036\n",
      "Saving model corresponding to last_loss == 0.4036435035288838\n",
      "\n",
      "\n",
      "Epoch(194) - Training Loss: 0.3936\n",
      "Epoch(194) - Fold 0 - Validation Loss : 0.4694\n",
      "Epoch(194) - Fold 1 - Validation Loss : 0.3830\n",
      "Epoch(194) - Fold 2 - Validation Loss : 0.4097\n",
      "Epoch(194) - Fold 3 - Validation Loss : 0.4230\n",
      "Epoch(194) - Fold 4 - Validation Loss : 0.3326\n",
      "Epoch(194) - GLOBAL - Validation Loss: 0.4035\n",
      "Saving model corresponding to last_loss == 0.40354138703641274\n",
      "\n",
      "\n",
      "Epoch(195) - Training Loss: 0.3935\n",
      "Epoch(195) - Fold 0 - Validation Loss : 0.4688\n",
      "Epoch(195) - Fold 1 - Validation Loss : 0.3825\n",
      "Epoch(195) - Fold 2 - Validation Loss : 0.4093\n",
      "Epoch(195) - Fold 3 - Validation Loss : 0.4227\n",
      "Epoch(195) - Fold 4 - Validation Loss : 0.3323\n",
      "Epoch(195) - GLOBAL - Validation Loss: 0.4031\n",
      "Saving model corresponding to last_loss == 0.4030910588999278\n",
      "\n",
      "\n",
      "Epoch(196) - Training Loss: 0.3931\n",
      "Epoch(196) - Fold 0 - Validation Loss : 0.4687\n",
      "Epoch(196) - Fold 1 - Validation Loss : 0.3821\n",
      "Epoch(196) - Fold 2 - Validation Loss : 0.4090\n",
      "Epoch(196) - Fold 3 - Validation Loss : 0.4219\n",
      "Epoch(196) - Fold 4 - Validation Loss : 0.3317\n",
      "Epoch(196) - GLOBAL - Validation Loss: 0.4027\n",
      "Saving model corresponding to last_loss == 0.4026995921980693\n",
      "\n",
      "\n",
      "Epoch(197) - Training Loss: 0.3926\n",
      "Epoch(197) - Fold 0 - Validation Loss : 0.4684\n",
      "Epoch(197) - Fold 1 - Validation Loss : 0.3816\n",
      "Epoch(197) - Fold 2 - Validation Loss : 0.4083\n",
      "Epoch(197) - Fold 3 - Validation Loss : 0.4217\n",
      "Epoch(197) - Fold 4 - Validation Loss : 0.3315\n",
      "Epoch(197) - GLOBAL - Validation Loss: 0.4023\n",
      "Saving model corresponding to last_loss == 0.402313882824597\n",
      "\n",
      "\n",
      "Epoch(198) - Training Loss: 0.3924\n",
      "Epoch(198) - Fold 0 - Validation Loss : 0.4675\n",
      "Epoch(198) - Fold 1 - Validation Loss : 0.3811\n",
      "Epoch(198) - Fold 2 - Validation Loss : 0.4077\n",
      "Epoch(198) - Fold 3 - Validation Loss : 0.4211\n",
      "Epoch(198) - Fold 4 - Validation Loss : 0.3310\n",
      "Epoch(198) - GLOBAL - Validation Loss: 0.4017\n",
      "Saving model corresponding to last_loss == 0.40168676781619217\n",
      "\n",
      "\n",
      "Epoch(199) - Training Loss: 0.3926\n",
      "Epoch(199) - Fold 0 - Validation Loss : 0.4673\n",
      "Epoch(199) - Fold 1 - Validation Loss : 0.3810\n",
      "Epoch(199) - Fold 2 - Validation Loss : 0.4077\n",
      "Epoch(199) - Fold 3 - Validation Loss : 0.4210\n",
      "Epoch(199) - Fold 4 - Validation Loss : 0.3308\n",
      "Epoch(199) - GLOBAL - Validation Loss: 0.4016\n",
      "Saving model corresponding to last_loss == 0.40155720578799636\n",
      "\n",
      "\n",
      "Epoch(200) - Training Loss: 0.3918\n",
      "Epoch(200) - Fold 0 - Validation Loss : 0.4678\n",
      "Epoch(200) - Fold 1 - Validation Loss : 0.3809\n",
      "Epoch(200) - Fold 2 - Validation Loss : 0.4075\n",
      "Epoch(200) - Fold 3 - Validation Loss : 0.4208\n",
      "Epoch(200) - Fold 4 - Validation Loss : 0.3305\n",
      "Epoch(200) - GLOBAL - Validation Loss: 0.4015\n",
      "Saving model corresponding to last_loss == 0.40151838252251276\n",
      "\n",
      "\n",
      "Epoch(201) - Training Loss: 0.3915\n",
      "Epoch(201) - Fold 0 - Validation Loss : 0.4675\n",
      "Epoch(201) - Fold 1 - Validation Loss : 0.3808\n",
      "Epoch(201) - Fold 2 - Validation Loss : 0.4075\n",
      "Epoch(201) - Fold 3 - Validation Loss : 0.4206\n",
      "Epoch(201) - Fold 4 - Validation Loss : 0.3306\n",
      "Epoch(201) - GLOBAL - Validation Loss: 0.4014\n",
      "Saving model corresponding to last_loss == 0.40138914059481856\n",
      "\n",
      "\n",
      "Epoch(202) - Training Loss: 0.3920\n",
      "Epoch(202) - Fold 0 - Validation Loss : 0.4688\n",
      "Epoch(202) - Fold 1 - Validation Loss : 0.3819\n",
      "Epoch(202) - Fold 2 - Validation Loss : 0.4087\n",
      "Epoch(202) - Fold 3 - Validation Loss : 0.4217\n",
      "Epoch(202) - Fold 4 - Validation Loss : 0.3315\n",
      "Epoch(202) - GLOBAL - Validation Loss: 0.4025\n",
      "Intermediate early stopping : vepoch_loss = 0.4025, the_last_loss=0.4014\n",
      "\n",
      "\n",
      "Epoch(203) - Training Loss: 0.3916\n",
      "Epoch(203) - Fold 0 - Validation Loss : 0.4683\n",
      "Epoch(203) - Fold 1 - Validation Loss : 0.3813\n",
      "Epoch(203) - Fold 2 - Validation Loss : 0.4080\n",
      "Epoch(203) - Fold 3 - Validation Loss : 0.4211\n",
      "Epoch(203) - Fold 4 - Validation Loss : 0.3312\n",
      "Epoch(203) - GLOBAL - Validation Loss: 0.4020\n",
      "Intermediate early stopping : vepoch_loss = 0.4020, the_last_loss=0.4014\n",
      "\n",
      "\n",
      "Epoch(204) - Training Loss: 0.3948\n",
      "Epoch(204) - Fold 0 - Validation Loss : 0.4667\n",
      "Epoch(204) - Fold 1 - Validation Loss : 0.3796\n",
      "Epoch(204) - Fold 2 - Validation Loss : 0.4062\n",
      "Epoch(204) - Fold 3 - Validation Loss : 0.4192\n",
      "Epoch(204) - Fold 4 - Validation Loss : 0.3293\n",
      "Epoch(204) - GLOBAL - Validation Loss: 0.4002\n",
      "Saving model corresponding to last_loss == 0.40019330601855796\n",
      "\n",
      "\n",
      "Epoch(205) - Training Loss: 0.3941\n",
      "Epoch(205) - Fold 0 - Validation Loss : 0.4658\n",
      "Epoch(205) - Fold 1 - Validation Loss : 0.3794\n",
      "Epoch(205) - Fold 2 - Validation Loss : 0.4060\n",
      "Epoch(205) - Fold 3 - Validation Loss : 0.4195\n",
      "Epoch(205) - Fold 4 - Validation Loss : 0.3295\n",
      "Epoch(205) - GLOBAL - Validation Loss: 0.4000\n",
      "Saving model corresponding to last_loss == 0.4000490773335115\n",
      "\n",
      "\n",
      "Epoch(206) - Training Loss: 0.3922\n",
      "Epoch(206) - Fold 0 - Validation Loss : 0.4696\n",
      "Epoch(206) - Fold 1 - Validation Loss : 0.3819\n",
      "Epoch(206) - Fold 2 - Validation Loss : 0.4088\n",
      "Epoch(206) - Fold 3 - Validation Loss : 0.4218\n",
      "Epoch(206) - Fold 4 - Validation Loss : 0.3317\n",
      "Epoch(206) - GLOBAL - Validation Loss: 0.4027\n",
      "Intermediate early stopping : vepoch_loss = 0.4027, the_last_loss=0.4000\n",
      "\n",
      "\n",
      "Epoch(207) - Training Loss: 0.3938\n",
      "Epoch(207) - Fold 0 - Validation Loss : 0.4697\n",
      "Epoch(207) - Fold 1 - Validation Loss : 0.3822\n",
      "Epoch(207) - Fold 2 - Validation Loss : 0.4088\n",
      "Epoch(207) - Fold 3 - Validation Loss : 0.4217\n",
      "Epoch(207) - Fold 4 - Validation Loss : 0.3320\n",
      "Epoch(207) - GLOBAL - Validation Loss: 0.4029\n",
      "Intermediate early stopping : vepoch_loss = 0.4029, the_last_loss=0.4000\n",
      "\n",
      "\n",
      "Epoch(208) - Training Loss: 0.3924\n",
      "Epoch(208) - Fold 0 - Validation Loss : 0.4681\n",
      "Epoch(208) - Fold 1 - Validation Loss : 0.3807\n",
      "Epoch(208) - Fold 2 - Validation Loss : 0.4075\n",
      "Epoch(208) - Fold 3 - Validation Loss : 0.4203\n",
      "Epoch(208) - Fold 4 - Validation Loss : 0.3305\n",
      "Epoch(208) - GLOBAL - Validation Loss: 0.4014\n",
      "Intermediate early stopping : vepoch_loss = 0.4014, the_last_loss=0.4000\n",
      "\n",
      "\n",
      "Epoch(209) - Training Loss: 0.3906\n",
      "Epoch(209) - Fold 0 - Validation Loss : 0.4649\n",
      "Epoch(209) - Fold 1 - Validation Loss : 0.3785\n",
      "Epoch(209) - Fold 2 - Validation Loss : 0.4055\n",
      "Epoch(209) - Fold 3 - Validation Loss : 0.4184\n",
      "Epoch(209) - Fold 4 - Validation Loss : 0.3290\n",
      "Epoch(209) - GLOBAL - Validation Loss: 0.3993\n",
      "Saving model corresponding to last_loss == 0.39926201300310693\n",
      "\n",
      "\n",
      "Epoch(210) - Training Loss: 0.3896\n",
      "Epoch(210) - Fold 0 - Validation Loss : 0.4644\n",
      "Epoch(210) - Fold 1 - Validation Loss : 0.3781\n",
      "Epoch(210) - Fold 2 - Validation Loss : 0.4053\n",
      "Epoch(210) - Fold 3 - Validation Loss : 0.4182\n",
      "Epoch(210) - Fold 4 - Validation Loss : 0.3289\n",
      "Epoch(210) - GLOBAL - Validation Loss: 0.3990\n",
      "Saving model corresponding to last_loss == 0.39897843228620056\n",
      "\n",
      "\n",
      "Epoch(211) - Training Loss: 0.3891\n",
      "Epoch(211) - Fold 0 - Validation Loss : 0.4646\n",
      "Epoch(211) - Fold 1 - Validation Loss : 0.3777\n",
      "Epoch(211) - Fold 2 - Validation Loss : 0.4049\n",
      "Epoch(211) - Fold 3 - Validation Loss : 0.4178\n",
      "Epoch(211) - Fold 4 - Validation Loss : 0.3284\n",
      "Epoch(211) - GLOBAL - Validation Loss: 0.3987\n",
      "Saving model corresponding to last_loss == 0.39868919210514087\n",
      "\n",
      "\n",
      "Epoch(212) - Training Loss: 0.3888\n",
      "Epoch(212) - Fold 0 - Validation Loss : 0.4645\n",
      "Epoch(212) - Fold 1 - Validation Loss : 0.3777\n",
      "Epoch(212) - Fold 2 - Validation Loss : 0.4044\n",
      "Epoch(212) - Fold 3 - Validation Loss : 0.4173\n",
      "Epoch(212) - Fold 4 - Validation Loss : 0.3282\n",
      "Epoch(212) - GLOBAL - Validation Loss: 0.3984\n",
      "Saving model corresponding to last_loss == 0.3984136617945033\n",
      "\n",
      "\n",
      "Epoch(213) - Training Loss: 0.3886\n",
      "Epoch(213) - Fold 0 - Validation Loss : 0.4634\n",
      "Epoch(213) - Fold 1 - Validation Loss : 0.3770\n",
      "Epoch(213) - Fold 2 - Validation Loss : 0.4038\n",
      "Epoch(213) - Fold 3 - Validation Loss : 0.4169\n",
      "Epoch(213) - Fold 4 - Validation Loss : 0.3274\n",
      "Epoch(213) - GLOBAL - Validation Loss: 0.3977\n",
      "Saving model corresponding to last_loss == 0.39768426485752\n",
      "\n",
      "\n",
      "Epoch(214) - Training Loss: 0.3884\n",
      "Epoch(214) - Fold 0 - Validation Loss : 0.4631\n",
      "Epoch(214) - Fold 1 - Validation Loss : 0.3765\n",
      "Epoch(214) - Fold 2 - Validation Loss : 0.4037\n",
      "Epoch(214) - Fold 3 - Validation Loss : 0.4164\n",
      "Epoch(214) - Fold 4 - Validation Loss : 0.3272\n",
      "Epoch(214) - GLOBAL - Validation Loss: 0.3974\n",
      "Saving model corresponding to last_loss == 0.397390959891625\n",
      "\n",
      "\n",
      "Epoch(215) - Training Loss: 0.3879\n",
      "Epoch(215) - Fold 0 - Validation Loss : 0.4626\n",
      "Epoch(215) - Fold 1 - Validation Loss : 0.3763\n",
      "Epoch(215) - Fold 2 - Validation Loss : 0.4034\n",
      "Epoch(215) - Fold 3 - Validation Loss : 0.4165\n",
      "Epoch(215) - Fold 4 - Validation Loss : 0.3273\n",
      "Epoch(215) - GLOBAL - Validation Loss: 0.3972\n",
      "Saving model corresponding to last_loss == 0.3972174837381621\n",
      "\n",
      "\n",
      "Epoch(216) - Training Loss: 0.3878\n",
      "Epoch(216) - Fold 0 - Validation Loss : 0.4633\n",
      "Epoch(216) - Fold 1 - Validation Loss : 0.3769\n",
      "Epoch(216) - Fold 2 - Validation Loss : 0.4040\n",
      "Epoch(216) - Fold 3 - Validation Loss : 0.4169\n",
      "Epoch(216) - Fold 4 - Validation Loss : 0.3275\n",
      "Epoch(216) - GLOBAL - Validation Loss: 0.3977\n",
      "Intermediate early stopping : vepoch_loss = 0.3977, the_last_loss=0.3972\n",
      "\n",
      "\n",
      "Epoch(217) - Training Loss: 0.3879\n",
      "Epoch(217) - Fold 0 - Validation Loss : 0.4628\n",
      "Epoch(217) - Fold 1 - Validation Loss : 0.3768\n",
      "Epoch(217) - Fold 2 - Validation Loss : 0.4037\n",
      "Epoch(217) - Fold 3 - Validation Loss : 0.4161\n",
      "Epoch(217) - Fold 4 - Validation Loss : 0.3270\n",
      "Epoch(217) - GLOBAL - Validation Loss: 0.3973\n",
      "Intermediate early stopping : vepoch_loss = 0.3973, the_last_loss=0.3972\n",
      "\n",
      "\n",
      "Epoch(218) - Training Loss: 0.3876\n",
      "Epoch(218) - Fold 0 - Validation Loss : 0.4634\n",
      "Epoch(218) - Fold 1 - Validation Loss : 0.3767\n",
      "Epoch(218) - Fold 2 - Validation Loss : 0.4037\n",
      "Epoch(218) - Fold 3 - Validation Loss : 0.4161\n",
      "Epoch(218) - Fold 4 - Validation Loss : 0.3275\n",
      "Epoch(218) - GLOBAL - Validation Loss: 0.3975\n",
      "Intermediate early stopping : vepoch_loss = 0.3975, the_last_loss=0.3972\n",
      "\n",
      "\n",
      "Epoch(219) - Training Loss: 0.3879\n",
      "Epoch(219) - Fold 0 - Validation Loss : 0.4624\n",
      "Epoch(219) - Fold 1 - Validation Loss : 0.3762\n",
      "Epoch(219) - Fold 2 - Validation Loss : 0.4031\n",
      "Epoch(219) - Fold 3 - Validation Loss : 0.4156\n",
      "Epoch(219) - Fold 4 - Validation Loss : 0.3267\n",
      "Epoch(219) - GLOBAL - Validation Loss: 0.3968\n",
      "Saving model corresponding to last_loss == 0.3968181140835293\n",
      "\n",
      "\n",
      "Epoch(220) - Training Loss: 0.3882\n",
      "Epoch(220) - Fold 0 - Validation Loss : 0.4631\n",
      "Epoch(220) - Fold 1 - Validation Loss : 0.3764\n",
      "Epoch(220) - Fold 2 - Validation Loss : 0.4034\n",
      "Epoch(220) - Fold 3 - Validation Loss : 0.4158\n",
      "Epoch(220) - Fold 4 - Validation Loss : 0.3270\n",
      "Epoch(220) - GLOBAL - Validation Loss: 0.3972\n",
      "Intermediate early stopping : vepoch_loss = 0.3972, the_last_loss=0.3968\n",
      "\n",
      "\n",
      "Epoch(221) - Training Loss: 0.3877\n",
      "Epoch(221) - Fold 0 - Validation Loss : 0.4618\n",
      "Epoch(221) - Fold 1 - Validation Loss : 0.3759\n",
      "Epoch(221) - Fold 2 - Validation Loss : 0.4033\n",
      "Epoch(221) - Fold 3 - Validation Loss : 0.4160\n",
      "Epoch(221) - Fold 4 - Validation Loss : 0.3271\n",
      "Epoch(221) - GLOBAL - Validation Loss: 0.3968\n",
      "Intermediate early stopping : vepoch_loss = 0.3968, the_last_loss=0.3968\n",
      "\n",
      "\n",
      "Epoch(222) - Training Loss: 0.3873\n",
      "Epoch(222) - Fold 0 - Validation Loss : 0.4610\n",
      "Epoch(222) - Fold 1 - Validation Loss : 0.3751\n",
      "Epoch(222) - Fold 2 - Validation Loss : 0.4021\n",
      "Epoch(222) - Fold 3 - Validation Loss : 0.4149\n",
      "Epoch(222) - Fold 4 - Validation Loss : 0.3261\n",
      "Epoch(222) - GLOBAL - Validation Loss: 0.3958\n",
      "Saving model corresponding to last_loss == 0.39583563866864263\n",
      "\n",
      "\n",
      "Epoch(223) - Training Loss: 0.3880\n",
      "Epoch(223) - Fold 0 - Validation Loss : 0.4650\n",
      "Epoch(223) - Fold 1 - Validation Loss : 0.3776\n",
      "Epoch(223) - Fold 2 - Validation Loss : 0.4044\n",
      "Epoch(223) - Fold 3 - Validation Loss : 0.4168\n",
      "Epoch(223) - Fold 4 - Validation Loss : 0.3278\n",
      "Epoch(223) - GLOBAL - Validation Loss: 0.3983\n",
      "Intermediate early stopping : vepoch_loss = 0.3983, the_last_loss=0.3958\n",
      "\n",
      "\n",
      "Epoch(224) - Training Loss: 0.3886\n",
      "Epoch(224) - Fold 0 - Validation Loss : 0.4621\n",
      "Epoch(224) - Fold 1 - Validation Loss : 0.3751\n",
      "Epoch(224) - Fold 2 - Validation Loss : 0.4021\n",
      "Epoch(224) - Fold 3 - Validation Loss : 0.4148\n",
      "Epoch(224) - Fold 4 - Validation Loss : 0.3258\n",
      "Epoch(224) - GLOBAL - Validation Loss: 0.3960\n",
      "Intermediate early stopping : vepoch_loss = 0.3960, the_last_loss=0.3958\n",
      "\n",
      "\n",
      "Epoch(225) - Training Loss: 0.3867\n",
      "Epoch(225) - Fold 0 - Validation Loss : 0.4614\n",
      "Epoch(225) - Fold 1 - Validation Loss : 0.3748\n",
      "Epoch(225) - Fold 2 - Validation Loss : 0.4019\n",
      "Epoch(225) - Fold 3 - Validation Loss : 0.4145\n",
      "Epoch(225) - Fold 4 - Validation Loss : 0.3258\n",
      "Epoch(225) - GLOBAL - Validation Loss: 0.3957\n",
      "Saving model corresponding to last_loss == 0.395681628365963\n",
      "\n",
      "\n",
      "Epoch(226) - Training Loss: 0.3865\n",
      "Epoch(226) - Fold 0 - Validation Loss : 0.4606\n",
      "Epoch(226) - Fold 1 - Validation Loss : 0.3743\n",
      "Epoch(226) - Fold 2 - Validation Loss : 0.4014\n",
      "Epoch(226) - Fold 3 - Validation Loss : 0.4137\n",
      "Epoch(226) - Fold 4 - Validation Loss : 0.3252\n",
      "Epoch(226) - GLOBAL - Validation Loss: 0.3950\n",
      "Saving model corresponding to last_loss == 0.3950381717847426\n",
      "\n",
      "\n",
      "Epoch(227) - Training Loss: 0.3861\n",
      "Epoch(227) - Fold 0 - Validation Loss : 0.4608\n",
      "Epoch(227) - Fold 1 - Validation Loss : 0.3744\n",
      "Epoch(227) - Fold 2 - Validation Loss : 0.4015\n",
      "Epoch(227) - Fold 3 - Validation Loss : 0.4138\n",
      "Epoch(227) - Fold 4 - Validation Loss : 0.3255\n",
      "Epoch(227) - GLOBAL - Validation Loss: 0.3952\n",
      "Intermediate early stopping : vepoch_loss = 0.3952, the_last_loss=0.3950\n",
      "\n",
      "\n",
      "Epoch(228) - Training Loss: 0.3865\n",
      "Epoch(228) - Fold 0 - Validation Loss : 0.4603\n",
      "Epoch(228) - Fold 1 - Validation Loss : 0.3740\n",
      "Epoch(228) - Fold 2 - Validation Loss : 0.4012\n",
      "Epoch(228) - Fold 3 - Validation Loss : 0.4136\n",
      "Epoch(228) - Fold 4 - Validation Loss : 0.3253\n",
      "Epoch(228) - GLOBAL - Validation Loss: 0.3949\n",
      "Saving model corresponding to last_loss == 0.3948951361344168\n",
      "\n",
      "\n",
      "Epoch(229) - Training Loss: 0.3870\n",
      "Epoch(229) - Fold 0 - Validation Loss : 0.4601\n",
      "Epoch(229) - Fold 1 - Validation Loss : 0.3736\n",
      "Epoch(229) - Fold 2 - Validation Loss : 0.4007\n",
      "Epoch(229) - Fold 3 - Validation Loss : 0.4137\n",
      "Epoch(229) - Fold 4 - Validation Loss : 0.3250\n",
      "Epoch(229) - GLOBAL - Validation Loss: 0.3946\n",
      "Saving model corresponding to last_loss == 0.39462541217687175\n",
      "\n",
      "\n",
      "Epoch(230) - Training Loss: 0.3872\n",
      "Epoch(230) - Fold 0 - Validation Loss : 0.4606\n",
      "Epoch(230) - Fold 1 - Validation Loss : 0.3740\n",
      "Epoch(230) - Fold 2 - Validation Loss : 0.4012\n",
      "Epoch(230) - Fold 3 - Validation Loss : 0.4135\n",
      "Epoch(230) - Fold 4 - Validation Loss : 0.3250\n",
      "Epoch(230) - GLOBAL - Validation Loss: 0.3949\n",
      "Intermediate early stopping : vepoch_loss = 0.3949, the_last_loss=0.3946\n",
      "\n",
      "\n",
      "Epoch(231) - Training Loss: 0.3873\n",
      "Epoch(231) - Fold 0 - Validation Loss : 0.4627\n",
      "Epoch(231) - Fold 1 - Validation Loss : 0.3758\n",
      "Epoch(231) - Fold 2 - Validation Loss : 0.4028\n",
      "Epoch(231) - Fold 3 - Validation Loss : 0.4149\n",
      "Epoch(231) - Fold 4 - Validation Loss : 0.3265\n",
      "Epoch(231) - GLOBAL - Validation Loss: 0.3965\n",
      "Intermediate early stopping : vepoch_loss = 0.3965, the_last_loss=0.3946\n",
      "\n",
      "\n",
      "Epoch(232) - Training Loss: 0.3865\n",
      "Epoch(232) - Fold 0 - Validation Loss : 0.4593\n",
      "Epoch(232) - Fold 1 - Validation Loss : 0.3731\n",
      "Epoch(232) - Fold 2 - Validation Loss : 0.4003\n",
      "Epoch(232) - Fold 3 - Validation Loss : 0.4131\n",
      "Epoch(232) - Fold 4 - Validation Loss : 0.3248\n",
      "Epoch(232) - GLOBAL - Validation Loss: 0.3941\n",
      "Saving model corresponding to last_loss == 0.3941087603339829\n",
      "\n",
      "\n",
      "Epoch(233) - Training Loss: 0.3853\n",
      "Epoch(233) - Fold 0 - Validation Loss : 0.4595\n",
      "Epoch(233) - Fold 1 - Validation Loss : 0.3732\n",
      "Epoch(233) - Fold 2 - Validation Loss : 0.4001\n",
      "Epoch(233) - Fold 3 - Validation Loss : 0.4124\n",
      "Epoch(233) - Fold 4 - Validation Loss : 0.3241\n",
      "Epoch(233) - GLOBAL - Validation Loss: 0.3939\n",
      "Saving model corresponding to last_loss == 0.3938523703678839\n",
      "\n",
      "\n",
      "Epoch(234) - Training Loss: 0.3855\n",
      "Epoch(234) - Fold 0 - Validation Loss : 0.4585\n",
      "Epoch(234) - Fold 1 - Validation Loss : 0.3726\n",
      "Epoch(234) - Fold 2 - Validation Loss : 0.3998\n",
      "Epoch(234) - Fold 3 - Validation Loss : 0.4121\n",
      "Epoch(234) - Fold 4 - Validation Loss : 0.3239\n",
      "Epoch(234) - GLOBAL - Validation Loss: 0.3934\n",
      "Saving model corresponding to last_loss == 0.393381730690116\n",
      "\n",
      "\n",
      "Epoch(235) - Training Loss: 0.3849\n",
      "Epoch(235) - Fold 0 - Validation Loss : 0.4583\n",
      "Epoch(235) - Fold 1 - Validation Loss : 0.3724\n",
      "Epoch(235) - Fold 2 - Validation Loss : 0.3995\n",
      "Epoch(235) - Fold 3 - Validation Loss : 0.4122\n",
      "Epoch(235) - Fold 4 - Validation Loss : 0.3235\n",
      "Epoch(235) - GLOBAL - Validation Loss: 0.3932\n",
      "Saving model corresponding to last_loss == 0.3931860688216825\n",
      "\n",
      "\n",
      "Epoch(236) - Training Loss: 0.3846\n",
      "Epoch(236) - Fold 0 - Validation Loss : 0.4599\n",
      "Epoch(236) - Fold 1 - Validation Loss : 0.3733\n",
      "Epoch(236) - Fold 2 - Validation Loss : 0.4010\n",
      "Epoch(236) - Fold 3 - Validation Loss : 0.4129\n",
      "Epoch(236) - Fold 4 - Validation Loss : 0.3249\n",
      "Epoch(236) - GLOBAL - Validation Loss: 0.3944\n",
      "Intermediate early stopping : vepoch_loss = 0.3944, the_last_loss=0.3932\n",
      "\n",
      "\n",
      "Epoch(237) - Training Loss: 0.3849\n",
      "Epoch(237) - Fold 0 - Validation Loss : 0.4582\n",
      "Epoch(237) - Fold 1 - Validation Loss : 0.3723\n",
      "Epoch(237) - Fold 2 - Validation Loss : 0.3997\n",
      "Epoch(237) - Fold 3 - Validation Loss : 0.4120\n",
      "Epoch(237) - Fold 4 - Validation Loss : 0.3239\n",
      "Epoch(237) - GLOBAL - Validation Loss: 0.3932\n",
      "Intermediate early stopping : vepoch_loss = 0.3932, the_last_loss=0.3932\n",
      "\n",
      "\n",
      "Epoch(238) - Training Loss: 0.3849\n",
      "Epoch(238) - Fold 0 - Validation Loss : 0.4594\n",
      "Epoch(238) - Fold 1 - Validation Loss : 0.3730\n",
      "Epoch(238) - Fold 2 - Validation Loss : 0.4000\n",
      "Epoch(238) - Fold 3 - Validation Loss : 0.4121\n",
      "Epoch(238) - Fold 4 - Validation Loss : 0.3243\n",
      "Epoch(238) - GLOBAL - Validation Loss: 0.3938\n",
      "Intermediate early stopping : vepoch_loss = 0.3938, the_last_loss=0.3932\n",
      "\n",
      "\n",
      "Epoch(239) - Training Loss: 0.3852\n",
      "Epoch(239) - Fold 0 - Validation Loss : 0.4578\n",
      "Epoch(239) - Fold 1 - Validation Loss : 0.3718\n",
      "Epoch(239) - Fold 2 - Validation Loss : 0.3993\n",
      "Epoch(239) - Fold 3 - Validation Loss : 0.4114\n",
      "Epoch(239) - Fold 4 - Validation Loss : 0.3234\n",
      "Epoch(239) - GLOBAL - Validation Loss: 0.3927\n",
      "Saving model corresponding to last_loss == 0.39273664747578657\n",
      "\n",
      "\n",
      "Epoch(240) - Training Loss: 0.3842\n",
      "Epoch(240) - Fold 0 - Validation Loss : 0.4587\n",
      "Epoch(240) - Fold 1 - Validation Loss : 0.3725\n",
      "Epoch(240) - Fold 2 - Validation Loss : 0.4000\n",
      "Epoch(240) - Fold 3 - Validation Loss : 0.4121\n",
      "Epoch(240) - Fold 4 - Validation Loss : 0.3241\n",
      "Epoch(240) - GLOBAL - Validation Loss: 0.3935\n",
      "Intermediate early stopping : vepoch_loss = 0.3935, the_last_loss=0.3927\n",
      "\n",
      "\n",
      "Epoch(241) - Training Loss: 0.3847\n",
      "Epoch(241) - Fold 0 - Validation Loss : 0.4624\n",
      "Epoch(241) - Fold 1 - Validation Loss : 0.3754\n",
      "Epoch(241) - Fold 2 - Validation Loss : 0.4025\n",
      "Epoch(241) - Fold 3 - Validation Loss : 0.4145\n",
      "Epoch(241) - Fold 4 - Validation Loss : 0.3260\n",
      "Epoch(241) - GLOBAL - Validation Loss: 0.3961\n",
      "Intermediate early stopping : vepoch_loss = 0.3961, the_last_loss=0.3927\n",
      "\n",
      "\n",
      "Epoch(242) - Training Loss: 0.3914\n",
      "Epoch(242) - Fold 0 - Validation Loss : 0.4642\n",
      "Epoch(242) - Fold 1 - Validation Loss : 0.3763\n",
      "Epoch(242) - Fold 2 - Validation Loss : 0.4041\n",
      "Epoch(242) - Fold 3 - Validation Loss : 0.4156\n",
      "Epoch(242) - Fold 4 - Validation Loss : 0.3271\n",
      "Epoch(242) - GLOBAL - Validation Loss: 0.3975\n",
      "Intermediate early stopping : vepoch_loss = 0.3975, the_last_loss=0.3927\n",
      "\n",
      "\n",
      "Epoch(243) - Training Loss: 0.3886\n",
      "Epoch(243) - Fold 0 - Validation Loss : 0.4583\n",
      "Epoch(243) - Fold 1 - Validation Loss : 0.3721\n",
      "Epoch(243) - Fold 2 - Validation Loss : 0.3998\n",
      "Epoch(243) - Fold 3 - Validation Loss : 0.4116\n",
      "Epoch(243) - Fold 4 - Validation Loss : 0.3238\n",
      "Epoch(243) - GLOBAL - Validation Loss: 0.3931\n",
      "Intermediate early stopping : vepoch_loss = 0.3931, the_last_loss=0.3927\n",
      "\n",
      "\n",
      "Epoch(244) - Training Loss: 0.3849\n",
      "Epoch(244) - Fold 0 - Validation Loss : 0.4591\n",
      "Epoch(244) - Fold 1 - Validation Loss : 0.3726\n",
      "Epoch(244) - Fold 2 - Validation Loss : 0.4003\n",
      "Epoch(244) - Fold 3 - Validation Loss : 0.4120\n",
      "Epoch(244) - Fold 4 - Validation Loss : 0.3240\n",
      "Epoch(244) - GLOBAL - Validation Loss: 0.3936\n",
      "Intermediate early stopping : vepoch_loss = 0.3936, the_last_loss=0.3927\n",
      "Meet Early stopping!\n",
      "Training summary:\n",
      "{'Loss': 0.39273664747578657, 'Loss folds': [0.45908590489886253, 0.37257796595077275, 0.40025526472708745, 0.41198810006771286, 0.32399610397824974], 'Loss_std': 0.04462610200642329}\n",
      "Training ended\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)  \n",
    "\n",
    "ENCODER_SIZE = 32\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            #nn.Dropout(0.5),  # Noise layer\n",
    "            nn.Linear(len(FEATURES_LIST_TOTRAIN), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, ENCODER_SIZE),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.Linear(ENCODER_SIZE, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, len(FEATURES_LIST_TOTRAIN)),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.training:\n",
    "            x = x + 0.01 * torch.randn(x.shape[0], x.shape[1]).double(). to('cuda')  # 0.01 = noise variance\n",
    "            \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x        \n",
    "\n",
    "model = AutoEncoder().double().to('cuda')\n",
    "    \n",
    "#print('Number of model parameters :')\n",
    "#numel_list = [p.numel() for p in model.parameters()]\n",
    "#sum(numel_list), numel_list\n",
    "    \n",
    "if (RETRAIN_MODEl_AE == False):\n",
    "    model_AE = model\n",
    "    model_AE.load_state_dict(torch.load(MODEL_FILE_AE,map_location=torch.device('cuda')))\n",
    "    print('Model AE loaded')\n",
    "\n",
    "else:    \n",
    "    print('Training started')\n",
    "    patience=5\n",
    "\n",
    "    utility_scores = [None] * 5\n",
    "    accuracy_scores = [None] * 5\n",
    "\n",
    "    today = datetime.datetime.now()\n",
    "    now_str = today.strftime(\"%b%d_%H-%M-%S\")\n",
    "    tensorboard_dir_AE = 'runs_AE/' + now_str\n",
    "    writer = SummaryWriter(log_dir=tensorboard_dir_AE)\n",
    "\n",
    "    ts_train = torch.tensor(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    #ts_train_y = torch.tensor((df.loc[folds_list_train_unique, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "    # Normalize data\n",
    "    ts_train_mean = torch.mean(ts_train, axis=0)\n",
    "    ts_train_std = torch.std(ts_train, axis=0)\n",
    "    #ts_train_mean = torch.tensor(f_mean)\n",
    "    # If you want to use Standard scale : calculate mean from f_mean and std scale from whole dataset\n",
    "    #ts_train = pyStandardScale(ts_train, ts_train_mean, ts_train_std)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(ts_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE_AE, shuffle=True)\n",
    "\n",
    "    ts_test = [None] * 5\n",
    "    #ts_test_y = [None] * 5    \n",
    "    test_dataset = [None] * 5\n",
    "    test_loader = [None] * 5\n",
    "\n",
    "    for fold_indice in range(5):\n",
    "        ts_test[fold_indice] = torch.tensor(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "        #ts_test_y[fold_indice] = torch.tensor((df.loc[folds_list_test[fold_indice], 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "        # Normalize\n",
    "        #ts_test[fold_indice] = pyStandardScale(ts_test[fold_indice], ts_train_mean, ts_train_std)\n",
    "\n",
    "        test_dataset[fold_indice] = torch.utils.data.TensorDataset(ts_test[fold_indice])\n",
    "        test_loader[fold_indice] = torch.utils.data.DataLoader(test_dataset[fold_indice], batch_size=BATCH_SIZE_AE)\n",
    "\n",
    "    loss_fn = nn.MSELoss().to('cuda')\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE_AE, weight_decay=WEIGHT_DECAY_AE) \n",
    "\n",
    "    scheduler = None\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
    "    #                                                         max_lr=1e-4, epochs=NUM_EPOCHS, steps_per_epoch=len(train_loader))\n",
    "\n",
    "    #model.eval()\n",
    "    #start_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #start_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #print('Start Validation Accuracy: {:.4f}'.format(start_accuracy))\n",
    "    #print('Start Validation Utility: {:.4f}'.format(start_utility_score))\n",
    "\n",
    "    Val_Loss = 0\n",
    "    N_Samples = 0\n",
    "\n",
    "    the_last_loss = 10000\n",
    "    the_last_utility_score = 0\n",
    "    the_last_accuracy = 0\n",
    "    trigger_times=0\n",
    "    early_stopping_met = False\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS_AE): \n",
    "        running_loss = 0.0        \n",
    "\n",
    "        ### Call back to save activation stats (mean, std dev and near 0 values after activation functions)\n",
    "        # Setting hook for activation layers stats\n",
    "\n",
    "        hook_handles = []\n",
    "        save_output_activation_stats = []\n",
    "\n",
    "        for layer in model.modules():\n",
    "            if ('activation' in str(type(layer))):\n",
    "                save_output_activation_stats_1layer = SaveOutputActivationStats()\n",
    "                handle = layer.register_forward_hook(save_output_activation_stats_1layer)\n",
    "                save_output_activation_stats.append(save_output_activation_stats_1layer)\n",
    "                hook_handles.append(handle)    \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs = batch[0].to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, inputs.double())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "        # update local train loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # update global train loss\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print('Epoch({}) - Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "        writer.add_scalar(f\"Global train/loss\", epoch_loss, epoch)\n",
    "\n",
    "        # Write activation stats graphs\n",
    "        for layer_number,save_output_activation_stats_layer in enumerate(save_output_activation_stats):\n",
    "            df_stats_layer = pd.DataFrame(save_output_activation_stats_layer.outputs)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(25, 4))\n",
    "\n",
    "            ax[0].set_title(f'Layer {layer_number} : Mean activation value', fontsize=16)\n",
    "            ax[0].set_xlabel('Batch instances')\n",
    "            ax[0].set_ylabel('Mean')\n",
    "            ax[0].plot(range(df_stats_layer.shape[0]), df_stats_layer['mean'])\n",
    "\n",
    "            ax[1].set_title(f'Layer {layer_number} : Std deviation activation value', fontsize=16)\n",
    "            ax[1].set_xlabel('Batch instances')\n",
    "            ax[1].set_ylabel('Standard deviation')\n",
    "            ax[1].plot(range(df_stats_layer.shape[0]), df_stats_layer['std'])\n",
    "\n",
    "            ax[2].set_title(f'Layer {layer_number} : Percentage of activation values near zero', fontsize=16)\n",
    "            ax[2].set_xlabel('Batch instances')\n",
    "            ax[2].set_ylabel('Percentage')\n",
    "            ax[2].plot(range(df_stats_layer.shape[0]), df_stats_layer['near_zero']);\n",
    "\n",
    "            plot_buf = io.BytesIO()\n",
    "            plt.savefig(plot_buf, format='jpeg')\n",
    "            plt.close()\n",
    "\n",
    "            plot_buf.seek(0)\n",
    "            image = PIL.Image.open(plot_buf)\n",
    "            image = transforms.ToTensor()(image)\n",
    "            writer.add_image(\"Train activation stats/Activation stats layer \" + str(layer_number), image, epoch)\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "\n",
    "        vrunning_loss = [None] * 5\n",
    "        num_samples = [None] * 5\n",
    "        vepoch_loss_folds = [None] * 5\n",
    "        vepoch_accuracy_folds = [None] * 5\n",
    "        vepoch_utility_score_folds = [None] * 5\n",
    "\n",
    "        for fold_indice in range(5):    \n",
    "            vrunning_loss[fold_indice] = 0.0\n",
    "            num_samples[fold_indice] = 0\n",
    "\n",
    "            for batch in test_loader[fold_indice]:\n",
    "                inputs = batch[0].to('cuda')\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, inputs.double())\n",
    "\n",
    "                vrunning_loss[fold_indice] += loss.item() * inputs.size(0)\n",
    "                num_samples[fold_indice] += inputs.size(0)\n",
    "\n",
    "                vepoch_loss_folds[fold_indice] = vrunning_loss[fold_indice] / num_samples[fold_indice]\n",
    "\n",
    "            print('Epoch({}) - Fold {} - Validation Loss : {:.4f}'.format(epoch, fold_indice, vepoch_loss_folds[fold_indice]))        \n",
    "\n",
    "        # update epoch loss\n",
    "        vepoch_loss = sum(vepoch_loss_folds) / len(vepoch_loss_folds)\n",
    "        print('Epoch({}) - GLOBAL - Validation Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "\n",
    "        #print(f'Sum of model parameters ({epoch}):')\n",
    "        #[print(p.sum()) for p in model.parameters()]\n",
    "\n",
    "        writer.add_scalar(\"Global valid/Loss\", vepoch_loss, epoch)\n",
    "\n",
    "        for fold_indice in range(5):\n",
    "            writer.add_scalar(\"Fold valid Loss/Loss fold \"+str(fold_indice), vepoch_loss_folds[fold_indice], epoch)        \n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        # Check if Early Stopping\n",
    "\n",
    "        if (vepoch_loss > the_last_loss):\n",
    "            if (EARLY_STOPPING == True):\n",
    "                trigger_times += 1\n",
    "\n",
    "                print(f'Intermediate early stopping : vepoch_loss = {vepoch_loss:.4f}, the_last_loss={the_last_loss:.4f}')\n",
    "\n",
    "                if trigger_times >= patience:\n",
    "                    print('Meet Early stopping!')\n",
    "                    early_stopping_met = True\n",
    "                    ##torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "                    break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "            the_last_loss = vepoch_loss\n",
    "\n",
    "            the_best_epoch = epoch\n",
    "\n",
    "            # Save model for the best version so far\n",
    "            print(f'Saving model corresponding to last_loss == {the_last_loss}')\n",
    "            torch.save(model.state_dict(), MODEL_FILE_AE)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "    if (early_stopping_met == False):\n",
    "        print(\"Didn't meet early stopping : saving final model\")\n",
    "        # Save model if don't meet early stopping\n",
    "        torch.save(model.state_dict(), MODEL_FILE_AE)\n",
    "\n",
    "    writer.add_text(f\"Global valid/Loss\", f\"Best loss: {the_last_loss}\", the_best_epoch)\n",
    "\n",
    "    scores_results = {'Loss': the_last_loss, 'Loss folds': vepoch_loss_folds, 'Loss_std': np.std(vepoch_loss_folds)}\n",
    "\n",
    "    writer.add_text('Final score', str(scores_results))\n",
    "    writer.add_text('Batch size', str(BATCH_SIZE_AE))\n",
    "    writer.add_text('Patience', str(patience))\n",
    "    writer.add_text('Number of epochs', str(NUM_EPOCHS_AE))\n",
    "    writer.add_text('Best epoch', str(the_best_epoch))\n",
    "    writer.add_text('Number of parameters per layer', str([p.numel() for p in model.parameters()]))\n",
    "    writer.add_text('Model architecture', str(model).replace('\\n', '<BR>'))\n",
    "    writer.add_text('Comment', MODEL_COMMENT_AE)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    print('Training summary:')\n",
    "    print(scores_results)\n",
    "\n",
    "    model_AE = model\n",
    "    model_AE.eval()\n",
    "    print('Training ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3105054402961592"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[FEATURES_LIST_TOTRAIN].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4781999277368514"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.08 / df[FEATURES_LIST_TOTRAIN].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(ts_train[0:5, :], model_AE(ts_train[0:5, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_AE(ts_train[0:5, :])[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_train[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(ts_train[50, 5], model_AE(ts_train[:, :])[50, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: r3hng0x7\n",
      "Sweep URL: https://wandb.ai/fboyer/janestreet-mlp/sweeps/r3hng0x7\n"
     ]
    }
   ],
   "source": [
    "if (DO_SWEEP == True):\n",
    "    sweep_id = wandb.sweep(sweep_config, entity='fboyer', project=\"janestreet-mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_MLP = dict(\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay = WEIGHT_DECAY,\n",
    "        dropout = DROPOUT,\n",
    "        use_autoenc = 'encoder'\n",
    "        )\n",
    "\n",
    "    if (DO_SWEEP == True):\n",
    "        #run = wandb.init()\n",
    "        #config_MLP = run.config\n",
    "        wandb.init(config=config_MLP)\n",
    "        config_MLP = wandb.config\n",
    "\n",
    "    else:\n",
    "        wandb.init(project='janestreet-mlp', config=config_MLP)\n",
    "        config_MLP = wandb.config\n",
    "\n",
    "    print('Training started')\n",
    "    patience=5\n",
    "\n",
    "    print('Run config :')\n",
    "    print(\"config:\", dict(config_MLP))\n",
    "\n",
    "    utility_scores = [None] * 5\n",
    "    accuracy_scores = [None] * 5\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    ts_train = torch.tensor(df.loc[folds_list_train2_unique, FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "    ts_train_y = torch.tensor((df.loc[folds_list_train2_unique, 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "    # Normalize data\n",
    "    ts_train_mean = torch.mean(ts_train, axis=0)\n",
    "    ts_train_std = torch.std(ts_train, axis=0)\n",
    "    #ts_train = pyStandardScale(ts_train, ts_train_mean, ts_train_std)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(ts_train, ts_train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config_MLP.batch_size, shuffle=True) # pin_memory : VOIR RESULTAT\n",
    "\n",
    "    ts_test = [None] * 5\n",
    "    ts_test_y = [None] * 5    \n",
    "    test_dataset = [None] * 5\n",
    "    test_loader = [None] * 5\n",
    "\n",
    "    for fold_indice in range(5):\n",
    "        ts_test[fold_indice] = torch.tensor(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN].to_numpy(), device='cuda')\n",
    "        ts_test_y[fold_indice] = torch.tensor((df.loc[folds_list_test[fold_indice], 'resp'] > 0).astype(np.byte).to_numpy(), device='cuda')\n",
    "\n",
    "        # Normalize\n",
    "        #ts_test[fold_indice] = pyStandardScale(ts_test[fold_indice], ts_train_mean, ts_train_std)\n",
    "\n",
    "        test_dataset[fold_indice] = torch.utils.data.TensorDataset(ts_test[fold_indice], ts_test_y[fold_indice])\n",
    "        test_loader[fold_indice] = torch.utils.data.DataLoader(test_dataset[fold_indice], batch_size=config_MLP.batch_size)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, AEncoder):\n",
    "        #def __init__(self):\n",
    "            super(MLP,self).__init__()\n",
    "\n",
    "            #self.act_n = torch.zeros((config_MLP.batch_size, ACT_N_SIZE))\n",
    "            if (ACT_N == True):\n",
    "                self.act_n = torch.zeros(ACT_N_SIZE, device='cuda').unsqueeze(0)\n",
    "                act_n_size = ACT_N_SIZE\n",
    "\n",
    "            else:\n",
    "                act_n_size = 0\n",
    "\n",
    "            self.AEncoder = AEncoder\n",
    "\n",
    "            if (config_MLP.use_autoenc == 'encoder-decoder'):\n",
    "                #self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) + self.AEncoder.decoder[0].in_features, 200)\n",
    "                self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) * 2 + act_n_size, 200) # <= % near 0 élevé\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder'):\n",
    "                self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) + ENCODER_SIZE + act_n_size, 200) # <= % near 0 élevé\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder-only'):\n",
    "                self.layer1 = nn.Linear(ENCODER_SIZE + act_n_size, 200) # <= % near 0 élevé\n",
    "                \n",
    "            else:\n",
    "                self.layer1 = nn.Linear(len(FEATURES_LIST_TOTRAIN) + act_n_size, 200) # <= % near 0 élevé\n",
    "\n",
    "            if (config_MLP.activation_function == 'relu'):\n",
    "                self.act1 = nn.ReLU()\n",
    "                \n",
    "            elif (config_MLP.activation_function == 'leakyrelu'):\n",
    "                self.act1 = nn.LeakyReLU()\n",
    "                \n",
    "            self.dropout1 = nn.Dropout(config_MLP.dropout)\n",
    "\n",
    "            self.layer2 = nn.Linear(200, 100)\n",
    "\n",
    "            if (config_MLP.activation_function == 'relu'):\n",
    "                self.act2 = nn.ReLU()\n",
    "                \n",
    "            elif (config_MLP.activation_function == 'leakyrelu'):\n",
    "                self.act2 = nn.LeakyReLU()\n",
    "\n",
    "            self.dropout2 = nn.Dropout(config_MLP.dropout)\n",
    "\n",
    "            self.layer3 = nn.Linear(100, 1)\n",
    "            self.act3 = nn.Sigmoid()\n",
    "\n",
    "        def encoder(self, x):\n",
    "            self.AEncoder.eval()\n",
    "\n",
    "            encoded = self.AEncoder.encoder(x)\n",
    "\n",
    "            return encoded\n",
    "\n",
    "        def encoder_decoder(self, x):\n",
    "            self.AEncoder.eval()\n",
    "\n",
    "            encoded_decoded = self.AEncoder(x)\n",
    "\n",
    "            return encoded_decoded\n",
    "\n",
    "        def forward(self,x):\n",
    "            #x_encoded = self.encoder(x)\n",
    "\n",
    "            #x = torch.cat((x, x_encoded), dim=1)\n",
    "            if (config_MLP.use_autoenc == 'encoder-decoder'):\n",
    "                x_decoded = self.encoder_decoder(x)\n",
    "                x = torch.cat((x, x_decoded), dim=1)\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder'):\n",
    "                x_decoded = self.encoder(x)\n",
    "                x = torch.cat((x, x_decoded), dim=1)\n",
    "\n",
    "            elif (config_MLP.use_autoenc == 'encoder-only'):\n",
    "                x_decoded = self.encoder(x)\n",
    "                x = x_decoded\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if (ACT_N == True):\n",
    "                x = torch.cat((x, self.act_n.expand(x.shape[0], ACT_N_SIZE)), dim=1)    \n",
    "\n",
    "            x = self.dropout1(self.act1(self.layer1(x)))\n",
    "            x = self.dropout2(self.act2(self.layer2(x)))\n",
    "\n",
    "            x = self.act3(self.layer3(x))            \n",
    "\n",
    "            # Remove oldest previously saved output (located at the end of tensor => index -1 for not selecting it) of NN and replace by new one (which is x, that we assign at start of tensor)\n",
    "            # expand() because self.act_n when first assigned has only 1 line, but here it expands to number of lines in x (batch size)\n",
    "            if (ACT_N == True):\n",
    "                self.act_n = torch.cat((x, self.act_n[:, :-1].expand(x.shape[0], ACT_N_SIZE-1)), dim=1)\n",
    "\n",
    "\n",
    "            #print('self.act_n:')\n",
    "            #print(self.act_n)\n",
    "\n",
    "            return x        \n",
    "\n",
    "    #model = MLP(model_AE)\n",
    "    model = MLP(model_AE).double().to('cuda')\n",
    "\n",
    "    for param in model.AEncoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    '''\n",
    "    model = nn.Sequential(\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(len(FEATURES_LIST_TOTRAIN), 200),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "\n",
    "            nn.Linear(200, 100),\n",
    "            #nn.BatchNorm1d(130),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid(),\n",
    "        ).double().to('cuda')\n",
    "    '''\n",
    "\n",
    "    #print('Number of model parameters :')\n",
    "    #numel_list = [p.numel() for p in model.parameters()]\n",
    "    #sum(numel_list), numel_list\n",
    "\n",
    "    loss_fn = nn.BCELoss().to('cuda')\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=config_MLP.learning_rate, weight_decay=config_MLP.weight_decay) \n",
    "    optimizer = optim.RAdam(model.parameters(), lr=config_MLP.learning_rate, weight_decay=config_MLP.weight_decay) \n",
    "\n",
    "    scheduler = None\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
    "    #                                                         max_lr=1e-4, epochs=config_MLP.epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "    if (DO_SWEEP == False):\n",
    "        wandb.watch(model, loss_fn, log=\"all\", log_freq=1)\n",
    "\n",
    "    model.eval()\n",
    "    #start_accuracy = accuracy_score(ts_test_y.cpu().numpy(), (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #start_utility_score = utility_function(df.loc[test_index], (model(ts_test).squeeze() > 0.5).cpu().numpy())\n",
    "    #print('Start Validation Accuracy: {:.4f}'.format(start_accuracy))\n",
    "    #print('Start Validation Utility: {:.4f}'.format(start_utility_score))\n",
    "\n",
    "\n",
    "    Val_Loss = 0\n",
    "    N_Samples = 0\n",
    "\n",
    "    the_last_loss = 100\n",
    "    the_last_utility_score = 0\n",
    "    the_last_accuracy = 0\n",
    "    trigger_times=0\n",
    "    early_stopping_met = False\n",
    "\n",
    "    for epoch in range(config_MLP.epochs): \n",
    "        running_loss = 0.0        \n",
    "\n",
    "        ### Call back to save activation stats (mean, std dev and near 0 values after activation functions)\n",
    "        # Setting hook for activation layers stats\n",
    "\n",
    "        hook_handles = []\n",
    "        save_output_activation_stats = []\n",
    "\n",
    "        for layer in model.modules():\n",
    "            if ('activation' in str(type(layer))):\n",
    "                save_output_activation_stats_1layer = SaveOutputActivationStats()\n",
    "                handle = layer.register_forward_hook(save_output_activation_stats_1layer)\n",
    "                save_output_activation_stats.append(save_output_activation_stats_1layer)\n",
    "                hook_handles.append(handle)    \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            #inputs, labels = batch[0], batch[1]\n",
    "            inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "                #loss.backward(retain_graph=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "        # update local train loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # update global train loss\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print('Epoch({}) - Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "\n",
    "        writer.add_scalar(f\"Global train/loss\", epoch_loss, epoch)\n",
    "        wandb.log({'Global train/loss' : epoch_loss}, step=epoch)\n",
    "\n",
    "        # Write activation stats graphs\n",
    "        for layer_number,save_output_activation_stats_layer in enumerate(save_output_activation_stats):\n",
    "            df_stats_layer = pd.DataFrame(save_output_activation_stats_layer.outputs)\n",
    "\n",
    "            if ((df_stats_layer.shape[0] == 0) and (df_stats_layer.shape[1] == 0)):\n",
    "                print(f'Activation stats: No data returned for stats at layer {layer_number}')\n",
    "\n",
    "            else:\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(25, 4))\n",
    "\n",
    "                ax[0].set_title(f'Layer {layer_number} : Mean activation value', fontsize=16)\n",
    "                ax[0].set_xlabel('Batch instances')\n",
    "                ax[0].set_ylabel('Mean')\n",
    "                ax[0].plot(range(df_stats_layer.shape[0]), df_stats_layer['mean'])\n",
    "\n",
    "                ax[1].set_title(f'Layer {layer_number} : Std deviation activation value', fontsize=16)\n",
    "                ax[1].set_xlabel('Batch instances')\n",
    "                ax[1].set_ylabel('Standard deviation')\n",
    "                ax[1].plot(range(df_stats_layer.shape[0]), df_stats_layer['std'])\n",
    "\n",
    "                ax[2].set_title(f'Layer {layer_number} : Percentage of activation values near zero', fontsize=16)\n",
    "                ax[2].set_xlabel('Batch instances')\n",
    "                ax[2].set_ylabel('Percentage')\n",
    "                ax[2].plot(range(df_stats_layer.shape[0]), df_stats_layer['near_zero']);\n",
    "\n",
    "                plot_buf = io.BytesIO()\n",
    "                plt.savefig(plot_buf, format='jpeg')\n",
    "                plt.close()\n",
    "\n",
    "                plot_buf.seek(0)\n",
    "                image = PIL.Image.open(plot_buf)\n",
    "                image = transforms.ToTensor()(image)\n",
    "                writer.add_image(\"Train activation stats/Activation stats layer \" + str(layer_number), image, epoch)\n",
    "                wandb.log({'Train activation stats/Activation stats layer' + str(layer_number) : wandb.Image(image)}, step=epoch)\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "\n",
    "        vrunning_loss = [None] * 5\n",
    "        num_samples = [None] * 5\n",
    "        vepoch_loss_folds = [None] * 5\n",
    "        vepoch_accuracy_folds = [None] * 5\n",
    "        vepoch_utility_score_folds = [None] * 5\n",
    "\n",
    "        for fold_indice in range(5):    \n",
    "            vrunning_loss[fold_indice] = 0.0\n",
    "            num_samples[fold_indice] = 0\n",
    "\n",
    "            for batch in test_loader[fold_indice]:\n",
    "                inputs, labels = batch[0].to('cuda'), batch[1].to('cuda')\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels.unsqueeze(-1).double())\n",
    "\n",
    "                vrunning_loss[fold_indice] += loss.item() * inputs.size(0)\n",
    "                num_samples[fold_indice] += labels.size(0)\n",
    "\n",
    "                vepoch_loss_folds[fold_indice] = vrunning_loss[fold_indice] / num_samples[fold_indice]\n",
    "\n",
    "            print('Epoch({}) - Fold {} - Validation Loss : {:.4f}'.format(epoch, fold_indice, vepoch_loss_folds[fold_indice]))\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                vepoch_accuracy_folds[fold_indice] = accuracy_score(ts_test_y[fold_indice].cpu().numpy(), (model(ts_test[fold_indice]).squeeze() > 0.5).cpu().numpy())\n",
    "                vepoch_utility_score_folds[fold_indice] = utility_function(df.loc[folds_list_test[fold_indice]], (model(ts_test[fold_indice]).squeeze() > 0.5).cpu().numpy())\n",
    "            print('Epoch({}) - Fold {} - Validation Accuracy : {:.4f}'.format(epoch, fold_indice, vepoch_accuracy_folds[fold_indice]))\n",
    "            print('Epoch({}) - Fold {} - Validation Utility score : {:.4f}'.format(epoch, fold_indice, vepoch_utility_score_folds[fold_indice]))\n",
    "\n",
    "\n",
    "        # update epoch loss\n",
    "        vepoch_loss = sum(vepoch_loss_folds) / len(vepoch_loss_folds)\n",
    "        vepoch_accuracy = sum(vepoch_accuracy_folds) / len(vepoch_accuracy_folds)\n",
    "        vepoch_utility_score = sum(vepoch_utility_score_folds) #/ len(vepoch_utility_score_folds)\n",
    "        print('Epoch({}) - GLOBAL - Validation Loss: {:.4f}'.format(epoch, vepoch_loss))\n",
    "        print('Epoch({}) - GLOBAL - Validation Accuracy: {:.4f}'.format(epoch, vepoch_accuracy))\n",
    "        print('Epoch({}) - GLOBAL - Validation Utility score: {:.4f}'.format(epoch, vepoch_utility_score))\n",
    "\n",
    "        #print(f'Sum of model parameters ({epoch}):')\n",
    "        #[print(p.sum()) for p in model.parameters()]\n",
    "\n",
    "        writer.add_scalar(\"Global valid/Loss\", vepoch_loss, epoch)\n",
    "        writer.add_scalar(\"Global valid/Accuracy\", vepoch_accuracy, epoch)\n",
    "        writer.add_scalar(\"Global valid/Utility\", vepoch_utility_score, epoch)\n",
    "        wandb.log({'Global valid/Loss' : vepoch_loss}, step=epoch)\n",
    "        wandb.log({'Global valid/Accuracy' : vepoch_accuracy}, step=epoch)\n",
    "        wandb.log({'Global valid/Utility' : vepoch_utility_score}, step=epoch)\n",
    "\n",
    "        for fold_indice in range(5):\n",
    "            writer.add_scalar(\"Fold valid Loss/Loss fold \"+str(fold_indice), vepoch_loss_folds[fold_indice], epoch)\n",
    "            writer.add_scalar(\"Fold valid Accuracy/Accuracy fold \"+str(fold_indice), vepoch_accuracy_folds[fold_indice], epoch)\n",
    "            writer.add_scalar(\"Fold valid Utility/Utility fold \"+str(fold_indice), vepoch_utility_score_folds[fold_indice], epoch)\n",
    "\n",
    "            wandb.log({\"Fold valid Loss/Loss fold \"+str(fold_indice) : vepoch_loss_folds[fold_indice]}, step=epoch)\n",
    "            wandb.log({\"Fold valid Accuracy/Accuracy fold \"+str(fold_indice) : vepoch_accuracy_folds[fold_indice]}, step=epoch)\n",
    "            wandb.log({\"Fold valid Utility/Utility fold \"+str(fold_indice) : vepoch_utility_score_folds[fold_indice]}, step=epoch)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        # Check if Early Stopping\n",
    "        #if vepoch_loss > the_last_loss:\n",
    "        #if (vepoch_utility_score < the_last_utility_score) and (vepoch_loss > the_last_loss) and (vepoch_accuracy < the_last_accuracy):\n",
    "\n",
    "        if (vepoch_loss > the_last_loss):\n",
    "            if (EARLY_STOPPING == True):\n",
    "                trigger_times += 1\n",
    "\n",
    "                print(f'Intermediate early stopping : vepoch_loss = {vepoch_loss:.4f}, the_last_loss={the_last_loss:.4f}')\n",
    "                #print(f'Intermediate early stopping : vepoch_accuracy = {vepoch_accuracy:.4f}, the_last_utility_score={the_last_accuracy:.4f}')\n",
    "                #print(f'Intermediate early stopping : vepoch_utility_score = {vepoch_utility_score:.4f}, the_last_utility_score={the_last_utility_score:.4f}')\n",
    "\n",
    "                if trigger_times >= patience:\n",
    "                    print('Meet Early stopping!')\n",
    "                    early_stopping_met = True\n",
    "                    ##torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "                    break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "            the_last_loss = vepoch_loss\n",
    "            the_last_utility_score = vepoch_utility_score\n",
    "            the_last_accuracy = vepoch_accuracy\n",
    "\n",
    "            the_last_utility_score_folds = vepoch_utility_score_folds\n",
    "            the_last_accuracy_folds = vepoch_accuracy_folds\n",
    "\n",
    "            the_best_epoch = epoch\n",
    "\n",
    "            # Save model for the best version so far\n",
    "            print(f'Saving model corresponding to last_utility_score == {the_last_utility_score}')\n",
    "            torch.save(model.state_dict(), MODEL_FILE)\n",
    "            #torch.onnx.export(model, batch, MODEL_FILE_ONNX)\n",
    "            #wandb.save(MODEL_FILE_ONNX)\n",
    "            wandb.log({\"Best accuracy\" : the_last_accuracy}, step=epoch)\n",
    "            wandb.log({\"Best utility\" : the_last_utility_score}, step=epoch)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "    if (early_stopping_met == False):\n",
    "        print(\"Didn't meet early stopping : saving final model\")\n",
    "        # Save model if don't meet early stopping\n",
    "        torch.save(model.state_dict(), MODEL_FILE)\n",
    "        #torch.onnx.export(model, test_loader[0], MODEL_FILE_ONNX)\n",
    "        #wandb.save(MODEL_FILE_ONNX)\n",
    "        wandb.log({\"Best accuracy\" : the_last_accuracy}, step=epoch)\n",
    "        wandb.log({\"Best utility\" : the_last_utility_score}, step=epoch)\n",
    "\n",
    "    #utility_scores.append(the_last_utility_score)\n",
    "    #accuracy_scores.append(the_last_accuracy)\n",
    "    writer.add_text(f\"Global valid/Utility\", f\"Best utility: {the_last_utility_score}\", the_best_epoch)\n",
    "\n",
    "    scores_results = {'utility_score': the_last_utility_score, 'utility_scores': the_last_utility_score_folds, 'utility_score_std': np.std(the_last_utility_score_folds), 'accuracy_scores': the_last_accuracy_folds}\n",
    "\n",
    "    writer.add_text('Final utility score', str(scores_results))\n",
    "    writer.add_text('Batch size', str(config_MLP.batch_size))\n",
    "    writer.add_text('Patience', str(patience))\n",
    "    writer.add_text('Number of epochs', str(NUM_EPOCHS))\n",
    "    writer.add_text('Best epoch', str(the_best_epoch))\n",
    "    writer.add_text('Number of parameters per layer', str([p.numel() for p in model.parameters()]))\n",
    "    writer.add_text('Model architecture', str(model).replace('\\n', '<BR>'))\n",
    "    writer.add_text('Comment', MODEL_COMMENT)\n",
    "\n",
    "\n",
    "    wandb.run.summary['Final utility score'] = str(scores_results)\n",
    "    wandb.run.summary['Batch size'] = str(config_MLP.batch_size)\n",
    "    wandb.run.summary['Patience'] = str(patience)\n",
    "    wandb.run.summary['Number of epochs'] = str(NUM_EPOCHS)\n",
    "    wandb.run.summary['Best epoch'] = str(the_best_epoch)\n",
    "    wandb.run.summary['Number of parameters per layer'] = str([p.numel() for p in model.parameters()])\n",
    "    wandb.run.summary['Model architecture'] = str(model).replace('\\n', '<BR>')\n",
    "    wandb.log({\"Best accuracy\" : the_last_accuracy}, step=epoch)\n",
    "    wandb.log({\"Best utility\" : the_last_utility_score}, step=epoch)\n",
    "\n",
    "    #wandb.log({\"Comment\" : MODEL_COMMENT})\n",
    "    wandb.run.summary['comment'] = MODEL_COMMENT\n",
    "\n",
    "    writer.close()\n",
    "    wandb.run.finish()\n",
    "    #run.finish()\n",
    "\n",
    "    print('Training summary:')\n",
    "    print(scores_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o9u80kb4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: leakyrelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_autoenc: encoder\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">efficient-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/sweeps/r3hng0x7\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/sweeps/r3hng0x7</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/o9u80kb4\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/o9u80kb4</a><br/>\n",
       "                Run data is saved locally in <code>/home/francois/coding/OC/PJ9/wandb/run-20210215_120342-o9u80kb4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Run config :\n",
      "config: {'activation_function': 'leakyrelu', 'batch_size': 32768, 'dropout': 0.3, 'learning_rate': 0.001, 'use_autoenc': 'encoder', 'weight_decay': 1e-05, 'epochs': 1000}\n",
      "Epoch(0) - Training Loss: 0.7266\n",
      "Activation stats: No data returned for stats at layer 2\n",
      "Epoch(0) - Fold 0 - Validation Loss : 0.6923\n",
      "Epoch(0) - Fold 0 - Validation Accuracy : 0.5151\n",
      "Epoch(0) - Fold 0 - Validation Utility score : 542.0935\n",
      "Epoch(0) - Fold 1 - Validation Loss : 0.6935\n",
      "Epoch(0) - Fold 1 - Validation Accuracy : 0.5099\n",
      "Epoch(0) - Fold 1 - Validation Utility score : 397.8004\n",
      "Epoch(0) - Fold 2 - Validation Loss : 0.6941\n",
      "Epoch(0) - Fold 2 - Validation Accuracy : 0.5038\n",
      "Epoch(0) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 3 - Validation Loss : 0.6946\n",
      "Epoch(0) - Fold 3 - Validation Accuracy : 0.4978\n",
      "Epoch(0) - Fold 3 - Validation Utility score : -0.0000\n",
      "Epoch(0) - Fold 4 - Validation Loss : 0.6935\n",
      "Epoch(0) - Fold 4 - Validation Accuracy : 0.5049\n",
      "Epoch(0) - Fold 4 - Validation Utility score : 925.3737\n",
      "Epoch(0) - GLOBAL - Validation Loss: 0.6936\n",
      "Epoch(0) - GLOBAL - Validation Accuracy: 0.5063\n",
      "Epoch(0) - GLOBAL - Validation Utility score: 1865.2675\n",
      "Saving model corresponding to last_utility_score == 1865.2675435438941\n",
      "\n",
      "\n",
      "Epoch(1) - Training Loss: 0.7033\n",
      "Activation stats: No data returned for stats at layer 2\n",
      "Epoch(1) - Fold 0 - Validation Loss : 0.6926\n",
      "Epoch(1) - Fold 0 - Validation Accuracy : 0.5116\n",
      "Epoch(1) - Fold 0 - Validation Utility score : 218.7327\n",
      "Epoch(1) - Fold 1 - Validation Loss : 0.6934\n",
      "Epoch(1) - Fold 1 - Validation Accuracy : 0.5012\n",
      "Epoch(1) - Fold 1 - Validation Utility score : 166.8074\n",
      "Epoch(1) - Fold 2 - Validation Loss : 0.6945\n",
      "Epoch(1) - Fold 2 - Validation Accuracy : 0.5030\n",
      "Epoch(1) - Fold 2 - Validation Utility score : -0.0000\n",
      "Epoch(1) - Fold 3 - Validation Loss : 0.6938\n",
      "Epoch(1) - Fold 3 - Validation Accuracy : 0.5032\n",
      "Epoch(1) - Fold 3 - Validation Utility score : 16.3733\n",
      "Epoch(1) - Fold 4 - Validation Loss : 0.6930\n",
      "Epoch(1) - Fold 4 - Validation Accuracy : 0.5095\n",
      "Epoch(1) - Fold 4 - Validation Utility score : 341.7108\n",
      "Epoch(1) - GLOBAL - Validation Loss: 0.6934\n",
      "Epoch(1) - GLOBAL - Validation Accuracy: 0.5057\n",
      "Epoch(1) - GLOBAL - Validation Utility score: 743.6243\n",
      "Saving model corresponding to last_utility_score == 743.6242741865531\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (DO_SWEEP == True):\n",
    "    wandb.agent(sweep_id, function=train, project=\"janestreet-mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">resplendent-romance-53</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/fboyer/janestreet-mlp/runs/2xb62n0v\" target=\"_blank\">https://wandb.ai/fboyer/janestreet-mlp/runs/2xb62n0v</a><br/>\n",
       "                Run data is saved locally in <code>/home/francois/coding/OC/PJ9/wandb/run-20210214_235027-2xb62n0v</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Run config :\n",
      "config: {'epochs': 1000, 'batch_size': 8192, 'learning_rate': 0.0001, 'weight_decay': 0.0001, 'dropout': 0.7, 'use_autoenc': 'encoder'}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"<ipython-input-72-6fede0deb223>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    train()\n",
      "  File \u001b[1;32m\"<ipython-input-70-6a61a2afe382>\"\u001b[0m, line \u001b[1;32m219\u001b[0m, in \u001b[1;35mtrain\u001b[0m\n    outputs = model(inputs)\n",
      "  File \u001b[1;32m\"/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[1;32m727\u001b[0m, in \u001b[1;35m_call_impl\u001b[0m\n    result = self.forward(*input, **kwargs)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-70-6a61a2afe382>\"\u001b[0;36m, line \u001b[0;32m135\u001b[0;36m, in \u001b[0;35mforward\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.act_n = torch.cat((x, self.act_n[:, :-1].expand(x.shape[0], ACT_N_SIZE)), dim=1)\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m\u001b[0;31m:\u001b[0m too many indices for tensor of dimension 1\n"
     ]
    }
   ],
   "source": [
    "if (DO_SINGLE_TRAIN == True):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-70d616b67f89>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    inputs.shape\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m\u001b[0;31m:\u001b[0m name 'inputs' is not defined\n"
     ]
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2579 with shuffle and RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.onnx.export(model, batch, MODEL_FILE_ONNX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note utility score précédent :  \n",
    "\n",
    "{'utility_score': 2699.3290911247423, 'utility_scores': [433.21587581983806, 842.8579165327393, -0.0, -0.0, 1423.255298772165], 'utility_score_std': 541.5654345633293, 'accuracy_scores': [0.5243157432212159, 0.5228992628992629, 0.5123754728769456, 0.5104606336878834, 0.5277785603606142]}\n",
    "\n",
    "(avec std scale)\n",
    "\n",
    "\n",
    "Training summary:\n",
    "{'utility_score': 2697.374406045479, 'utility_scores': [448.22515142892547, 938.181355255796, -0.0, -0.0, 1310.9678993607574], 'utility_score_std': 518.5674763422022, 'accuracy_scores': [0.5212753894345934, 0.5206107406107406, 0.5085924573123425, 0.5062820079914457, 0.525750105648683]}\n",
    "\n",
    "\n",
    "Essayer : \n",
    "> avec features supplémentaires  \n",
    "> augmenter le dropout de la couche avec bcp de 0, ou supprimer la couche  \n",
    "> bouger le weight decay : essayer 1e-5, et 1e-3  \n",
    "> augmenter la taille du batch  \n",
    "> label smoothing   (voir loss_fn = SmoothBCEwLogits(smoothing=0.005)  dans janestreet_kaggle....)  \n",
    "> différents triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_load = nn.Sequential(\n",
    "        #nn.Dropout(0.2),\n",
    "        nn.Linear(len(FEATURES_LIST_TOTRAIN), 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(130, 130),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "\n",
    "        nn.Linear(130, 60),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "    \n",
    "        nn.Linear(60, 30),\n",
    "        #nn.BatchNorm1d(130),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),    \n",
    "       \n",
    "        nn.Linear(30, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ).double().to('cuda')\n",
    "    \n",
    "model_load.load_state_dict(torch.load(f'model_NN_allfolds_V1.pt',map_location=torch.device('cuda')))\n",
    "'''\n",
    "\n",
    "#model_load.eval()\n",
    "#print(accuracy_score(ts_test_y.cpu().numpy(), (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n",
    "#\n",
    "#model_load.eval()\n",
    "#print(utility_function(df.loc[test_index], (model_load(ts_test).squeeze() > 0.5).cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, fill NA with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009838564545944745,\n",
       " 0.38557755173112973,\n",
       " 0.35768747744650975,\n",
       " 0.00891916614596665,\n",
       " 0.0041500560373424495,\n",
       " -0.0037146189993207766,\n",
       " -0.012589244366156346,\n",
       " 0.051776552018932685,\n",
       " 0.026828095990947754,\n",
       " 0.24881331937245502,\n",
       " 0.18234851148590248,\n",
       " 0.08912156181298928,\n",
       " 0.049485535154017296,\n",
       " 0.14310535183278583,\n",
       " 0.08902722352210106,\n",
       " 0.21167757450938102,\n",
       " 0.146300650876364,\n",
       " 0.12121931699105562,\n",
       " 0.11358210894993667,\n",
       " 0.2938148492026861,\n",
       " 0.26876788737703755,\n",
       " 0.18691131282167164,\n",
       " 0.1769785779830002,\n",
       " 0.25244128771902047,\n",
       " 0.23856075429165213,\n",
       " 0.29407078502434514,\n",
       " 0.273177703966479,\n",
       " 0.13548298050171867,\n",
       " 0.16087630644126466,\n",
       " 0.32189235718153003,\n",
       " 0.3425343272966006,\n",
       " 0.2205604165416505,\n",
       " 0.25013120792951216,\n",
       " 0.3082216783372597,\n",
       " 0.3353533199754549,\n",
       " 0.34145307300650396,\n",
       " 0.36582532760649067,\n",
       " 0.029320465264380657,\n",
       " 0.02289177995103487,\n",
       " 0.04002162079212139,\n",
       " 0.05074972651124518,\n",
       " 0.4450543980970518,\n",
       " 0.36018357114469624,\n",
       " 0.34602868865463743,\n",
       " 0.4115306048129169,\n",
       " 0.43803102237933605,\n",
       " 0.47611584684954844,\n",
       " 0.3478667314970123,\n",
       " 0.4996310121501766,\n",
       " 0.5640008775247856,\n",
       " 0.5122603244317466,\n",
       " 0.45738658177408503,\n",
       " 0.04574377016024373,\n",
       " 0.36269998611838594,\n",
       " 0.35886983636928554,\n",
       " 0.652597182609052,\n",
       " 0.8049459750150331,\n",
       " 0.6613497470714732,\n",
       " 0.679812464521779,\n",
       " 0.7625897106372482,\n",
       " 0.5563956951455176,\n",
       " 0.5581652498654769,\n",
       " 0.5455413826521486,\n",
       " 0.546778335577234,\n",
       " 0.43505876086903456,\n",
       " 0.6075653248439333,\n",
       " 0.6085039517279847,\n",
       " 0.5951947177318683,\n",
       " 0.5959424881601522,\n",
       " 0.36954104040881697,\n",
       " 0.24337091134397526,\n",
       " 0.33227354399541986,\n",
       " 0.005393298121375514,\n",
       " -0.03286771852370292,\n",
       " -0.00020445421701550574,\n",
       " -0.019091946690617916,\n",
       " -0.031898281803949866,\n",
       " -0.07680002772370834,\n",
       " -0.00605952373252944,\n",
       " -0.035434597323818526,\n",
       " -0.002099458966825259,\n",
       " -0.014418274525843731,\n",
       " -0.03461507559647192,\n",
       " -0.0800853052057879,\n",
       " 0.3982158398248166,\n",
       " 0.5578243392749649,\n",
       " 0.4024042638598482,\n",
       " 0.44445105036718896,\n",
       " 0.5140947514539359,\n",
       " 0.40051646364922716,\n",
       " 0.41025294617564356,\n",
       " 0.5205121351461534,\n",
       " 0.4050807138243217,\n",
       " 0.4088298932713245,\n",
       " 0.42889387778006083,\n",
       " 0.41763183586073954,\n",
       " 0.40226847198530646,\n",
       " 0.5590945852445534,\n",
       " 0.40710700821605167,\n",
       " 0.4368573700084827,\n",
       " 0.5001226990638947,\n",
       " 0.40856460238921344,\n",
       " 0.40506301819893675,\n",
       " 0.48140573756748783,\n",
       " 0.40165573205168165,\n",
       " 0.40706131905131204,\n",
       " 0.4530453026311098,\n",
       " 0.41501192155955596,\n",
       " 0.39999263512557864,\n",
       " 0.4165405195231404,\n",
       " 0.4007356675614491,\n",
       " 0.40687807118570485,\n",
       " 0.41228491406734663,\n",
       " 0.4026373333613518,\n",
       " 0.40711205681095564,\n",
       " 0.37341742416551793,\n",
       " 0.40443262685246384,\n",
       " 0.40103337181422305,\n",
       " 0.38581705373929415,\n",
       " 0.41559987309867774,\n",
       " 0.33512703226889273,\n",
       " 0.2687757036832287,\n",
       " 0.34355231662463237,\n",
       " 0.27999728178893696,\n",
       " 0.33515369461396705,\n",
       " 0.24487524464105123,\n",
       " 0.3391777949954427,\n",
       " 0.2323808666181667,\n",
       " 0.3425608266217579,\n",
       " 0.24561818215100586]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, normalize with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0088,  0.3957,  0.3306,  0.0092,  0.0034, -0.0050, -0.0146,  0.0553,\n",
       "         0.0251,  0.2647,  0.1671,  0.0949,  0.0445,  0.1525,  0.0800,  0.2217,\n",
       "         0.1283,  0.1218,  0.1096,  0.2977,  0.2646,  0.1881,  0.1725,  0.2547,\n",
       "         0.2327,  0.2979,  0.2685,  0.1399,  0.1629,  0.3306,  0.3439,  0.2268,\n",
       "         0.2519,  0.3164,  0.3360,  0.3528,  0.3677,  0.0265,  0.0186,  0.0432,\n",
       "         0.0530,  0.4542,  0.3776,  0.4162,  0.4393,  0.4865,  0.4921,  0.3684,\n",
       "         0.5014,  0.5438,  0.5307,  0.4567,  0.0565,  0.3890,  0.3769,  0.7755,\n",
       "         0.9247,  0.7859,  0.8085,  0.8990,  0.5534,  0.5555,  0.5592,  0.5614,\n",
       "         0.4423,  0.6188,  0.6172,  0.5977,  0.5981,  0.3774,  0.2389,  0.3080,\n",
       "         0.0041, -0.0322, -0.0016, -0.0199, -0.0316, -0.0932, -0.0081, -0.0358,\n",
       "        -0.0025, -0.0149, -0.0350, -0.1015,  0.3934,  0.5416,  0.3924,  0.4281,\n",
       "         0.4976,  0.3994,  0.4332,  0.5235,  0.4224,  0.4221,  0.4348,  0.4547,\n",
       "         0.3984,  0.5422,  0.3973,  0.4259,  0.4865,  0.4105,  0.4340,  0.4839,\n",
       "         0.4168,  0.4198,  0.4614,  0.4553,  0.3950,  0.3824,  0.3900,  0.3918,\n",
       "         0.3807,  0.4035,  0.4320,  0.3767,  0.4279,  0.4207,  0.3952,  0.4465,\n",
       "         0.3616,  0.2999,  0.3709,  0.3035,  0.3601,  0.2778,  0.3743,  0.2571,\n",
       "         0.3739,  0.2682], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  2.5724,  2.4543,  1.9501,  1.7327,  1.7503,  1.6707,  1.6297,\n",
       "         1.8147,  2.4146,  1.7088,  1.6546,  2.3538,  2.2777,  2.0171,  1.8925,\n",
       "         2.2236,  1.5821,  1.9931,  1.7196,  1.9315,  2.4339,  1.7864,  2.1404,\n",
       "         2.6117,  2.0852,  2.2918,  1.4115,  1.8827,  1.7403,  2.0817,  1.6850,\n",
       "         2.4817,  1.9481,  2.0704,  2.4316,  2.2853,  2.0566,  2.1225,  1.6162,\n",
       "         2.3138,  1.9912,  2.4127,  2.3259,  2.7887,  1.9650,  2.8462,  2.2005,\n",
       "         3.0387,  3.5713,  3.7193,  2.8348,  1.8877,  2.2007,  1.9352,  7.1295,\n",
       "        11.0556,  7.5957,  8.1579,  9.9465,  2.2164,  1.9947,  2.1712,  2.2567,\n",
       "         2.3735,  2.1915,  1.7553,  2.5883,  2.5263,  2.2991,  2.4406,  1.8076,\n",
       "         1.8125,  2.1476,  1.7792,  1.9738,  2.2283,  2.6795,  2.1807,  1.7949,\n",
       "         1.7688,  2.2805,  1.9767,  2.4821,  1.9748,  2.4948,  1.9679,  2.6387,\n",
       "         2.5943,  2.6160,  2.0864,  2.6942,  2.0670,  2.2104,  2.1212,  2.5054,\n",
       "         2.1243,  2.5619,  2.3017,  2.1175,  2.2689,  2.6136,  2.3357,  2.2557,\n",
       "         1.8389,  2.0931,  2.7896,  2.4935,  2.4409,  2.5761,  1.9721,  1.9469,\n",
       "         2.6040,  2.8786,  2.1073,  2.3353,  2.3193,  2.5076,  2.4064,  2.0617,\n",
       "         1.9008,  2.1498,  2.1102,  1.9852,  1.7631,  2.2454,  2.5548,  1.8003,\n",
       "         2.3295,  1.7714], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_train_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
