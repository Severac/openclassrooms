{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch classifier notebook\n",
    "\n",
    "V1 : only 1 split. First implementation  \n",
    "All folds V1 : with all folds  \n",
    "All folds V2 : add activation stats plot  \n",
    "All folds V2.1 : back to  best MLP found so far, and backport fix of activation layers stats. Add weight decay and scheduler (fit one cycle) code\n",
    "\n",
    "All folds autoencoder MLP V1  \n",
    "All folds autoencoder MLP V2 : with weights and biases  \n",
    "All folds autoencoder MLP V3 : replace MLP with xgboost\n",
    "All folds XGB resp N1 fold predict: start from code of V3 but without NN code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import io\n",
    "import PIL.Image\n",
    "\n",
    "import datetime\n",
    "\n",
    "import faiss\n",
    "\n",
    "DATASET_INPUT_FILE = 'train.csv'\n",
    "\n",
    "#FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)] + ['cross_41_42_43', 'cross_1_2']\n",
    "FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)]\n",
    "\n",
    "# For custom non-overlaped folds generation\n",
    "TRAIN_PERCENT = 0.70  \n",
    "TEST_PERCENT = 0.30\n",
    "\n",
    "# If subsplit of training set : percentage of second training set  \n",
    "TRAIN1_PERCENT = 0.20  \n",
    "\n",
    "ACT_N = False  # Add N previous predictions to input of MLP <= Does not work, logic is not right\n",
    "ACT_N_SIZE = 5\n",
    "\n",
    "CLUSTERING = False\n",
    "\n",
    "MODEL_FILE_META = 'model_XGB_meta_respn1_fold.bin'\n",
    "MODEL_FILE_RESPN1 = 'model_XGB_respn1.bin'\n",
    "MODEL_FILE_FOLD = 'model_XGB_fold.bin'\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Behavior\n",
    "seed = 42\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'Best utility',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [524288, 262144, 131072, 65536, 32768, 16384, 8192, 4096, 2048, 1024, 512]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            #'values': [1e-2, 1e-3, 1e-4, 3e-4, 1e-5]\n",
    "            #'values': [1e-2, 1e-3, 1e-4]\n",
    "            'values': [1e-2, 1e-3]\n",
    "        },\n",
    "\n",
    "        'weight_decay': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        },\n",
    "    \n",
    "        'use_autoenc': {\n",
    "            'values': ['encoder-decoder', 'encoder', 'encoder-only', 'None']\n",
    "            #'values': ['encoder-decoder', 'None']\n",
    "        },\n",
    "        \n",
    "        'activation_function': {\n",
    "            'values': ['relu', 'leakyrelu']\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'Best utility',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'min': 4096,\n",
    "            'max': 65536,\n",
    "            'distribution': 'int_uniform',\n",
    "        },\n",
    "        'dropout': {\n",
    "            'min': 0.3,\n",
    "            'max': 0.5,\n",
    "            'distribution': 'uniform',\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 0.0005,\n",
    "            'max': 0.002,\n",
    "            'distribution': 'uniform',\n",
    "        },\n",
    "\n",
    "        'weight_decay': {\n",
    "            'min': 0.00001,\n",
    "            'max': 0.0002,\n",
    "            'distribution': 'uniform',\n",
    "\n",
    "        },\n",
    "    \n",
    "        'use_autoenc': {\n",
    "            'values': ['encoder', 'encoder-only']\n",
    "            #'values': ['encoder-decoder', 'None']\n",
    "        },\n",
    "        \n",
    "        'activation_function': {\n",
    "            'values': ['leakyrelu']\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyStandardScale(tensor, mean, std):\n",
    "    return((tensor - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# this is code slightly modified from the sklearn docs here:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cv_indices_custom(cv_custom, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv_custom):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accounts for variable instance counts in each split by dividing utility_pi by number of instances (but this has been removed)\n",
    "# It also does some copy of dataframe to prevent memory overwrite\n",
    "def utility_function(df_test, df_test_predictions):\n",
    "    df_test_copy = df_test.copy(deep=True)\n",
    "    df_test_copy.loc[:, 'utility_pj'] = df_test_copy['weight'] * df_test_copy['resp'] * df_test_predictions\n",
    "    #df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum() / df_test_copy.groupby('date')['utility_pj'].count()\n",
    "    df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum()\n",
    "\n",
    "    nb_unique_dates = df_test_utility_pi.shape[0]\n",
    "\n",
    "    if (np.sqrt(df_test_utility_pi.pow(2).sum()) == 0):\n",
    "        t = 0\n",
    "\n",
    "    else:\n",
    "        t = (df_test_utility_pi.sum() / np.sqrt(df_test_utility_pi.pow(2).sum())) * (np.sqrt(250 / np.abs(nb_unique_dates)))\n",
    "\n",
    "    u = min(max(t, 0), 6) * df_test_utility_pi.sum()\n",
    "    del df_test_copy\n",
    "    \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "# The aim of this function is to return closest date from an index\n",
    "# So that split indices correspond to start or end of a new day\n",
    "# myList contains list of instances that correspond to start of a new da\n",
    "\n",
    "def take_closest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutputActivationStats:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        #self.outputs.append(module_out)\n",
    "        #print('Save output callback :')\n",
    "        #print(module)\n",
    "        #print({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        self.outputs.append({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#\n",
    "#plot_cv_indices(cv, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "    \n",
    "df = pd.read_csv(DATASET_INPUT_FILE)\n",
    "df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_41_42_43'] = df['feature_41'] + df['feature_42'] + df['feature_43']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_1_2'] = df['feature_1'] / (df['feature_2'] + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non overlap fold generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_indexes_list = df.groupby('date')['ts_id'].first().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_split_size = int((df.shape[0] // 5) * TRAIN_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_split_size = int((df.shape[0] // 5) * TEST_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 477711, 958233, 1435933, 1913985]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_start_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have 5 folds of 3 subsets each (2 training sets and 1 test set per fold)\n",
    "# (1st training set of each fold will be used for 1st model, ie auto encoder)\n",
    "\n",
    "NB_FOLDS = 5\n",
    "last_index = df.shape[0] - 1\n",
    "\n",
    "cv_table = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_train_start_index = train_split_start_indexes[fold_indice]\n",
    "    \n",
    "    if (fold_indice == NB_FOLDS - 1):    \n",
    "        nextfold_train_start_index = last_index\n",
    "        \n",
    "    else:\n",
    "        nextfold_train_start_index = train_split_start_indexes[fold_indice + 1]\n",
    "    \n",
    "    fold_test_start_index = take_closest(date_indexes_list, int(TRAIN_PERCENT * (nextfold_train_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    fold_train2_start_index = take_closest(date_indexes_list, int(TRAIN1_PERCENT * (fold_test_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    \n",
    "    cv_table.append(fold_train_start_index)\n",
    "    cv_table.append(fold_train2_start_index)\n",
    "    cv_table.append(fold_test_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_table.append(last_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples = []\n",
    "\n",
    "for i in range(0, NB_FOLDS*3, 3):\n",
    "    cv_tuples.append([df.loc[cv_table[i]:cv_table[i+1]-1, :].index.to_list(), df.loc[cv_table[i+1]:cv_table[i+2]-1, :].index.to_list(),\n",
    "                      df.loc[cv_table[i+2]:cv_table[i+3]-1, :].index.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141102"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_tuples[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#plot_cv_indices_custom(cv_tuples_generator, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20); \n",
    "\n",
    "#cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of training set :\n",
    "#train_sets_table =  [cv_tuples[i][0] for i in range(5)]\n",
    "#sum([len(train_set_table) for train_set_table in train_sets_table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our old time series split (with overlap : required 1 neural network trained per split)\n",
    "# But in this script it's not needed because we're training 1 unique network, with a different fold strategy (non overlaped)\n",
    "#cv = PurgedGroupTimeSeriesSplit(\n",
    "#    n_splits=5,\n",
    "#    max_train_group_size=180,\n",
    "#    group_gap=20,\n",
    "#    max_test_group_size=60\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index, test_index = next(cv.split(df, (df['resp'] > 0)*1, df['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.loc[train_index, 'resp'] > 0).astype(np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_mean = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No fill na with our XGB models.\n",
    "# df.fillna(f_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list = []\n",
    "\n",
    "for fold, (train1_index, train2_index, test_index) in enumerate(cv_tuples_generator):\n",
    "    folds_list.append((train1_index, train2_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_train1 = [folds_list[i][0] for i in range(5)]\n",
    "folds_list_train1_flat = [folds_list_train1_item for sublist in folds_list_train1 for folds_list_train1_item in sublist]\n",
    "folds_list_train1_unique = list(set(folds_list_train1_flat))\n",
    "\n",
    "folds_list_train2 = [folds_list[i][1] for i in range(5)]\n",
    "folds_list_train2_flat = [folds_list_train2_item for sublist in folds_list_train2 for folds_list_train2_item in sublist]\n",
    "folds_list_train2_unique = list(set(folds_list_train2_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train2_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train1_item) for folds_list_train1_item in folds_list_train1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train2_item) for folds_list_train2_item in folds_list_train2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_test = [folds_list[i][2] for i in range(5)]\n",
    "folds_list_test_flat = [folds_list_test_item for sublist in folds_list_test for folds_list_test_item in sublist]\n",
    "folds_list_test_unique = set(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_test_item) for folds_list_test_item in folds_list_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat) + len(folds_list_train2_flat) + len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141980, 130)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[folds_list_test[4], FEATURES_LIST_TOTRAIN].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00880718,  0.39574469,  0.33059838,         nan,         nan,\n",
       "       -0.00498373, -0.01455459,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,  0.02650339,  0.0186391 ,  0.04320553,\n",
       "        0.05298663,  0.45417433,  0.37762691,  0.41617323,         nan,\n",
       "               nan,  0.49207956,  0.36839975,  0.50144387,  0.54379067,\n",
       "        0.53074971,  0.45673965,  0.05646874,  0.38900233,  0.37690587,\n",
       "               nan,         nan,  0.78590429,         nan,         nan,\n",
       "        0.55335406,  0.55554392,  0.55922873,  0.56139559,  0.44231975,\n",
       "        0.61884351,  0.61715568,  0.59770334,  0.59814018,  0.37738388,\n",
       "        0.23893403,  0.30802914,         nan,         nan,         nan,\n",
       "               nan,         nan, -0.0931838 ,         nan,         nan,\n",
       "               nan,         nan,         nan, -0.10154564,         nan,\n",
       "               nan,         nan,         nan,         nan,  0.39935045,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "        0.4547188 ,         nan,         nan,         nan,         nan,\n",
       "               nan,  0.41054099,         nan,         nan,         nan,\n",
       "               nan,         nan,  0.455339  ,         nan,         nan,\n",
       "               nan,         nan,         nan,  0.40349802,         nan,\n",
       "               nan,         nan,         nan,         nan,  0.44651147,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(folds_list_train1_unique + folds_list_train2_unique), FEATURES_LIST_TOTRAIN].to_numpy().mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677155"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(folds_list_train1_unique + folds_list_train2_unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training XGB model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XGB model that predicts resp n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifier_wrapper(BaseEstimator, ClassifierMixin):  \n",
    "    ''' Params passed as dictionnary to __init__, for example :\n",
    "        params_space = {\n",
    "       'features': FEATURES_LIST_TOTRAIN, \n",
    "        'random_state': 42,\n",
    "        'max_depth': 12,\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': 0.01,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.3,\n",
    "        'tree_method': 'gpu_hist'\n",
    "        }\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        self.fitted = False\n",
    "        \n",
    "        self.features = list(params['features'])\n",
    "        self.random_state = params['random_state']\n",
    "        self.max_depth = params['max_depth']\n",
    "        self.n_estimators = params['n_estimators']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.subsample = params['subsample']\n",
    "        self.colsample_bytree = params['colsample_bytree']\n",
    "        self.gamma = params['gamma']\n",
    "        self.tree_method = params['tree_method']  \n",
    "        \n",
    "        #print('Features assigned :')\n",
    "        #print(self.features)\n",
    "\n",
    "        self.model_internal = XGBClassifier(\n",
    "            random_state= self.random_state,\n",
    "            max_depth= self.max_depth,\n",
    "            n_estimators= self.n_estimators,\n",
    "            learning_rate= self.learning_rate,\n",
    "            subsample= self.subsample,\n",
    "            colsample_bytree= self.colsample_bytree,\n",
    "            tree_method= self.tree_method,\n",
    "            gamma = self.gamma,\n",
    "            #objective= 'binary:logistic',\n",
    "            #disable_default_eval_metric=True,\n",
    "            )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('Model used for fitting:')\n",
    "        print(self.model_internal)\n",
    "        self.model_internal.fit(X[self.features], y)\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        if (self.fitted == True):\n",
    "            print('predict called')\n",
    "            return(self.model_internal.predict(X[self.features]))\n",
    "        \n",
    "        else:\n",
    "            print('You must fit model first')\n",
    "            return(None)\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        if (self.fitted == True):\n",
    "            print('predict proba called')\n",
    "            return(self.model_internal.predict_proba(X[self.features]))\n",
    "        \n",
    "        else:\n",
    "            print('You must fit model first')\n",
    "            return(None)\n",
    "        \n",
    "\n",
    "    #def set_params(self, **parameters):\n",
    "    #    for parameter, value in parameters.items():\n",
    "    #        setattr(self, parameter, value)\n",
    "\n",
    "        \n",
    "    def score(self, X, y=None):        \n",
    "        print('Type of X:')\n",
    "        print(type(X))\n",
    "        \n",
    "        print('Shape of X:')\n",
    "        print(X.shape)\n",
    "        \n",
    "        print('Type of y:')\n",
    "        print(type(y))\n",
    "        \n",
    "        print('model fitted ?')\n",
    "        print(self.fitted) # Usually returns yes at this point when called by cross_val_score\n",
    "        \n",
    "        if y is None:\n",
    "            print('y is None')\n",
    "            y_preds = pd.Series(self.model_internal.predict(X.reset_index(drop=True)[self.features]))\n",
    "            \n",
    "        else: # cross_val_score goes there\n",
    "            print('y is not None')\n",
    "            y_preds = pd.Series(y)\n",
    "        \n",
    "        return(utility_function(X.reset_index(drop=True), y_preds)) \n",
    "    \n",
    "    def accuracy_score(self, X, y=None):\n",
    "        if y is None:\n",
    "            print('y is None in accuracy_score method : pass predictions as y to avoid launching predict')\n",
    "            y_preds = pd.Series(self.model_internal.predict(X.reset_index(drop=True)[self.features]))\n",
    "            \n",
    "        else: # cross_val_score goes there\n",
    "            #print('y is not None')\n",
    "            y_preds = pd.Series(y)\n",
    "            \n",
    "        return(accuracy_score(X['resp_positive'], y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:22:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.9,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate label of current step\n",
    "y_train1_resp_positive = (df.loc[folds_list_train1_unique, 'resp'] > 0).astype(np.byte)\n",
    "\n",
    "# Shift values of resp to get resp of step n-1\n",
    "y_train1_resp_n1_positive = y_train1_resp_positive.shift(1, fill_value=0)\n",
    "\n",
    "\n",
    "model_n1 = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 12,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.01,\n",
    "    subsample= 0.9,\n",
    "    colsample_bytree= 0.2,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    )\n",
    "\n",
    "model_n1.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], y_train1_resp_n1_positive, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model that predicts original fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_indexes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_indexes.append([item for sublist in folds_list[fold_indice] for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_number, fold_indexes_1fold in enumerate(fold_indexes):\n",
    "    df.loc[fold_indexes_1fold, 'fold_number'] = str(int(fold_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.shape[0] - 1, 'fold_number'] = str(int(NB_FOLDS - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    480522\n",
       "3    478052\n",
       "0    477711\n",
       "2    477700\n",
       "4    476506\n",
       "Name: fold_number, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fold_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 140)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(df['fold_number'], prefix = 'fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df, pd.get_dummies(df['fold_number'], prefix = 'fold')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=['fold_number'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fold predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:23:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 10,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.02,\n",
    "    subsample= 0.5,\n",
    "    colsample_bytree= 0.6,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    #objective= 'binary:logistic',\n",
    "    #disable_default_eval_metric=True,\n",
    "    )\n",
    "\n",
    "#model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, ['fold_'+str(i) for i in range(NB_FOLDS)]])\n",
    "model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, 'fold_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    142450\n",
       "Name: fold_number, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[folds_list_test[1]]['fold_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    38608\n",
       "1    31708\n",
       "3    25560\n",
       "2    23863\n",
       "4    21363\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[0], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    35380\n",
       "3    34525\n",
       "4    28870\n",
       "1    27587\n",
       "0    16088\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[1], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    43197\n",
       "3    41715\n",
       "2    30115\n",
       "1    15469\n",
       "0    15155\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[2], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    55001\n",
       "3    42224\n",
       "2    20747\n",
       "1    12120\n",
       "0    12060\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[3], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    56798\n",
       "3    38220\n",
       "2    19590\n",
       "0    13698\n",
       "1    13674\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[4], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03349742, 0.44114932, 0.2217906 , 0.24658968, 0.05697299],\n",
       "       [0.2493427 , 0.6202549 , 0.02858509, 0.032284  , 0.06953336],\n",
       "       [0.12408023, 0.64893204, 0.07998472, 0.05955048, 0.08745254],\n",
       "       ...,\n",
       "       [0.32993764, 0.14755715, 0.17608246, 0.1451546 , 0.20126821],\n",
       "       [0.15299153, 0.11114156, 0.28277832, 0.28438374, 0.16870484],\n",
       "       [0.06557676, 0.4378262 , 0.22874515, 0.11568245, 0.15216942]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.predict_proba(df.loc[folds_list_test[1], FEATURES_LIST_TOTRAIN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_scores': [0.27361766665249254, 0.19366093366093365, 0.20676136792744298, 0.29703416061680454, 0.40004225947316524]}\n",
      "{'precision_scores': [0.27361766665249254, 0.19366093366093365, 0.20676136792744298, 0.29703416061680454, 0.40004225947316524]}\n",
      "{'recall_scores': [0.27361766665249254, 0.19366093366093365, 0.20676136792744298, 0.29703416061680454, 0.40004225947316524]}\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS): \n",
    "    test_predictions = model_xgb.predict(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN])\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions))  \n",
    "    precision_scores.append(precision_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions, average='micro'))  \n",
    "    recall_scores.append(recall_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions, average='micro'))  \n",
    "\n",
    "    df_featimportance = pd.DataFrame(model_xgb.feature_importances_, index=df[FEATURES_LIST_TOTRAIN].columns, columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "    df_featimportance_cumulated = pd.concat([df_featimportance, pd.DataFrame({'% feat importance cumulÃ©' : (df_featimportance['Importance'] / df_featimportance['Importance'].sum()).cumsum()})], axis=1)\n",
    "    #print(f'Feature importances for split {fold_indice}:')\n",
    "    #print(df_featimportance_cumulated)\n",
    "\n",
    "print({'accuracy_scores': accuracy_scores})\n",
    "print({'precision_scores': precision_scores})\n",
    "print({'recall_scores': recall_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 140)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost with fold prediction as input AND resp n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapped = XGBClassifier_wrapper({\n",
    "   #'features': ['feature_'+str(i) for i in range(130)] + [0,1,2,3,4] + ['resp_n1_predict'], \n",
    "    'features': ['feature_'+str(i) for i in range(130)] + [0,3,4] + ['resp_n1_predict'], \n",
    "    'random_state': 42,\n",
    "    'max_depth': 10,\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.02,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'gamma': None,\n",
    "    'tree_method': 'gpu_hist'        \n",
    "    #'tree_method': 'hist' # CPU\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [df, \n",
    "     pd.DataFrame(model_xgb.predict_proba(df.loc[:, FEATURES_LIST_TOTRAIN]))], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 145)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'resp_n1_predict'] = model_n1.predict_proba(df.loc[:, FEATURES_LIST_TOTRAIN])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for fitting:\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.6, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.02, max_delta_step=None, max_depth=10,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=42, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=0.5, tree_method='gpu_hist',\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:26:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier_wrapper(params=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapped.fit(\n",
    "    df.loc[folds_list_train2_unique], \n",
    "    (df.loc[folds_list_train2_unique]['resp'] > 0).astype(np.byte)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(141102, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(142450, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(145651, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(142152, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(141980, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "{'utility_score': 2201.1077946132314, 'utility_scores': [282.6849057991371, 660.7564349123575, -0.0, 0.9151242560035899, 1256.7513296457332], 'utility_score_std': 474.754559654155, 'accuracy_scores': [0.5208714263440631, 0.5191154791154791, 0.5074802095419874, 0.5142171759806404, 0.5256655867023524]}\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "accuracy_scores = []\n",
    "xgb_test_predictions_folds = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):     \n",
    "    test_predictions = model_wrapped.predict(df.loc[folds_list_test[fold_indice], :])\n",
    "    test_predictions_probas = model_wrapped.predict_proba(df.loc[folds_list_test[fold_indice], :])[:, 1]\n",
    "    xgb_test_predictions_folds.append(test_predictions_probas)\n",
    "\n",
    "    scores.append(model_wrapped.score(df.loc[folds_list_test[fold_indice]], test_predictions))\n",
    "    accuracy_scores.append(model_wrapped.accuracy_score(df.loc[folds_list_test[fold_indice]], test_predictions))  \n",
    "\n",
    "    df_featimportance = pd.DataFrame(model_wrapped.model_internal.feature_importances_, index=FEATURES_LIST_TOTRAIN + [0,3,4] + ['resp_n1_predict'], columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "    df_featimportance_cumulated = pd.concat([df_featimportance, pd.DataFrame({'% feat importance cumulÃ©' : (df_featimportance['Importance'] / df_featimportance['Importance'].sum()).cumsum()})], axis=1)\n",
    "    #print(f'Feature importances for split {fold_indice}:')\n",
    "    #print(df_featimportance_cumulated)\n",
    "\n",
    "print({'utility_score': sum(scores), 'utility_scores': scores, 'utility_score_std': np.std(scores), 'accuracy_scores': accuracy_scores})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Score avec resp n-1 proba et fold prediction : \n",
    "{'utility_score': 2005.4261341927581, 'utility_scores': [185.71548536644758, 720.3905799549489, -0.0, 0.4066629524023048, 1098.9134059189594], 'utility_score_std': 437.4281363458974, 'accuracy_scores': [0.5191280066901958, 0.5175078975078975, 0.5103363519646278, 0.5155889470426023, 0.5262149598535005]}  \n",
    "\n",
    "Score mÃªme chose avec proba de resp n-1 :\n",
    "{'utility_score': 2006.7004942007125, 'utility_scores': [161.07432736133686, 828.7819592596372, -0.0, 1.3074578209567136, 1015.5367497587815], 'utility_score_std': 433.30628885366, 'accuracy_scores': [0.5194327507760343, 0.5173885573885574, 0.509134849743565, 0.515117620575159, 0.525764192139738]}\n",
    "\n",
    "Score mÃªme chose avec proba resp n-1 et fold prediction seulement 0, 3 et 4 (les plus fiables) :\n",
    "{'utility_score': 2201.1077946132314, 'utility_scores': [282.6849057991371, 660.7564349123575, -0.0, 0.9151242560035899, 1256.7513296457332], 'utility_score_std': 474.754559654155, 'accuracy_scores': [0.5208714263440631, 0.5191154791154791, 0.5074802095419874, 0.5142171759806404, 0.5256655867023524]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load my NN Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torch.optim as optim\n",
    "import torch_optimizer as optim  # Custom optimizers (not officially pytorch) : to use RAdam https://pypi.org/project/torch-optimizer/#radam\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_Resnet(\n",
       "  (batch_norm0): BatchNorm1d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout0): Dropout(p=0.2, inplace=False)\n",
       "  (dense1): Linear(in_features=130, out_features=134, bias=True)\n",
       "  (batch_norm1): BatchNorm1d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.4611, inplace=False)\n",
       "  (dense2): Linear(in_features=264, out_features=134, bias=True)\n",
       "  (batch_norm2): BatchNorm1d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.4611, inplace=False)\n",
       "  (dense3): Linear(in_features=268, out_features=134, bias=True)\n",
       "  (batch_norm3): BatchNorm1d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout3): Dropout(p=0.4611, inplace=False)\n",
       "  (dense4): Linear(in_features=268, out_features=134, bias=True)\n",
       "  (batch_norm4): BatchNorm1d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout4): Dropout(p=0.4611, inplace=False)\n",
       "  (dense5): Linear(in_features=268, out_features=1, bias=True)\n",
       "  (act5): Sigmoid()\n",
       "  (LeakyReLU1): LeakyReLU(negative_slope=0.01)\n",
       "  (LeakyReLU2): LeakyReLU(negative_slope=0.01)\n",
       "  (LeakyReLU3): LeakyReLU(negative_slope=0.01)\n",
       "  (LeakyReLU4): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_MLP_apricot = dict(\n",
    "    dropout = 0.4222,\n",
    "    activation_function = 'leakyrelu',\n",
    "    hidden_size = 309,\n",
    "    )\n",
    "\n",
    "config_MLP_celestial = dict(\n",
    "    dropout = 0.4611,\n",
    "    activation_function = 'leakyrelu',\n",
    "    hidden_size = 134,\n",
    "    )\n",
    "\n",
    "class Model_Resnet(nn.Module):\n",
    "    def __init__(self, config_MLP):\n",
    "        super(Model_Resnet, self).__init__()\n",
    "        self.batch_norm0 = nn.BatchNorm1d(len(FEATURES_LIST_TOTRAIN))\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "\n",
    "        dropout_rate = config_MLP['dropout']\n",
    "        hidden_size = config_MLP['hidden_size']\n",
    "        self.dense1 = nn.Linear(len(FEATURES_LIST_TOTRAIN), hidden_size)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.cat1 = lambda a,b : torch.cat([a, b], 1)\n",
    "\n",
    "        self.dense2 = nn.Linear(hidden_size+len(FEATURES_LIST_TOTRAIN), hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.cat2 = lambda a,b : torch.cat([a, b], 1)\n",
    "\n",
    "        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.cat3 = lambda a,b : torch.cat([a, b], 1)            \n",
    "\n",
    "        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.cat4 = lambda a,b : torch.cat([a, b], 1)            \n",
    "\n",
    "        self.dense5 = nn.Linear(hidden_size+hidden_size, 1)\n",
    "        self.act5 = nn.Sigmoid()\n",
    "\n",
    "        self.LeakyReLU1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.LeakyReLU2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.LeakyReLU3 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.LeakyReLU4 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout0(self.batch_norm0(x))\n",
    "\n",
    "        x1 = self.dropout1(self.LeakyReLU1(self.batch_norm1(self.dense1(x))))\n",
    "        x = self.cat1(x, x1)\n",
    "\n",
    "        x2 = self.dropout2(self.LeakyReLU2(self.batch_norm2(self.dense2(x))))\n",
    "        x = self.cat2(x1, x2)\n",
    "\n",
    "        x3 = self.dropout3(self.LeakyReLU3(self.batch_norm3(self.dense3(x))))\n",
    "        x = self.cat3(x2, x3)\n",
    "\n",
    "        x4 = self.dropout4(self.LeakyReLU4(self.batch_norm4(self.dense4(x))))\n",
    "\n",
    "        x = self.cat4(x3, x4)\n",
    "\n",
    "        x = self.act5(self.dense5(x))\n",
    "\n",
    "        return x        \n",
    "\n",
    "model_myNN_1 = Model_Resnet(config_MLP_apricot).double()\n",
    "model_myNN_2 = Model_Resnet(config_MLP_celestial).double()\n",
    "\n",
    "model_myNN_1.load_state_dict(torch.load('model_NN_allfolds_V1.pt.apricot-sweep-29',  map_location=torch.device('cuda')))\n",
    "model_myNN_1.eval()\n",
    "\n",
    "model_myNN_2.load_state_dict(torch.load('model_NN_allfolds_V1.pt.celestial-sweep-40',  map_location=torch.device('cuda')))\n",
    "model_myNN_2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate base models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filled for public models. df (without fill NA) for xgboost\n",
    "f_mean_df = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)\n",
    "df_filled = df.copy(deep=True)\n",
    "df_filled.fillna(f_mean_df, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336609</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.132142</td>\n",
       "      <td>-1.722265</td>\n",
       "      <td>0.139549</td>\n",
       "      <td>0.263441</td>\n",
       "      <td>-0.285683</td>\n",
       "      <td>-0.451095</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>-1.026340</td>\n",
       "      <td>-0.026965</td>\n",
       "      <td>0.089122</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>0.460824</td>\n",
       "      <td>1.423821</td>\n",
       "      <td>-1.321313</td>\n",
       "      <td>-1.520189</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>0.113582</td>\n",
       "      <td>0.711860</td>\n",
       "      <td>1.831054</td>\n",
       "      <td>0.186911</td>\n",
       "      <td>0.176979</td>\n",
       "      <td>1.591871</td>\n",
       "      <td>3.882418</td>\n",
       "      <td>0.831198</td>\n",
       "      <td>2.023286</td>\n",
       "      <td>0.135483</td>\n",
       "      <td>0.160876</td>\n",
       "      <td>-0.542242</td>\n",
       "      <td>-1.355774</td>\n",
       "      <td>0.220560</td>\n",
       "      <td>0.250131</td>\n",
       "      <td>-0.371036</td>\n",
       "      <td>-0.801558</td>\n",
       "      <td>-0.762771</td>\n",
       "      <td>-1.360442</td>\n",
       "      <td>0.494082</td>\n",
       "      <td>0.999206</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410253</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>0.405081</td>\n",
       "      <td>-0.355835</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>-3.742871</td>\n",
       "      <td>0.402268</td>\n",
       "      <td>0.068609</td>\n",
       "      <td>0.407107</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>-0.136181</td>\n",
       "      <td>-2.518476</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.280897</td>\n",
       "      <td>0.401656</td>\n",
       "      <td>0.741095</td>\n",
       "      <td>0.223319</td>\n",
       "      <td>-2.954312</td>\n",
       "      <td>0.399993</td>\n",
       "      <td>-1.848702</td>\n",
       "      <td>0.400736</td>\n",
       "      <td>-0.464594</td>\n",
       "      <td>-1.368804</td>\n",
       "      <td>-4.399766</td>\n",
       "      <td>0.407112</td>\n",
       "      <td>-1.120449</td>\n",
       "      <td>0.404433</td>\n",
       "      <td>0.378352</td>\n",
       "      <td>-0.221765</td>\n",
       "      <td>-2.340275</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>3.127101</td>\n",
       "      <td>1.047990</td>\n",
       "      <td>7.997623</td>\n",
       "      <td>1.422275</td>\n",
       "      <td>6.468159</td>\n",
       "      <td>0.587420</td>\n",
       "      <td>4.304226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336610</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.649220</td>\n",
       "      <td>1.384605</td>\n",
       "      <td>0.313669</td>\n",
       "      <td>0.378719</td>\n",
       "      <td>0.190328</td>\n",
       "      <td>0.243118</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>0.970846</td>\n",
       "      <td>1.477726</td>\n",
       "      <td>0.089122</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>0.229903</td>\n",
       "      <td>0.570816</td>\n",
       "      <td>-0.050573</td>\n",
       "      <td>0.497482</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>0.113582</td>\n",
       "      <td>-1.686178</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>0.186911</td>\n",
       "      <td>0.176979</td>\n",
       "      <td>-1.688478</td>\n",
       "      <td>-2.757282</td>\n",
       "      <td>-2.098536</td>\n",
       "      <td>-3.114046</td>\n",
       "      <td>0.135483</td>\n",
       "      <td>0.160876</td>\n",
       "      <td>1.627948</td>\n",
       "      <td>2.985653</td>\n",
       "      <td>0.220560</td>\n",
       "      <td>0.250131</td>\n",
       "      <td>1.904859</td>\n",
       "      <td>2.897747</td>\n",
       "      <td>2.618605</td>\n",
       "      <td>3.667457</td>\n",
       "      <td>-0.295169</td>\n",
       "      <td>-0.378538</td>\n",
       "      <td>0.293775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410253</td>\n",
       "      <td>-0.349451</td>\n",
       "      <td>0.405081</td>\n",
       "      <td>-1.082233</td>\n",
       "      <td>-0.482489</td>\n",
       "      <td>-4.574880</td>\n",
       "      <td>0.402268</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>0.407107</td>\n",
       "      <td>-0.669679</td>\n",
       "      <td>-0.572228</td>\n",
       "      <td>-2.936297</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>0.401656</td>\n",
       "      <td>-1.122232</td>\n",
       "      <td>-1.080657</td>\n",
       "      <td>-5.173614</td>\n",
       "      <td>0.399993</td>\n",
       "      <td>-1.065521</td>\n",
       "      <td>0.400736</td>\n",
       "      <td>-0.779230</td>\n",
       "      <td>-1.214024</td>\n",
       "      <td>-4.467923</td>\n",
       "      <td>0.407112</td>\n",
       "      <td>-2.355870</td>\n",
       "      <td>0.404433</td>\n",
       "      <td>-3.319975</td>\n",
       "      <td>-2.606388</td>\n",
       "      <td>-3.859116</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.644885</td>\n",
       "      <td>-0.447958</td>\n",
       "      <td>1.649931</td>\n",
       "      <td>1.845694</td>\n",
       "      <td>2.319056</td>\n",
       "      <td>1.313572</td>\n",
       "      <td>1.598569</td>\n",
       "      <td>0.558448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336611</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.397873</td>\n",
       "      <td>-0.832325</td>\n",
       "      <td>-0.402410</td>\n",
       "      <td>-0.540271</td>\n",
       "      <td>-0.678723</td>\n",
       "      <td>-1.028448</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>0.492441</td>\n",
       "      <td>1.513903</td>\n",
       "      <td>0.089122</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>-2.280709</td>\n",
       "      <td>-2.341846</td>\n",
       "      <td>-1.166688</td>\n",
       "      <td>-1.272732</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>0.113582</td>\n",
       "      <td>1.130229</td>\n",
       "      <td>2.549689</td>\n",
       "      <td>0.186911</td>\n",
       "      <td>0.176979</td>\n",
       "      <td>1.060111</td>\n",
       "      <td>2.485509</td>\n",
       "      <td>1.459011</td>\n",
       "      <td>3.097567</td>\n",
       "      <td>0.135483</td>\n",
       "      <td>0.160876</td>\n",
       "      <td>-0.534896</td>\n",
       "      <td>-1.223043</td>\n",
       "      <td>0.220560</td>\n",
       "      <td>0.250131</td>\n",
       "      <td>-2.382483</td>\n",
       "      <td>-3.565640</td>\n",
       "      <td>-0.750757</td>\n",
       "      <td>-1.221140</td>\n",
       "      <td>0.096776</td>\n",
       "      <td>0.207405</td>\n",
       "      <td>-0.325804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410253</td>\n",
       "      <td>-0.606741</td>\n",
       "      <td>0.405081</td>\n",
       "      <td>0.379020</td>\n",
       "      <td>0.146722</td>\n",
       "      <td>-2.797375</td>\n",
       "      <td>0.402268</td>\n",
       "      <td>0.365407</td>\n",
       "      <td>0.407107</td>\n",
       "      <td>0.544587</td>\n",
       "      <td>0.365830</td>\n",
       "      <td>-1.838506</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.035888</td>\n",
       "      <td>0.401656</td>\n",
       "      <td>0.501421</td>\n",
       "      <td>0.208150</td>\n",
       "      <td>-2.689166</td>\n",
       "      <td>0.399993</td>\n",
       "      <td>-1.207566</td>\n",
       "      <td>0.400736</td>\n",
       "      <td>0.343792</td>\n",
       "      <td>-0.082624</td>\n",
       "      <td>-2.785583</td>\n",
       "      <td>0.407112</td>\n",
       "      <td>-1.516834</td>\n",
       "      <td>0.404433</td>\n",
       "      <td>0.571418</td>\n",
       "      <td>-0.096426</td>\n",
       "      <td>-1.968371</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.433369</td>\n",
       "      <td>6.171715</td>\n",
       "      <td>0.569810</td>\n",
       "      <td>8.904305</td>\n",
       "      <td>0.845862</td>\n",
       "      <td>7.517883</td>\n",
       "      <td>0.546109</td>\n",
       "      <td>6.255790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336612</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>0.371323</td>\n",
       "      <td>0.578108</td>\n",
       "      <td>-0.521081</td>\n",
       "      <td>-0.785774</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>-0.718056</td>\n",
       "      <td>0.167745</td>\n",
       "      <td>0.089122</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>0.272748</td>\n",
       "      <td>1.007410</td>\n",
       "      <td>-0.773596</td>\n",
       "      <td>-0.456857</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>0.113582</td>\n",
       "      <td>0.497939</td>\n",
       "      <td>1.196513</td>\n",
       "      <td>0.186911</td>\n",
       "      <td>0.176979</td>\n",
       "      <td>1.221053</td>\n",
       "      <td>2.846036</td>\n",
       "      <td>1.179481</td>\n",
       "      <td>2.553935</td>\n",
       "      <td>0.135483</td>\n",
       "      <td>0.160876</td>\n",
       "      <td>0.612291</td>\n",
       "      <td>1.474382</td>\n",
       "      <td>0.220560</td>\n",
       "      <td>0.250131</td>\n",
       "      <td>-1.506995</td>\n",
       "      <td>-2.215967</td>\n",
       "      <td>0.394439</td>\n",
       "      <td>0.744948</td>\n",
       "      <td>0.440925</td>\n",
       "      <td>0.825803</td>\n",
       "      <td>-0.144538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410253</td>\n",
       "      <td>-1.515613</td>\n",
       "      <td>0.405081</td>\n",
       "      <td>-0.807140</td>\n",
       "      <td>-0.673392</td>\n",
       "      <td>-4.320637</td>\n",
       "      <td>0.402268</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>0.407107</td>\n",
       "      <td>0.483518</td>\n",
       "      <td>-1.194013</td>\n",
       "      <td>-2.192633</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>0.401656</td>\n",
       "      <td>-0.062102</td>\n",
       "      <td>-1.781693</td>\n",
       "      <td>-3.616335</td>\n",
       "      <td>0.399993</td>\n",
       "      <td>-3.548454</td>\n",
       "      <td>0.400736</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>-3.287816</td>\n",
       "      <td>-3.726751</td>\n",
       "      <td>0.407112</td>\n",
       "      <td>-2.922627</td>\n",
       "      <td>0.404433</td>\n",
       "      <td>-0.832615</td>\n",
       "      <td>-2.897081</td>\n",
       "      <td>-2.849086</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.301936</td>\n",
       "      <td>2.223840</td>\n",
       "      <td>0.562993</td>\n",
       "      <td>4.057539</td>\n",
       "      <td>0.802714</td>\n",
       "      <td>3.175238</td>\n",
       "      <td>0.408292</td>\n",
       "      <td>2.260603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336613</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.404034</td>\n",
       "      <td>1.012231</td>\n",
       "      <td>-0.262774</td>\n",
       "      <td>-0.279760</td>\n",
       "      <td>1.109238</td>\n",
       "      <td>1.402120</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>-3.335564</td>\n",
       "      <td>-2.368646</td>\n",
       "      <td>0.089122</td>\n",
       "      <td>0.049486</td>\n",
       "      <td>0.159018</td>\n",
       "      <td>0.449517</td>\n",
       "      <td>-2.953844</td>\n",
       "      <td>-4.315450</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>0.113582</td>\n",
       "      <td>-1.186143</td>\n",
       "      <td>-1.976399</td>\n",
       "      <td>0.186911</td>\n",
       "      <td>0.176979</td>\n",
       "      <td>-2.214412</td>\n",
       "      <td>-3.441187</td>\n",
       "      <td>-2.804880</td>\n",
       "      <td>-3.955774</td>\n",
       "      <td>0.135483</td>\n",
       "      <td>0.160876</td>\n",
       "      <td>2.150748</td>\n",
       "      <td>3.797262</td>\n",
       "      <td>0.220560</td>\n",
       "      <td>0.250131</td>\n",
       "      <td>1.527637</td>\n",
       "      <td>2.273965</td>\n",
       "      <td>2.670225</td>\n",
       "      <td>3.650173</td>\n",
       "      <td>-0.419471</td>\n",
       "      <td>-0.535436</td>\n",
       "      <td>0.188438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410253</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.405081</td>\n",
       "      <td>-0.335426</td>\n",
       "      <td>-0.273326</td>\n",
       "      <td>-3.768540</td>\n",
       "      <td>0.402268</td>\n",
       "      <td>0.202177</td>\n",
       "      <td>0.407107</td>\n",
       "      <td>-0.517945</td>\n",
       "      <td>-0.070384</td>\n",
       "      <td>-2.344217</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>-0.173560</td>\n",
       "      <td>0.401656</td>\n",
       "      <td>-0.834834</td>\n",
       "      <td>-0.577478</td>\n",
       "      <td>-3.608454</td>\n",
       "      <td>0.399993</td>\n",
       "      <td>-0.601877</td>\n",
       "      <td>0.400736</td>\n",
       "      <td>-0.433851</td>\n",
       "      <td>-0.894544</td>\n",
       "      <td>-3.748598</td>\n",
       "      <td>0.407112</td>\n",
       "      <td>-1.099637</td>\n",
       "      <td>0.404433</td>\n",
       "      <td>-1.351879</td>\n",
       "      <td>-1.408661</td>\n",
       "      <td>-2.674982</td>\n",
       "      <td>0.335127</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.992109</td>\n",
       "      <td>2.396974</td>\n",
       "      <td>2.007911</td>\n",
       "      <td>6.018636</td>\n",
       "      <td>2.796428</td>\n",
       "      <td>4.890906</td>\n",
       "      <td>1.879257</td>\n",
       "      <td>3.514155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477706</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.437303</td>\n",
       "      <td>-0.362515</td>\n",
       "      <td>0.555022</td>\n",
       "      <td>1.163277</td>\n",
       "      <td>0.177143</td>\n",
       "      <td>0.412206</td>\n",
       "      <td>-0.331780</td>\n",
       "      <td>-0.359870</td>\n",
       "      <td>-1.513231</td>\n",
       "      <td>-0.252694</td>\n",
       "      <td>-0.321431</td>\n",
       "      <td>-0.114518</td>\n",
       "      <td>-0.692729</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>-0.871394</td>\n",
       "      <td>-0.150478</td>\n",
       "      <td>-0.150613</td>\n",
       "      <td>-0.493871</td>\n",
       "      <td>-0.528325</td>\n",
       "      <td>-1.401675</td>\n",
       "      <td>-0.397318</td>\n",
       "      <td>-0.551676</td>\n",
       "      <td>-0.473016</td>\n",
       "      <td>-1.458345</td>\n",
       "      <td>-0.623840</td>\n",
       "      <td>-1.615848</td>\n",
       "      <td>0.110647</td>\n",
       "      <td>0.382548</td>\n",
       "      <td>0.267037</td>\n",
       "      <td>1.042512</td>\n",
       "      <td>0.217934</td>\n",
       "      <td>0.848732</td>\n",
       "      <td>0.389876</td>\n",
       "      <td>1.096452</td>\n",
       "      <td>0.697503</td>\n",
       "      <td>1.828764</td>\n",
       "      <td>-0.348224</td>\n",
       "      <td>-0.630723</td>\n",
       "      <td>0.900741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566545</td>\n",
       "      <td>0.344307</td>\n",
       "      <td>-0.018956</td>\n",
       "      <td>-0.171640</td>\n",
       "      <td>-0.084345</td>\n",
       "      <td>-0.460863</td>\n",
       "      <td>0.156520</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>0.712076</td>\n",
       "      <td>0.645609</td>\n",
       "      <td>-0.263383</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.740980</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>-0.049737</td>\n",
       "      <td>0.048424</td>\n",
       "      <td>-0.860146</td>\n",
       "      <td>-0.388650</td>\n",
       "      <td>0.209272</td>\n",
       "      <td>-0.389966</td>\n",
       "      <td>0.501432</td>\n",
       "      <td>0.413398</td>\n",
       "      <td>-0.451428</td>\n",
       "      <td>0.336393</td>\n",
       "      <td>-0.655987</td>\n",
       "      <td>-1.324612</td>\n",
       "      <td>-0.114259</td>\n",
       "      <td>-0.218888</td>\n",
       "      <td>-1.400489</td>\n",
       "      <td>-0.320792</td>\n",
       "      <td>-1.625904</td>\n",
       "      <td>-1.176226</td>\n",
       "      <td>-1.093271</td>\n",
       "      <td>0.866209</td>\n",
       "      <td>-1.477243</td>\n",
       "      <td>-1.530666</td>\n",
       "      <td>-2.296308</td>\n",
       "      <td>-0.681871</td>\n",
       "      <td>-1.542378</td>\n",
       "      <td>0.266622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477707</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.489115</td>\n",
       "      <td>0.747780</td>\n",
       "      <td>0.468274</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.577694</td>\n",
       "      <td>1.076868</td>\n",
       "      <td>0.025698</td>\n",
       "      <td>0.371077</td>\n",
       "      <td>-0.703578</td>\n",
       "      <td>0.447254</td>\n",
       "      <td>-0.040592</td>\n",
       "      <td>0.622462</td>\n",
       "      <td>0.248471</td>\n",
       "      <td>1.259631</td>\n",
       "      <td>-0.433358</td>\n",
       "      <td>0.580575</td>\n",
       "      <td>-0.146107</td>\n",
       "      <td>-0.413919</td>\n",
       "      <td>-0.515539</td>\n",
       "      <td>-1.191054</td>\n",
       "      <td>-0.383816</td>\n",
       "      <td>-0.465867</td>\n",
       "      <td>-0.458650</td>\n",
       "      <td>-1.216752</td>\n",
       "      <td>-0.606793</td>\n",
       "      <td>-1.361451</td>\n",
       "      <td>0.409130</td>\n",
       "      <td>1.229911</td>\n",
       "      <td>0.619475</td>\n",
       "      <td>1.776330</td>\n",
       "      <td>0.718490</td>\n",
       "      <td>2.336154</td>\n",
       "      <td>0.897586</td>\n",
       "      <td>2.017635</td>\n",
       "      <td>0.926897</td>\n",
       "      <td>1.982486</td>\n",
       "      <td>-0.441451</td>\n",
       "      <td>-0.748090</td>\n",
       "      <td>0.765461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627780</td>\n",
       "      <td>0.087559</td>\n",
       "      <td>1.064923</td>\n",
       "      <td>1.336549</td>\n",
       "      <td>1.317102</td>\n",
       "      <td>1.105201</td>\n",
       "      <td>0.521934</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>0.516312</td>\n",
       "      <td>-0.762148</td>\n",
       "      <td>-0.675919</td>\n",
       "      <td>0.910600</td>\n",
       "      <td>1.012672</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>0.618398</td>\n",
       "      <td>-0.880939</td>\n",
       "      <td>-1.066773</td>\n",
       "      <td>1.496741</td>\n",
       "      <td>0.603313</td>\n",
       "      <td>-1.723325</td>\n",
       "      <td>0.454099</td>\n",
       "      <td>0.016205</td>\n",
       "      <td>-0.190634</td>\n",
       "      <td>0.982925</td>\n",
       "      <td>0.750987</td>\n",
       "      <td>-1.680913</td>\n",
       "      <td>1.102378</td>\n",
       "      <td>0.537863</td>\n",
       "      <td>0.382723</td>\n",
       "      <td>1.012653</td>\n",
       "      <td>-1.967861</td>\n",
       "      <td>-0.518521</td>\n",
       "      <td>-1.028979</td>\n",
       "      <td>2.723214</td>\n",
       "      <td>-1.725535</td>\n",
       "      <td>-0.604659</td>\n",
       "      <td>-2.875429</td>\n",
       "      <td>-0.234777</td>\n",
       "      <td>-1.970870</td>\n",
       "      <td>0.741710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477708</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.122297</td>\n",
       "      <td>1.549778</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>1.180336</td>\n",
       "      <td>0.058843</td>\n",
       "      <td>0.124733</td>\n",
       "      <td>-0.060697</td>\n",
       "      <td>0.165334</td>\n",
       "      <td>-0.550465</td>\n",
       "      <td>0.544354</td>\n",
       "      <td>-0.253543</td>\n",
       "      <td>-0.069905</td>\n",
       "      <td>0.019183</td>\n",
       "      <td>0.850359</td>\n",
       "      <td>-0.511220</td>\n",
       "      <td>0.310013</td>\n",
       "      <td>-0.190350</td>\n",
       "      <td>-0.513987</td>\n",
       "      <td>-0.638606</td>\n",
       "      <td>-1.456781</td>\n",
       "      <td>-0.517591</td>\n",
       "      <td>-0.573668</td>\n",
       "      <td>-0.598508</td>\n",
       "      <td>-1.519734</td>\n",
       "      <td>-0.770697</td>\n",
       "      <td>-1.681493</td>\n",
       "      <td>0.212842</td>\n",
       "      <td>0.607414</td>\n",
       "      <td>0.696455</td>\n",
       "      <td>1.882878</td>\n",
       "      <td>0.428458</td>\n",
       "      <td>1.350808</td>\n",
       "      <td>0.744698</td>\n",
       "      <td>1.614847</td>\n",
       "      <td>0.893433</td>\n",
       "      <td>1.824506</td>\n",
       "      <td>-0.030414</td>\n",
       "      <td>-0.037339</td>\n",
       "      <td>1.219874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271947</td>\n",
       "      <td>3.775286</td>\n",
       "      <td>1.140731</td>\n",
       "      <td>2.693780</td>\n",
       "      <td>1.999331</td>\n",
       "      <td>0.516708</td>\n",
       "      <td>0.216476</td>\n",
       "      <td>-1.338859</td>\n",
       "      <td>1.258084</td>\n",
       "      <td>0.874030</td>\n",
       "      <td>0.809693</td>\n",
       "      <td>0.877870</td>\n",
       "      <td>-0.714713</td>\n",
       "      <td>-0.940190</td>\n",
       "      <td>0.263431</td>\n",
       "      <td>0.227476</td>\n",
       "      <td>0.146444</td>\n",
       "      <td>0.062240</td>\n",
       "      <td>1.044940</td>\n",
       "      <td>2.655343</td>\n",
       "      <td>1.433573</td>\n",
       "      <td>2.397503</td>\n",
       "      <td>2.671179</td>\n",
       "      <td>1.535036</td>\n",
       "      <td>-0.129669</td>\n",
       "      <td>1.625558</td>\n",
       "      <td>0.927247</td>\n",
       "      <td>2.111121</td>\n",
       "      <td>1.497974</td>\n",
       "      <td>0.218819</td>\n",
       "      <td>-1.323733</td>\n",
       "      <td>-1.948010</td>\n",
       "      <td>-1.360185</td>\n",
       "      <td>-1.185980</td>\n",
       "      <td>-1.181586</td>\n",
       "      <td>-2.112499</td>\n",
       "      <td>-1.901803</td>\n",
       "      <td>-1.448704</td>\n",
       "      <td>-1.621296</td>\n",
       "      <td>-1.197365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477709</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.267178</td>\n",
       "      <td>1.684549</td>\n",
       "      <td>-1.616354</td>\n",
       "      <td>-2.569574</td>\n",
       "      <td>-0.969837</td>\n",
       "      <td>-1.593147</td>\n",
       "      <td>0.139004</td>\n",
       "      <td>0.536574</td>\n",
       "      <td>-0.144341</td>\n",
       "      <td>0.957873</td>\n",
       "      <td>0.369662</td>\n",
       "      <td>1.628314</td>\n",
       "      <td>0.933388</td>\n",
       "      <td>2.062437</td>\n",
       "      <td>0.402529</td>\n",
       "      <td>2.186436</td>\n",
       "      <td>-0.106110</td>\n",
       "      <td>-0.256929</td>\n",
       "      <td>-0.387794</td>\n",
       "      <td>-0.790927</td>\n",
       "      <td>-0.258936</td>\n",
       "      <td>-0.298834</td>\n",
       "      <td>-0.319932</td>\n",
       "      <td>-0.740813</td>\n",
       "      <td>-0.436879</td>\n",
       "      <td>-0.870435</td>\n",
       "      <td>0.621646</td>\n",
       "      <td>1.703243</td>\n",
       "      <td>1.072573</td>\n",
       "      <td>2.718712</td>\n",
       "      <td>0.838227</td>\n",
       "      <td>2.515573</td>\n",
       "      <td>1.408170</td>\n",
       "      <td>2.857521</td>\n",
       "      <td>1.727032</td>\n",
       "      <td>3.275871</td>\n",
       "      <td>-1.965684</td>\n",
       "      <td>-3.598521</td>\n",
       "      <td>0.227190</td>\n",
       "      <td>...</td>\n",
       "      <td>2.797396</td>\n",
       "      <td>6.872263</td>\n",
       "      <td>2.530522</td>\n",
       "      <td>2.576913</td>\n",
       "      <td>4.742838</td>\n",
       "      <td>3.070599</td>\n",
       "      <td>0.994198</td>\n",
       "      <td>-0.478647</td>\n",
       "      <td>0.326116</td>\n",
       "      <td>-0.556971</td>\n",
       "      <td>-0.684610</td>\n",
       "      <td>1.845386</td>\n",
       "      <td>1.869707</td>\n",
       "      <td>-0.459874</td>\n",
       "      <td>0.539517</td>\n",
       "      <td>-0.589962</td>\n",
       "      <td>-1.073245</td>\n",
       "      <td>2.801966</td>\n",
       "      <td>1.960878</td>\n",
       "      <td>1.989937</td>\n",
       "      <td>0.908788</td>\n",
       "      <td>0.525519</td>\n",
       "      <td>1.880232</td>\n",
       "      <td>2.471087</td>\n",
       "      <td>2.294814</td>\n",
       "      <td>3.712643</td>\n",
       "      <td>2.054794</td>\n",
       "      <td>1.734063</td>\n",
       "      <td>3.241711</td>\n",
       "      <td>2.374002</td>\n",
       "      <td>-1.999512</td>\n",
       "      <td>-0.552139</td>\n",
       "      <td>-0.878920</td>\n",
       "      <td>3.297772</td>\n",
       "      <td>-1.629102</td>\n",
       "      <td>-0.107320</td>\n",
       "      <td>-2.569374</td>\n",
       "      <td>0.315121</td>\n",
       "      <td>-1.726590</td>\n",
       "      <td>1.304793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477710</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.423650</td>\n",
       "      <td>2.759741</td>\n",
       "      <td>-0.022720</td>\n",
       "      <td>0.028887</td>\n",
       "      <td>-0.208434</td>\n",
       "      <td>-0.408458</td>\n",
       "      <td>-0.566975</td>\n",
       "      <td>-0.942952</td>\n",
       "      <td>-1.794568</td>\n",
       "      <td>-0.528992</td>\n",
       "      <td>-0.294059</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>-0.989432</td>\n",
       "      <td>-0.429891</td>\n",
       "      <td>-1.194258</td>\n",
       "      <td>-0.948735</td>\n",
       "      <td>-0.545794</td>\n",
       "      <td>-1.283983</td>\n",
       "      <td>-0.423095</td>\n",
       "      <td>-1.132827</td>\n",
       "      <td>-0.291810</td>\n",
       "      <td>-0.441790</td>\n",
       "      <td>-0.357495</td>\n",
       "      <td>-1.148596</td>\n",
       "      <td>-0.483781</td>\n",
       "      <td>-1.290609</td>\n",
       "      <td>0.134098</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.630255</td>\n",
       "      <td>2.238220</td>\n",
       "      <td>0.267347</td>\n",
       "      <td>1.114718</td>\n",
       "      <td>0.485438</td>\n",
       "      <td>1.412554</td>\n",
       "      <td>0.791230</td>\n",
       "      <td>2.140403</td>\n",
       "      <td>-0.478030</td>\n",
       "      <td>-0.952501</td>\n",
       "      <td>0.459254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186427</td>\n",
       "      <td>1.066335</td>\n",
       "      <td>-0.089677</td>\n",
       "      <td>0.660810</td>\n",
       "      <td>0.332996</td>\n",
       "      <td>-0.948006</td>\n",
       "      <td>1.776057</td>\n",
       "      <td>1.492461</td>\n",
       "      <td>0.086594</td>\n",
       "      <td>0.911409</td>\n",
       "      <td>1.346315</td>\n",
       "      <td>1.423696</td>\n",
       "      <td>-0.453650</td>\n",
       "      <td>-0.118629</td>\n",
       "      <td>-0.941821</td>\n",
       "      <td>-0.510116</td>\n",
       "      <td>-0.319264</td>\n",
       "      <td>-0.702748</td>\n",
       "      <td>2.497181</td>\n",
       "      <td>2.915640</td>\n",
       "      <td>0.968959</td>\n",
       "      <td>2.061940</td>\n",
       "      <td>2.639890</td>\n",
       "      <td>1.617590</td>\n",
       "      <td>-0.323472</td>\n",
       "      <td>-0.049253</td>\n",
       "      <td>-0.807764</td>\n",
       "      <td>0.059132</td>\n",
       "      <td>-0.278139</td>\n",
       "      <td>-0.607517</td>\n",
       "      <td>-0.950794</td>\n",
       "      <td>-2.081971</td>\n",
       "      <td>-1.079180</td>\n",
       "      <td>-1.563114</td>\n",
       "      <td>-0.663384</td>\n",
       "      <td>-1.462657</td>\n",
       "      <td>-0.927241</td>\n",
       "      <td>-0.867121</td>\n",
       "      <td>-1.202512</td>\n",
       "      <td>-1.435868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141102 rows Ã 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature_0  feature_1  feature_2  ...  feature_127  feature_128  feature_129\n",
       "336609          1  -2.132142  -1.722265  ...     6.468159     0.587420     4.304226\n",
       "336610         -1   0.649220   1.384605  ...     1.313572     1.598569     0.558448\n",
       "336611          1  -1.397873  -0.832325  ...     7.517883     0.546109     6.255790\n",
       "336612          1  -3.172026  -3.093182  ...     3.175238     0.408292     2.260603\n",
       "336613         -1   0.404034   1.012231  ...     4.890906     1.879257     3.514155\n",
       "...           ...        ...        ...  ...          ...          ...          ...\n",
       "477706         -1  -1.437303  -0.362515  ...    -0.681871    -1.542378     0.266622\n",
       "477707         -1  -0.489115   0.747780  ...    -0.234777    -1.970870     0.741710\n",
       "477708         -1   0.122297   1.549778  ...    -1.448704    -1.621296    -1.197365\n",
       "477709         -1   0.267178   1.684549  ...     0.315121    -1.726590     1.304793\n",
       "477710         -1   0.423650   2.759741  ...    -0.867121    -1.202512    -1.435868\n",
       "\n",
       "[141102 rows x 130 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filled.loc[folds_list_test[0], ['feature_' + str(i) for i in range(130)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds_myNN_1 = model_myNN_1(torch.tensor(df_filled.loc[folds_list_test[0], ['feature_' + str(i) for i in range(130)]].to_numpy(), dtype=torch.double)).squeeze().numpy()\n",
    "    preds_myNN_2 = model_myNN_2(torch.tensor(df_filled.loc[folds_list_test[0], ['feature_' + str(i) for i in range(130)]].to_numpy(), dtype=torch.double)).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141102,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_myNN_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of logistic regression for fold 0:\n",
      "[[0.55170975 2.98991743]]\n",
      "Intercept for fold 0\n",
      "[-1.79342183]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.47427408]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.52572592 0.47427408]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 1:\n",
      "[[1.75646163 2.97744507]]\n",
      "Intercept for fold 1\n",
      "[-2.36203286]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.51553319]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.48446681 0.51553319]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 2:\n",
      "[[-1.42003956  2.73022048]]\n",
      "Intercept for fold 2\n",
      "[-0.65775461]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.50032987]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.49967013 0.50032987]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 3:\n",
      "[[0.21907055 2.39600189]]\n",
      "Intercept for fold 3\n",
      "[-1.32076957]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.4921046]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.5078954 0.4921046]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 4:\n",
      "[[4.27658617 0.34029251]]\n",
      "Intercept for fold 4\n",
      "[-2.33183246]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.4701657]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.5298343 0.4701657]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_coefs = []\n",
    "logreg_intercepts = []\n",
    "\n",
    "for fold_indice in range(5):\n",
    "    df_np = df_filled.loc[folds_list_test[fold_indice], ['feature_' + str(i) for i in range(130)]].values \n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_myNN_1 = model_myNN_1(torch.tensor(df_filled.loc[folds_list_test[fold_indice], ['feature_' + str(i) for i in range(130)]].to_numpy(), dtype=torch.double)).squeeze().numpy()\n",
    "        preds_myNN_2 = model_myNN_2(torch.tensor(df_filled.loc[folds_list_test[fold_indice], ['feature_' + str(i) for i in range(130)]].to_numpy(), dtype=torch.double)).squeeze().numpy()\n",
    "    preds_xgb = xgb_test_predictions_folds[fold_indice]\n",
    "    \n",
    "    #preds = np.stack([preds_nb1, preds_nb2, preds_nb3, preds_xgb], axis=1)\n",
    "    preds = np.stack([preds_myNN_1, preds_myNN_2], axis=1)\n",
    "    #preds = np.stack([preds_myNN_1, preds_myNN_2, preds_xgb], axis=1)\n",
    "\n",
    "    # Train logistic regression\n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(preds, df.loc[folds_list_test[fold_indice], 'resp_positive'])\n",
    "\n",
    "    print(f'Coefficients of logistic regression for fold {fold_indice}:')\n",
    "    print(logreg.coef_)\n",
    "    logreg_coefs.append(logreg.coef_[0])\n",
    "    \n",
    "    print(f'Intercept for fold {fold_indice}')\n",
    "    print(logreg.intercept_)\n",
    "    logreg_intercepts.append(logreg.intercept_[0])\n",
    "\n",
    "    print('Manually calculating proba for instance 0:')\n",
    "    print(1/(1 + np.exp(-(np.dot(logreg.coef_[0], preds[0]) + logreg.intercept_))) )\n",
    "\n",
    "    print('Predict proba by scikit learn for instance 0:')\n",
    "    print(logreg.predict_proba(preds[0:1]))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55170975,  2.98991743],\n",
       "       [ 1.75646163,  2.97744507],\n",
       "       [-1.42003956,  2.73022048],\n",
       "       [ 0.21907055,  2.39600189],\n",
       "       [ 4.27658617,  0.34029251]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(logreg_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.793421829160303,\n",
       " -2.3620328559210417,\n",
       " -0.6577546092455759,\n",
       " -1.3207695733370102,\n",
       " -2.3318324625160276]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07675771, 2.28677548])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.stack(logreg_coefs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6931622660359917"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logreg_intercepts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model retrain on maximum data possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to retrain meta model on all data except data used to train base model (to avoid base model providing overfitted predictions),  \n",
    "and retrain base models (resp n-1 prediction and fold prediction) on all data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain meta model (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for fitting:\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.6, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.02, max_delta_step=None, max_depth=10,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=42, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=0.5, tree_method='gpu_hist',\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:40:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier_wrapper(params=None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapped_final = XGBClassifier_wrapper({\n",
    "   #'features': ['feature_'+str(i) for i in range(130)] + [0,1,2,3,4] + ['resp_n1_predict'], \n",
    "    'features': ['feature_'+str(i) for i in range(130)] + [0,3,4] + ['resp_n1_predict'], \n",
    "    'random_state': 42,\n",
    "    'max_depth': 10,\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.02,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'gamma': None,\n",
    "    'tree_method': 'gpu_hist'        \n",
    "    #'tree_method': 'hist' # CPU\n",
    "    })\n",
    "\n",
    "model_wrapped_final.fit(\n",
    "    df, \n",
    "    (df['resp'] > 0).astype(np.byte)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapped_final.model_internal.save_model(MODEL_FILE_META)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featimportance_final = pd.DataFrame(model_wrapped.model_internal.feature_importances_, index=FEATURES_LIST_TOTRAIN + [0,3,4] + ['resp_n1_predict'], columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "df_featimportance_cumulated_final = pd.concat([df_featimportance, pd.DataFrame({'% feat importance cumulÃ©' : (df_featimportance['Importance'] / df_featimportance['Importance'].sum()).cumsum()})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "      <th>% feat importance cumulÃ©</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature_42</th>\n",
       "      <td>0.013739</td>\n",
       "      <td>0.013739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_43</th>\n",
       "      <td>0.013437</td>\n",
       "      <td>0.027176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_45</th>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.040407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_41</th>\n",
       "      <td>0.013018</td>\n",
       "      <td>0.053426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_44</th>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.066004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_63</th>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.077999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_39</th>\n",
       "      <td>0.011919</td>\n",
       "      <td>0.089918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_61</th>\n",
       "      <td>0.011712</td>\n",
       "      <td>0.101630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_5</th>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.112847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_6</th>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.123941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_27</th>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.134846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_62</th>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.145697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_60</th>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.156432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_107</th>\n",
       "      <td>0.010168</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_83</th>\n",
       "      <td>0.010065</td>\n",
       "      <td>0.176665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_3</th>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.186597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_4</th>\n",
       "      <td>0.009886</td>\n",
       "      <td>0.196483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_38</th>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.206292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.216053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_40</th>\n",
       "      <td>0.009753</td>\n",
       "      <td>0.225806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009349</td>\n",
       "      <td>0.235155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_119</th>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.244474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_120</th>\n",
       "      <td>0.009314</td>\n",
       "      <td>0.253788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_37</th>\n",
       "      <td>0.009265</td>\n",
       "      <td>0.263052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.272078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_77</th>\n",
       "      <td>0.008942</td>\n",
       "      <td>0.281019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_114</th>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.289883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_55</th>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.298714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_64</th>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.307474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_113</th>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.316028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_95</th>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.324546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_121</th>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.333041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_20</th>\n",
       "      <td>0.008483</td>\n",
       "      <td>0.341525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_124</th>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.349997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_90</th>\n",
       "      <td>0.008446</td>\n",
       "      <td>0.358443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_102</th>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.366827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_68</th>\n",
       "      <td>0.008243</td>\n",
       "      <td>0.375070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_66</th>\n",
       "      <td>0.008105</td>\n",
       "      <td>0.383175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_125</th>\n",
       "      <td>0.008020</td>\n",
       "      <td>0.391196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_89</th>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.399203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_57</th>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.407159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_101</th>\n",
       "      <td>0.007893</td>\n",
       "      <td>0.415053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_126</th>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.422901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_28</th>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.430683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_71</th>\n",
       "      <td>0.007502</td>\n",
       "      <td>0.438185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_65</th>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.445682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_58</th>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.453176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_108</th>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.460642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_78</th>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.468035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_84</th>\n",
       "      <td>0.007372</td>\n",
       "      <td>0.475407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_67</th>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.482719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_92</th>\n",
       "      <td>0.007264</td>\n",
       "      <td>0.489983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_96</th>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.497177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_127</th>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.504360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_8</th>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_69</th>\n",
       "      <td>0.007081</td>\n",
       "      <td>0.518581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_31</th>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.525644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_26</th>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.532660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_18</th>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.539654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_110</th>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.546603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_70</th>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.553552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_104</th>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_17</th>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.567399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_116</th>\n",
       "      <td>0.006904</td>\n",
       "      <td>0.574303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_59</th>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.581155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_36</th>\n",
       "      <td>0.006824</td>\n",
       "      <td>0.587979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_53</th>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.594766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_22</th>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.601532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_50</th>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.608286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_32</th>\n",
       "      <td>0.006746</td>\n",
       "      <td>0.615032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_128</th>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.621734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_7</th>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.628413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_49</th>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.635031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_21</th>\n",
       "      <td>0.006617</td>\n",
       "      <td>0.641648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_72</th>\n",
       "      <td>0.006609</td>\n",
       "      <td>0.648257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_33</th>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.654857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_23</th>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.661453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_86</th>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.668042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_98</th>\n",
       "      <td>0.006502</td>\n",
       "      <td>0.674543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_34</th>\n",
       "      <td>0.006396</td>\n",
       "      <td>0.680939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_10</th>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.687306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_24</th>\n",
       "      <td>0.006362</td>\n",
       "      <td>0.693668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_111</th>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.700020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_47</th>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.706307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_129</th>\n",
       "      <td>0.006281</td>\n",
       "      <td>0.712588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_117</th>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.718857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_56</th>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.725127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_48</th>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.731395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_30</th>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.737659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_25</th>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.743923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_12</th>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.750165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_35</th>\n",
       "      <td>0.006240</td>\n",
       "      <td>0.756405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_122</th>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.762628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_54</th>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.768843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_46</th>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.775003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_123</th>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.781153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_105</th>\n",
       "      <td>0.006142</td>\n",
       "      <td>0.787295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_51</th>\n",
       "      <td>0.006141</td>\n",
       "      <td>0.793436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_29</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.799571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_19</th>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.805655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_11</th>\n",
       "      <td>0.006058</td>\n",
       "      <td>0.811713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_93</th>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.817768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_2</th>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.823799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_9</th>\n",
       "      <td>0.005997</td>\n",
       "      <td>0.829796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_115</th>\n",
       "      <td>0.005977</td>\n",
       "      <td>0.835773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_118</th>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.841740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_94</th>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.847701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_80</th>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.853660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_1</th>\n",
       "      <td>0.005943</td>\n",
       "      <td>0.859603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_99</th>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.865529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_109</th>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.871418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_74</th>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.877300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_14</th>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.883182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_87</th>\n",
       "      <td>0.005864</td>\n",
       "      <td>0.889046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_112</th>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.894896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_106</th>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.900742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_97</th>\n",
       "      <td>0.005805</td>\n",
       "      <td>0.906547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_82</th>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.912277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_81</th>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.917997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_91</th>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.923714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_85</th>\n",
       "      <td>0.005716</td>\n",
       "      <td>0.929430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_76</th>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.935113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_16</th>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.940775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_100</th>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.946409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_15</th>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.952032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_52</th>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.957627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_79</th>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.963222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_73</th>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.968747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_13</th>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.974229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_88</th>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.979681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_103</th>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.985125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_75</th>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.990532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resp_n1_predict</th>\n",
       "      <td>0.005354</td>\n",
       "      <td>0.995886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_0</th>\n",
       "      <td>0.004114</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Importance  % feat importance cumulÃ©\n",
       "feature_42         0.013739                  0.013739\n",
       "feature_43         0.013437                  0.027176\n",
       "feature_45         0.013231                  0.040407\n",
       "feature_41         0.013018                  0.053426\n",
       "feature_44         0.012578                  0.066004\n",
       "feature_63         0.011994                  0.077999\n",
       "feature_39         0.011919                  0.089918\n",
       "feature_61         0.011712                  0.101630\n",
       "feature_5          0.011217                  0.112847\n",
       "feature_6          0.011094                  0.123941\n",
       "feature_27         0.010905                  0.134846\n",
       "feature_62         0.010851                  0.145697\n",
       "feature_60         0.010735                  0.156432\n",
       "feature_107        0.010168                  0.166600\n",
       "feature_83         0.010065                  0.176665\n",
       "feature_3          0.009933                  0.186597\n",
       "feature_4          0.009886                  0.196483\n",
       "feature_38         0.009809                  0.206292\n",
       "4                  0.009761                  0.216053\n",
       "feature_40         0.009753                  0.225806\n",
       "0                  0.009349                  0.235155\n",
       "feature_119        0.009319                  0.244474\n",
       "feature_120        0.009314                  0.253788\n",
       "feature_37         0.009265                  0.263052\n",
       "3                  0.009025                  0.272078\n",
       "feature_77         0.008942                  0.281019\n",
       "feature_114        0.008864                  0.289883\n",
       "feature_55         0.008831                  0.298714\n",
       "feature_64         0.008761                  0.307474\n",
       "feature_113        0.008554                  0.316028\n",
       "feature_95         0.008518                  0.324546\n",
       "feature_121        0.008495                  0.333041\n",
       "feature_20         0.008483                  0.341525\n",
       "feature_124        0.008472                  0.349997\n",
       "feature_90         0.008446                  0.358443\n",
       "feature_102        0.008384                  0.366827\n",
       "feature_68         0.008243                  0.375070\n",
       "feature_66         0.008105                  0.383175\n",
       "feature_125        0.008020                  0.391196\n",
       "feature_89         0.008008                  0.399203\n",
       "feature_57         0.007956                  0.407159\n",
       "feature_101        0.007893                  0.415053\n",
       "feature_126        0.007848                  0.422901\n",
       "feature_28         0.007782                  0.430683\n",
       "feature_71         0.007502                  0.438185\n",
       "feature_65         0.007497                  0.445682\n",
       "feature_58         0.007494                  0.453176\n",
       "feature_108        0.007466                  0.460642\n",
       "feature_78         0.007393                  0.468035\n",
       "feature_84         0.007372                  0.475407\n",
       "feature_67         0.007312                  0.482719\n",
       "feature_92         0.007264                  0.489983\n",
       "feature_96         0.007194                  0.497177\n",
       "feature_127        0.007183                  0.504360\n",
       "feature_8          0.007140                  0.511500\n",
       "feature_69         0.007081                  0.518581\n",
       "feature_31         0.007063                  0.525644\n",
       "feature_26         0.007016                  0.532660\n",
       "feature_18         0.006994                  0.539654\n",
       "feature_110        0.006950                  0.546603\n",
       "feature_70         0.006949                  0.553552\n",
       "feature_104        0.006927                  0.560480\n",
       "feature_17         0.006919                  0.567399\n",
       "feature_116        0.006904                  0.574303\n",
       "feature_59         0.006853                  0.581155\n",
       "feature_36         0.006824                  0.587979\n",
       "feature_53         0.006787                  0.594766\n",
       "feature_22         0.006766                  0.601532\n",
       "feature_50         0.006754                  0.608286\n",
       "feature_32         0.006746                  0.615032\n",
       "feature_128        0.006702                  0.621734\n",
       "feature_7          0.006679                  0.628413\n",
       "feature_49         0.006618                  0.635031\n",
       "feature_21         0.006617                  0.641648\n",
       "feature_72         0.006609                  0.648257\n",
       "feature_33         0.006599                  0.654857\n",
       "feature_23         0.006596                  0.661453\n",
       "feature_86         0.006589                  0.668042\n",
       "feature_98         0.006502                  0.674543\n",
       "feature_34         0.006396                  0.680939\n",
       "feature_10         0.006367                  0.687306\n",
       "feature_24         0.006362                  0.693668\n",
       "feature_111        0.006352                  0.700020\n",
       "feature_47         0.006287                  0.706307\n",
       "feature_129        0.006281                  0.712588\n",
       "feature_117        0.006270                  0.718857\n",
       "feature_56         0.006269                  0.725127\n",
       "feature_48         0.006269                  0.731395\n",
       "feature_30         0.006264                  0.737659\n",
       "feature_25         0.006264                  0.743923\n",
       "feature_12         0.006243                  0.750165\n",
       "feature_35         0.006240                  0.756405\n",
       "feature_122        0.006222                  0.762628\n",
       "feature_54         0.006216                  0.768843\n",
       "feature_46         0.006160                  0.775003\n",
       "feature_123        0.006150                  0.781153\n",
       "feature_105        0.006142                  0.787295\n",
       "feature_51         0.006141                  0.793436\n",
       "feature_29         0.006135                  0.799571\n",
       "feature_19         0.006083                  0.805655\n",
       "feature_11         0.006058                  0.811713\n",
       "feature_93         0.006055                  0.817768\n",
       "feature_2          0.006030                  0.823799\n",
       "feature_9          0.005997                  0.829796\n",
       "feature_115        0.005977                  0.835773\n",
       "feature_118        0.005967                  0.841740\n",
       "feature_94         0.005961                  0.847701\n",
       "feature_80         0.005959                  0.853660\n",
       "feature_1          0.005943                  0.859603\n",
       "feature_99         0.005926                  0.865529\n",
       "feature_109        0.005889                  0.871418\n",
       "feature_74         0.005883                  0.877300\n",
       "feature_14         0.005882                  0.883182\n",
       "feature_87         0.005864                  0.889046\n",
       "feature_112        0.005850                  0.894896\n",
       "feature_106        0.005846                  0.900742\n",
       "feature_97         0.005805                  0.906547\n",
       "feature_82         0.005729                  0.912277\n",
       "feature_81         0.005720                  0.917997\n",
       "feature_91         0.005717                  0.923714\n",
       "feature_85         0.005716                  0.929430\n",
       "feature_76         0.005683                  0.935113\n",
       "feature_16         0.005662                  0.940775\n",
       "feature_100        0.005634                  0.946409\n",
       "feature_15         0.005623                  0.952032\n",
       "feature_52         0.005595                  0.957627\n",
       "feature_79         0.005595                  0.963222\n",
       "feature_73         0.005525                  0.968747\n",
       "feature_13         0.005482                  0.974229\n",
       "feature_88         0.005451                  0.979681\n",
       "feature_103        0.005444                  0.985125\n",
       "feature_75         0.005408                  0.990532\n",
       "resp_n1_predict    0.005354                  0.995886\n",
       "feature_0          0.004114                  1.000000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_featimportance_cumulated_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain resp n-1 model (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:46:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.9,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate label of current step\n",
    "y_train1_resp_positive = (df.loc[:, 'resp'] > 0).astype(np.byte)\n",
    "\n",
    "# Shift values of resp to get resp of step n-1\n",
    "y_train1_resp_n1_positive = y_train1_resp_positive.shift(1, fill_value=0)\n",
    "\n",
    "\n",
    "model_n1_final = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 12,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.01,\n",
    "    subsample= 0.9,\n",
    "    colsample_bytree= 0.2,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    )\n",
    "\n",
    "model_n1_final.fit(df.loc[:, FEATURES_LIST_TOTRAIN], y_train1_resp_n1_positive, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n1_final.save_model(MODEL_FILE_RESPN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain fold model (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:48:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb_final = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 10,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.02,\n",
    "    subsample= 0.5,\n",
    "    colsample_bytree= 0.6,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    #objective= 'binary:logistic',\n",
    "    #disable_default_eval_metric=True,\n",
    "    )\n",
    "\n",
    "#model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, ['fold_'+str(i) for i in range(NB_FOLDS)]])\n",
    "model_xgb_final.fit(df.loc[:, FEATURES_LIST_TOTRAIN], df.loc[:, 'fold_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_final.save_model(MODEL_FILE_FOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check possible return values for fold 3 that does not perform well :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fold3_resp_positive = df.loc[folds_list_test[3]].query('resp > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2201.1234249072004"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_fold3_resp_positive['weight'] * df_fold3_resp_positive['resp']).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
