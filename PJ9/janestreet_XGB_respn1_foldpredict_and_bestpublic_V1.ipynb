{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch classifier notebook\n",
    "\n",
    "V1 : only 1 split. First implementation  \n",
    "All folds V1 : with all folds  \n",
    "All folds V2 : add activation stats plot  \n",
    "All folds V2.1 : back to  best MLP found so far, and backport fix of activation layers stats. Add weight decay and scheduler (fit one cycle) code\n",
    "\n",
    "All folds autoencoder MLP V1  \n",
    "All folds autoencoder MLP V2 : with weights and biases  \n",
    "All folds autoencoder MLP V3 : replace MLP with xgboost\n",
    "All folds XGB resp N1 fold predict: start from code of V3 but without NN code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import io\n",
    "import PIL.Image\n",
    "\n",
    "import datetime\n",
    "\n",
    "import faiss\n",
    "\n",
    "DATASET_INPUT_FILE = 'train.csv'\n",
    "\n",
    "#FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)] + ['cross_41_42_43', 'cross_1_2']\n",
    "FEATURES_LIST_TOTRAIN = ['feature_'+str(i) for i in range(130)]\n",
    "\n",
    "# For custom non-overlaped folds generation\n",
    "TRAIN_PERCENT = 0.70  \n",
    "TEST_PERCENT = 0.30\n",
    "\n",
    "# If subsplit of training set : percentage of second training set  \n",
    "TRAIN1_PERCENT = 0.20  \n",
    "\n",
    "ACT_N = False  # Add N previous predictions to input of MLP <= Does not work, logic is not right\n",
    "ACT_N_SIZE = 5\n",
    "\n",
    "CLUSTERING = False\n",
    "\n",
    "MODEL_FILE_META = 'model_XGB_meta_respn1_fold.bin'\n",
    "MODEL_FILE_RESPN1 = 'model_XGB_respn1.bin'\n",
    "MODEL_FILE_FOLD = 'model_XGB_fold.bin'\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic Behavior\n",
    "seed = 42\n",
    "#os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Python RNG\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'Best utility',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [524288, 262144, 131072, 65536, 32768, 16384, 8192, 4096, 2048, 1024, 512]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            #'values': [1e-2, 1e-3, 1e-4, 3e-4, 1e-5]\n",
    "            #'values': [1e-2, 1e-3, 1e-4]\n",
    "            'values': [1e-2, 1e-3]\n",
    "        },\n",
    "\n",
    "        'weight_decay': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        },\n",
    "    \n",
    "        'use_autoenc': {\n",
    "            'values': ['encoder-decoder', 'encoder', 'encoder-only', 'None']\n",
    "            #'values': ['encoder-decoder', 'None']\n",
    "        },\n",
    "        \n",
    "        'activation_function': {\n",
    "            'values': ['relu', 'leakyrelu']\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'Best utility',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'min': 4096,\n",
    "            'max': 65536,\n",
    "            'distribution': 'int_uniform',\n",
    "        },\n",
    "        'dropout': {\n",
    "            'min': 0.3,\n",
    "            'max': 0.5,\n",
    "            'distribution': 'uniform',\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 0.0005,\n",
    "            'max': 0.002,\n",
    "            'distribution': 'uniform',\n",
    "        },\n",
    "\n",
    "        'weight_decay': {\n",
    "            'min': 0.00001,\n",
    "            'max': 0.0002,\n",
    "            'distribution': 'uniform',\n",
    "\n",
    "        },\n",
    "    \n",
    "        'use_autoenc': {\n",
    "            'values': ['encoder', 'encoder-only']\n",
    "            #'values': ['encoder-decoder', 'None']\n",
    "        },\n",
    "        \n",
    "        'activation_function': {\n",
    "            'values': ['leakyrelu']\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyStandardScale(tensor, mean, std):\n",
    "    return((tensor - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# this is code slightly modified from the sklearn docs here:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cv_indices_custom(cv_custom, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)   # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv_custom):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accounts for variable instance counts in each split by dividing utility_pi by number of instances (but this has been removed)\n",
    "# It also does some copy of dataframe to prevent memory overwrite\n",
    "def utility_function(df_test, df_test_predictions):\n",
    "    df_test_copy = df_test.copy(deep=True)\n",
    "    df_test_copy.loc[:, 'utility_pj'] = df_test_copy['weight'] * df_test_copy['resp'] * df_test_predictions\n",
    "    #df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum() / df_test_copy.groupby('date')['utility_pj'].count()\n",
    "    df_test_utility_pi = df_test_copy.groupby('date')['utility_pj'].sum()\n",
    "\n",
    "    nb_unique_dates = df_test_utility_pi.shape[0]\n",
    "\n",
    "    if (np.sqrt(df_test_utility_pi.pow(2).sum()) == 0):\n",
    "        t = 0\n",
    "\n",
    "    else:\n",
    "        t = (df_test_utility_pi.sum() / np.sqrt(df_test_utility_pi.pow(2).sum())) * (np.sqrt(250 / np.abs(nb_unique_dates)))\n",
    "\n",
    "    u = min(max(t, 0), 6) * df_test_utility_pi.sum()\n",
    "    del df_test_copy\n",
    "    \n",
    "    return(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "# The aim of this function is to return closest date from an index\n",
    "# So that split indices correspond to start or end of a new day\n",
    "# myList contains list of instances that correspond to start of a new da\n",
    "\n",
    "def take_closest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutputActivationStats:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        #self.outputs.append(module_out)\n",
    "        #print('Save output callback :')\n",
    "        #print(module)\n",
    "        #print({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        self.outputs.append({'mean': module_out.mean().item(), 'std': module_out.std().item(),'near_zero': (module_out<=0.05).long().sum().item()/module_out.numel()})\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "#\n",
    "#plot_cv_indices(cv, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "    \n",
    "df = pd.read_csv(DATASET_INPUT_FILE)\n",
    "df['resp_positive'] = ((df['resp'])>0)*1  # Target to predict\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_41_42_43'] = df['feature_41'] + df['feature_42'] + df['feature_43']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cross_1_2'] = df['feature_1'] / (df['feature_2'] + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non overlap fold generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_indexes_list = df.groupby('date')['ts_id'].first().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_split_size = int((df.shape[0] // 5) * TRAIN_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_split_size = int((df.shape[0] // 5) * TEST_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_split_start_indexes = [take_closest(date_indexes_list, (base_train_split_size + base_test_split_size)*fold_indice) for fold_indice in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 477711, 958233, 1435933, 1913985]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_start_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have 5 folds of 3 subsets each (2 training sets and 1 test set per fold)\n",
    "# (1st training set of each fold will be used for 1st model, ie auto encoder)\n",
    "\n",
    "NB_FOLDS = 5\n",
    "last_index = df.shape[0] - 1\n",
    "\n",
    "cv_table = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_train_start_index = train_split_start_indexes[fold_indice]\n",
    "    \n",
    "    if (fold_indice == NB_FOLDS - 1):    \n",
    "        nextfold_train_start_index = last_index\n",
    "        \n",
    "    else:\n",
    "        nextfold_train_start_index = train_split_start_indexes[fold_indice + 1]\n",
    "    \n",
    "    fold_test_start_index = take_closest(date_indexes_list, int(TRAIN_PERCENT * (nextfold_train_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    fold_train2_start_index = take_closest(date_indexes_list, int(TRAIN1_PERCENT * (fold_test_start_index - fold_train_start_index) + fold_train_start_index  ))\n",
    "    \n",
    "    cv_table.append(fold_train_start_index)\n",
    "    cv_table.append(fold_train2_start_index)\n",
    "    cv_table.append(fold_test_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_table.append(last_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 66091,\n",
       " 336609,\n",
       " 477711,\n",
       " 546983,\n",
       " 815783,\n",
       " 958233,\n",
       " 1024471,\n",
       " 1290282,\n",
       " 1435933,\n",
       " 1505171,\n",
       " 1771833,\n",
       " 1913985,\n",
       " 1980610,\n",
       " 2248510,\n",
       " 2390490]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples = []\n",
    "\n",
    "for i in range(0, NB_FOLDS*3, 3):\n",
    "    cv_tuples.append([df.loc[cv_table[i]:cv_table[i+1]-1, :].index.to_list(), df.loc[cv_table[i+1]:cv_table[i+2]-1, :].index.to_list(),\n",
    "                      df.loc[cv_table[i+2]:cv_table[i+3]-1, :].index.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141102"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_tuples[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#plot_cv_indices_custom(cv_tuples_generator, df.loc[:, FEATURES_LIST_TOTRAIN], (df['resp'] > 0), df['date'], \n",
    "#                         ax, 5, lw=20); \n",
    "\n",
    "#cv_tuples_generator = iter(cv_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of training set :\n",
    "#train_sets_table =  [cv_tuples[i][0] for i in range(5)]\n",
    "#sum([len(train_set_table) for train_set_table in train_sets_table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our old time series split (with overlap : required 1 neural network trained per split)\n",
    "# But in this script it's not needed because we're training 1 unique network, with a different fold strategy (non overlaped)\n",
    "#cv = PurgedGroupTimeSeriesSplit(\n",
    "#    n_splits=5,\n",
    "#    max_train_group_size=180,\n",
    "#    group_gap=20,\n",
    "#    max_test_group_size=60\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_index, test_index = next(cv.split(df, (df['resp'] > 0)*1, df['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df.loc[train_index, 'resp'] > 0).astype(np.byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_mean = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No fill na with our XGB models.\n",
    "# df.fillna(f_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list = []\n",
    "\n",
    "for fold, (train1_index, train2_index, test_index) in enumerate(cv_tuples_generator):\n",
    "    folds_list.append((train1_index, train2_index, test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_train1 = [folds_list[i][0] for i in range(5)]\n",
    "folds_list_train1_flat = [folds_list_train1_item for sublist in folds_list_train1 for folds_list_train1_item in sublist]\n",
    "folds_list_train1_unique = list(set(folds_list_train1_flat))\n",
    "\n",
    "folds_list_train2 = [folds_list[i][1] for i in range(5)]\n",
    "folds_list_train2_flat = [folds_list_train2_item for sublist in folds_list_train2 for folds_list_train2_item in sublist]\n",
    "folds_list_train2_unique = list(set(folds_list_train2_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train2_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train1_item) for folds_list_train1_item in folds_list_train1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1339691"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_train2_item) for folds_list_train2_item in folds_list_train2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337464"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_list_test = [folds_list[i][2] for i in range(5)]\n",
    "folds_list_test_flat = [folds_list_test_item for sublist in folds_list_test for folds_list_test_item in sublist]\n",
    "folds_list_test_unique = set(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(folds_list_test_item) for folds_list_test_item in folds_list_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713335"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2390490"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_list_train1_flat) + len(folds_list_train2_flat) + len(folds_list_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141980, 130)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[folds_list_test[4], FEATURES_LIST_TOTRAIN].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00880718,  0.39574469,  0.33059838,         nan,         nan,\n",
       "       -0.00498373, -0.01455459,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,  0.02650339,  0.0186391 ,  0.04320553,\n",
       "        0.05298663,  0.45417433,  0.37762691,  0.41617323,         nan,\n",
       "               nan,  0.49207956,  0.36839975,  0.50144387,  0.54379067,\n",
       "        0.53074971,  0.45673965,  0.05646874,  0.38900233,  0.37690587,\n",
       "               nan,         nan,  0.78590429,         nan,         nan,\n",
       "        0.55335406,  0.55554392,  0.55922873,  0.56139559,  0.44231975,\n",
       "        0.61884351,  0.61715568,  0.59770334,  0.59814018,  0.37738388,\n",
       "        0.23893403,  0.30802914,         nan,         nan,         nan,\n",
       "               nan,         nan, -0.0931838 ,         nan,         nan,\n",
       "               nan,         nan,         nan, -0.10154564,         nan,\n",
       "               nan,         nan,         nan,         nan,  0.39935045,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "        0.4547188 ,         nan,         nan,         nan,         nan,\n",
       "               nan,  0.41054099,         nan,         nan,         nan,\n",
       "               nan,         nan,  0.455339  ,         nan,         nan,\n",
       "               nan,         nan,         nan,  0.40349802,         nan,\n",
       "               nan,         nan,         nan,         nan,  0.44651147,\n",
       "               nan,         nan,         nan,         nan,         nan,\n",
       "               nan,         nan,         nan,         nan,         nan])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(folds_list_train1_unique + folds_list_train2_unique), FEATURES_LIST_TOTRAIN].to_numpy().mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677155"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(folds_list_train1_unique + folds_list_train2_unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training XGB model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XGB model that predicts resp n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifier_wrapper(BaseEstimator, ClassifierMixin):  \n",
    "    ''' Params passed as dictionnary to __init__, for example :\n",
    "        params_space = {\n",
    "       'features': FEATURES_LIST_TOTRAIN, \n",
    "        'random_state': 42,\n",
    "        'max_depth': 12,\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': 0.01,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.3,\n",
    "        'tree_method': 'gpu_hist'\n",
    "        }\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        self.fitted = False\n",
    "        \n",
    "        self.features = list(params['features'])\n",
    "        self.random_state = params['random_state']\n",
    "        self.max_depth = params['max_depth']\n",
    "        self.n_estimators = params['n_estimators']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.subsample = params['subsample']\n",
    "        self.colsample_bytree = params['colsample_bytree']\n",
    "        self.gamma = params['gamma']\n",
    "        self.tree_method = params['tree_method']  \n",
    "        \n",
    "        #print('Features assigned :')\n",
    "        #print(self.features)\n",
    "\n",
    "        self.model_internal = XGBClassifier(\n",
    "            random_state= self.random_state,\n",
    "            max_depth= self.max_depth,\n",
    "            n_estimators= self.n_estimators,\n",
    "            learning_rate= self.learning_rate,\n",
    "            subsample= self.subsample,\n",
    "            colsample_bytree= self.colsample_bytree,\n",
    "            tree_method= self.tree_method,\n",
    "            gamma = self.gamma,\n",
    "            #objective= 'binary:logistic',\n",
    "            #disable_default_eval_metric=True,\n",
    "            )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('Model used for fitting:')\n",
    "        print(self.model_internal)\n",
    "        self.model_internal.fit(X[self.features], y)\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        if (self.fitted == True):\n",
    "            print('predict called')\n",
    "            return(self.model_internal.predict(X[self.features]))\n",
    "        \n",
    "        else:\n",
    "            print('You must fit model first')\n",
    "            return(None)\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        if (self.fitted == True):\n",
    "            print('predict proba called')\n",
    "            return(self.model_internal.predict_proba(X[self.features]))\n",
    "        \n",
    "        else:\n",
    "            print('You must fit model first')\n",
    "            return(None)\n",
    "        \n",
    "\n",
    "    #def set_params(self, **parameters):\n",
    "    #    for parameter, value in parameters.items():\n",
    "    #        setattr(self, parameter, value)\n",
    "\n",
    "        \n",
    "    def score(self, X, y=None):        \n",
    "        print('Type of X:')\n",
    "        print(type(X))\n",
    "        \n",
    "        print('Shape of X:')\n",
    "        print(X.shape)\n",
    "        \n",
    "        print('Type of y:')\n",
    "        print(type(y))\n",
    "        \n",
    "        print('model fitted ?')\n",
    "        print(self.fitted) # Usually returns yes at this point when called by cross_val_score\n",
    "        \n",
    "        if y is None:\n",
    "            print('y is None')\n",
    "            y_preds = pd.Series(self.model_internal.predict(X.reset_index(drop=True)[self.features]))\n",
    "            \n",
    "        else: # cross_val_score goes there\n",
    "            print('y is not None')\n",
    "            y_preds = pd.Series(y)\n",
    "        \n",
    "        return(utility_function(X.reset_index(drop=True), y_preds)) \n",
    "    \n",
    "    def accuracy_score(self, X, y=None):\n",
    "        if y is None:\n",
    "            print('y is None in accuracy_score method : pass predictions as y to avoid launching predict')\n",
    "            y_preds = pd.Series(self.model_internal.predict(X.reset_index(drop=True)[self.features]))\n",
    "            \n",
    "        else: # cross_val_score goes there\n",
    "            #print('y is not None')\n",
    "            y_preds = pd.Series(y)\n",
    "            \n",
    "        return(accuracy_score(X['resp_positive'], y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:45:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.9,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate label of current step\n",
    "y_train1_resp_positive = (df.loc[folds_list_train1_unique, 'resp'] > 0).astype(np.byte)\n",
    "\n",
    "# Shift values of resp to get resp of step n-1\n",
    "y_train1_resp_n1_positive = y_train1_resp_positive.shift(1, fill_value=0)\n",
    "\n",
    "\n",
    "model_n1 = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 12,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.01,\n",
    "    subsample= 0.9,\n",
    "    colsample_bytree= 0.2,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    )\n",
    "\n",
    "model_n1.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], y_train1_resp_n1_positive, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model that predicts original fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_indexes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_indice in range(NB_FOLDS):\n",
    "    fold_indexes.append([item for sublist in folds_list[fold_indice] for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_number, fold_indexes_1fold in enumerate(fold_indexes):\n",
    "    df.loc[fold_indexes_1fold, 'fold_number'] = str(int(fold_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.shape[0] - 1, 'fold_number'] = str(int(NB_FOLDS - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    480522\n",
       "3    478052\n",
       "0    477711\n",
       "2    477700\n",
       "4    476506\n",
       "Name: fold_number, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fold_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 140)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(df['fold_number'], prefix = 'fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df, pd.get_dummies(df['fold_number'], prefix = 'fold')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=['fold_number'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fold predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:46:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 10,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.02,\n",
    "    subsample= 0.5,\n",
    "    colsample_bytree= 0.6,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    #objective= 'binary:logistic',\n",
    "    #disable_default_eval_metric=True,\n",
    "    )\n",
    "\n",
    "#model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, ['fold_'+str(i) for i in range(NB_FOLDS)]])\n",
    "model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, 'fold_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    142450\n",
       "Name: fold_number, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[folds_list_test[1]]['fold_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    38608\n",
       "1    31708\n",
       "3    25560\n",
       "2    23863\n",
       "4    21363\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[0], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    35380\n",
       "3    34525\n",
       "4    28870\n",
       "1    27587\n",
       "0    16088\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[1], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    43197\n",
       "3    41715\n",
       "2    30115\n",
       "1    15469\n",
       "0    15155\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[2], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    55001\n",
       "3    42224\n",
       "2    20747\n",
       "1    12120\n",
       "0    12060\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[3], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    56798\n",
       "3    38220\n",
       "2    19590\n",
       "0    13698\n",
       "1    13674\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model_xgb.predict(df.loc[folds_list_test[4], FEATURES_LIST_TOTRAIN])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03349742, 0.44114932, 0.2217906 , 0.24658968, 0.05697299],\n",
       "       [0.2493427 , 0.6202549 , 0.02858509, 0.032284  , 0.06953336],\n",
       "       [0.12408023, 0.64893204, 0.07998472, 0.05955048, 0.08745254],\n",
       "       ...,\n",
       "       [0.32993764, 0.14755715, 0.17608246, 0.1451546 , 0.20126821],\n",
       "       [0.15299153, 0.11114156, 0.28277832, 0.28438374, 0.16870484],\n",
       "       [0.06557676, 0.4378262 , 0.22874515, 0.11568245, 0.15216942]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.predict_proba(df.loc[folds_list_test[1], FEATURES_LIST_TOTRAIN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_scores': [0.27361766665249254, 0.19366093366093365, 0.20676136792744298, 0.29703416061680454, 0.40004225947316524]}\n",
      "{'precision_scores': [0.27361766665249254, 0.19366093366093365, 0.20676136792744298, 0.29703416061680454, 0.40004225947316524]}\n",
      "{'recall_scores': [0.27361766665249254, 0.19366093366093365, 0.20676136792744298, 0.29703416061680454, 0.40004225947316524]}\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS): \n",
    "    test_predictions = model_xgb.predict(df.loc[folds_list_test[fold_indice], FEATURES_LIST_TOTRAIN])\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions))  \n",
    "    precision_scores.append(precision_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions, average='micro'))  \n",
    "    recall_scores.append(recall_score(df.loc[folds_list_test[fold_indice], 'fold_number'], test_predictions, average='micro'))  \n",
    "\n",
    "    df_featimportance = pd.DataFrame(model_xgb.feature_importances_, index=df[FEATURES_LIST_TOTRAIN].columns, columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "    df_featimportance_cumulated = pd.concat([df_featimportance, pd.DataFrame({'% feat importance cumulé' : (df_featimportance['Importance'] / df_featimportance['Importance'].sum()).cumsum()})], axis=1)\n",
    "    #print(f'Feature importances for split {fold_indice}:')\n",
    "    #print(df_featimportance_cumulated)\n",
    "\n",
    "print({'accuracy_scores': accuracy_scores})\n",
    "print({'precision_scores': precision_scores})\n",
    "print({'recall_scores': recall_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 140)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost with fold prediction as input AND resp n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapped = XGBClassifier_wrapper({\n",
    "   #'features': ['feature_'+str(i) for i in range(130)] + [0,1,2,3,4] + ['resp_n1_predict'], \n",
    "    'features': ['feature_'+str(i) for i in range(130)] + [0,3,4] + ['resp_n1_predict'], \n",
    "    'random_state': 42,\n",
    "    'max_depth': 10,\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.02,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'gamma': None,\n",
    "    'tree_method': 'gpu_hist'        \n",
    "    #'tree_method': 'hist' # CPU\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [df, \n",
    "     pd.DataFrame(model_xgb.predict_proba(df.loc[:, FEATURES_LIST_TOTRAIN]))], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390491, 145)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'resp_n1_predict'] = model_n1.predict_proba(df.loc[:, FEATURES_LIST_TOTRAIN])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for fitting:\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.6, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.02, max_delta_step=None, max_depth=10,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=42, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=0.5, tree_method='gpu_hist',\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:49:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier_wrapper(params=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapped.fit(\n",
    "    df.loc[folds_list_train2_unique], \n",
    "    (df.loc[folds_list_train2_unique]['resp'] > 0).astype(np.byte)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(141102, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(142450, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(145651, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(142152, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "predict called\n",
      "predict proba called\n",
      "Type of X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X:\n",
      "(141980, 146)\n",
      "Type of y:\n",
      "<class 'numpy.ndarray'>\n",
      "model fitted ?\n",
      "True\n",
      "y is not None\n",
      "{'utility_score': 2201.1077946132314, 'utility_scores': [282.6849057991371, 660.7564349123575, -0.0, 0.9151242560035899, 1256.7513296457332], 'utility_score_std': 474.754559654155, 'accuracy_scores': [0.5208714263440631, 0.5191154791154791, 0.5074802095419874, 0.5142171759806404, 0.5256655867023524]}\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "accuracy_scores = []\n",
    "xgb_test_predictions_folds = []\n",
    "\n",
    "for fold_indice in range(NB_FOLDS):     \n",
    "    test_predictions = model_wrapped.predict(df.loc[folds_list_test[fold_indice], :])\n",
    "    test_predictions_probas = model_wrapped.predict_proba(df.loc[folds_list_test[fold_indice], :])[:, 1]\n",
    "    xgb_test_predictions_folds.append(test_predictions_probas)\n",
    "\n",
    "    scores.append(model_wrapped.score(df.loc[folds_list_test[fold_indice]], test_predictions))\n",
    "    accuracy_scores.append(model_wrapped.accuracy_score(df.loc[folds_list_test[fold_indice]], test_predictions))  \n",
    "\n",
    "    df_featimportance = pd.DataFrame(model_wrapped.model_internal.feature_importances_, index=FEATURES_LIST_TOTRAIN + [0,3,4] + ['resp_n1_predict'], columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "    df_featimportance_cumulated = pd.concat([df_featimportance, pd.DataFrame({'% feat importance cumulé' : (df_featimportance['Importance'] / df_featimportance['Importance'].sum()).cumsum()})], axis=1)\n",
    "    #print(f'Feature importances for split {fold_indice}:')\n",
    "    #print(df_featimportance_cumulated)\n",
    "\n",
    "print({'utility_score': sum(scores), 'utility_scores': scores, 'utility_score_std': np.std(scores), 'accuracy_scores': accuracy_scores})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Score avec resp n-1 proba et fold prediction : \n",
    "{'utility_score': 2005.4261341927581, 'utility_scores': [185.71548536644758, 720.3905799549489, -0.0, 0.4066629524023048, 1098.9134059189594], 'utility_score_std': 437.4281363458974, 'accuracy_scores': [0.5191280066901958, 0.5175078975078975, 0.5103363519646278, 0.5155889470426023, 0.5262149598535005]}  \n",
    "\n",
    "Score même chose avec proba de resp n-1 :\n",
    "{'utility_score': 2006.7004942007125, 'utility_scores': [161.07432736133686, 828.7819592596372, -0.0, 1.3074578209567136, 1015.5367497587815], 'utility_score_std': 433.30628885366, 'accuracy_scores': [0.5194327507760343, 0.5173885573885574, 0.509134849743565, 0.515117620575159, 0.525764192139738]}\n",
    "\n",
    "Score même chose avec proba resp n-1 et fold prediction seulement 0, 3 et 4 (les plus fiables) :\n",
    "{'utility_score': 2201.1077946132314, 'utility_scores': [282.6849057991371, 660.7564349123575, -0.0, 0.9151242560035899, 1256.7513296457332], 'utility_score_std': 474.754559654155, 'accuracy_scores': [0.5208714263440631, 0.5191154791154791, 0.5074802095419874, 0.5142171759806404, 0.5256655867023524]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load public models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 load : Using TensorFlow and PyTorch\n",
    "https://www.kaggle.com/yonikremer/using-tensorflow-and-pytorch/data?scriptVersionId=54551772  \n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#DATA_PATH = '../input/jane-street-market-prediction/'\n",
    "\n",
    "NFOLDS = 5\n",
    "\n",
    "TRAIN = False\n",
    "CACHE_PATH = '/home/francois/coding/OC/PJ9/blending/notebook1'\n",
    "\n",
    "def save_pickle(dic, save_path):\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(dic, f)\n",
    "\n",
    "def load_pickle(load_path):\n",
    "    with open(load_path, 'rb') as f:\n",
    "        message_dict = pickle.load(f)\n",
    "    return message_dict\n",
    "\n",
    "feat_cols = [f'feature_{i}' for i in range(130)]\n",
    "\n",
    "target_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n",
    "\n",
    "f_mean = np.load(f'{CACHE_PATH}/f_mean_online.npy')\n",
    "\n",
    "#     Making features\n",
    "all_feat_cols = [col for col in feat_cols]\n",
    "all_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n",
    "\n",
    "#     Model&Data fnc\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n",
    "        self.dropout0 = nn.Dropout(0.2)\n",
    "\n",
    "        dropout_rate = 0.2\n",
    "        hidden_size = 256\n",
    "        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n",
    "\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "        self.PReLU = nn.PReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.RReLU = nn.RReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x1 = self.dense1(x)\n",
    "        x1 = self.batch_norm1(x1)\n",
    "        x1 = self.LeakyReLU(x1)\n",
    "        x1 = self.dropout1(x1)\n",
    "\n",
    "        x = torch.cat([x, x1], 1)\n",
    "\n",
    "        x2 = self.dense2(x)\n",
    "        x2 = self.batch_norm2(x2)\n",
    "        x2 = self.LeakyReLU(x2)\n",
    "        x2 = self.dropout2(x2)\n",
    "\n",
    "        x = torch.cat([x1, x2], 1)\n",
    "\n",
    "        x3 = self.dense3(x)\n",
    "        x3 = self.batch_norm3(x3)\n",
    "        x3 = self.LeakyReLU(x3)\n",
    "        x3 = self.dropout3(x3)\n",
    "\n",
    "        x = torch.cat([x2, x3], 1)\n",
    "\n",
    "        x4 = self.dense4(x)\n",
    "        x4 = self.batch_norm4(x4)\n",
    "        x4 = self.LeakyReLU(x4)\n",
    "        x4 = self.dropout4(x4)\n",
    "\n",
    "        x = torch.cat([x3, x4], 1)\n",
    "\n",
    "        x = self.dense5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "if True:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    model_list = []\n",
    "    tmp = np.zeros(len(feat_cols))\n",
    "    for _fold in range(NFOLDS):\n",
    "        torch.cuda.empty_cache()\n",
    "        model = Model()\n",
    "        model.to(device)\n",
    "        model_weights = f\"{CACHE_PATH}/online_model{_fold}.pth\"\n",
    "        model.load_state_dict(torch.load(model_weights, map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "        model_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 19:50:32.926745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-02-19 19:50:33.711343: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-02-19 19:50:33.711395: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-02-19 19:50:33.711509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-02-19 19:50:33.712098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:09:00.0 name: GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2021-02-19 19:50:33.712118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-02-19 19:50:33.712171: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-02-19 19:50:33.712205: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-02-19 19:50:33.713206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-02-19 19:50:33.713381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-02-19 19:50:33.714329: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-02-19 19:50:33.714381: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-02-19 19:50:33.714416: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-02-19 19:50:33.714421: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-02-19 19:50:33.714543: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-02-19 19:50:33.715157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-02-19 19:50:33.715163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "2021-02-19 19:50:33.715167: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "\n",
    "\n",
    "SEED = 1111\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# fit\n",
    "def create_mlp(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    '''\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    return model\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 4096\n",
    "hidden_units = [160, 160, 160]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(SEED)\n",
    "clf = create_mlp(\n",
    "    len(feat_cols), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "clf.load_weights(f'{CACHE_PATH}/model.h5')\n",
    "\n",
    "tf_models = [clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean_NB1 = f_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 : load CNN keras\n",
    "https://www.kaggle.com/hyperbeam/cnn-using-keras/data  \n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols_NB1_NB2 = [f'feature_{i}' for i in range(130)]\n",
    "f_mean_NB2 = f_mean[1:]\n",
    "\n",
    "model_NB2 = tf.keras.models.load_model('/home/francois/coding/OC/PJ9/blending/notebook2/cnn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate base models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filled for public models. df (without fill NA) for xgboost\n",
    "f_mean_df = df.loc[:, FEATURES_LIST_TOTRAIN].mean(axis=0)\n",
    "df_filled = df.copy(deep=True)\n",
    "df_filled.fillna(f_mean_df, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of logistic regression for fold 0:\n",
      "[[0.73505422 1.14404314 0.88368141]]\n",
      "Intercept for fold 0\n",
      "[-1.40017756]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.47032119]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.52967881 0.47032119]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 1:\n",
      "[[ 9.28878195 -1.66280907 -1.49119587]]\n",
      "Intercept for fold 1\n",
      "[-3.067424]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.48399739]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.51600261 0.48399739]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 2:\n",
      "[[11.08829042 -2.55277271 -3.14534251]]\n",
      "Intercept for fold 2\n",
      "[-2.70574782]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.48547541]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.51452459 0.48547541]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 3:\n",
      "[[11.01947756 -1.93777247 -3.87512962]]\n",
      "Intercept for fold 3\n",
      "[-2.61857389]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.55365945]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.44634055 0.55365945]]\n",
      "\n",
      "\n",
      "Coefficients of logistic regression for fold 4:\n",
      "[[ 9.69298325 -2.21555271 -1.55129055]]\n",
      "Intercept for fold 4\n",
      "[-2.99580242]\n",
      "Manually calculating proba for instance 0:\n",
      "[0.50903267]\n",
      "Predict proba by scikit learn for instance 0:\n",
      "[[0.49096733 0.50903267]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_coefs = []\n",
    "logreg_intercepts = []\n",
    "\n",
    "for fold_indice in range(5):\n",
    "    df_np = df_filled.loc[folds_list_test[fold_indice], ['feature_' + str(i) for i in range(130)]].values \n",
    "\n",
    "    feature_inp = np.concatenate((\n",
    "        df_np,\n",
    "        (df_np[:, 41] + df_np[:, 42] + df_np[:, 43]).reshape(df_np.shape[0], 1), # cross_41_42_43\n",
    "        (df_np[:, 1] / (df_np[:, 2] + 1e-5)).reshape(df_np.shape[0], 1), # cross_1_2\n",
    "    ), axis=1)\n",
    "\n",
    "    # Predictions model 1\n",
    "    with torch.no_grad():\n",
    "        preds_nb1 = np.median(\n",
    "                        np.stack(\n",
    "                            [model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().numpy() for model in model_list]\n",
    "                        ).sum(axis=0) / 5, axis=1)\n",
    "\n",
    "    del feature_inp\n",
    "\n",
    "    # Predictions model 2\n",
    "    preds_nb2 = np.median(np.mean([model(df_np, training = False).numpy() for model in tf_models],axis=0), axis=1)\n",
    "\n",
    "    batch_size = 20000\n",
    "\n",
    "    # Predictions model 3\n",
    "    preds_nb3 = np.empty([0, ])\n",
    "\n",
    "    for min_bound in range(0, df_np.shape[0], batch_size):\n",
    "        nb_elems = min(batch_size, df_np.shape[0] - min_bound)\n",
    "        max_bound = min_bound + nb_elems - 1\n",
    "\n",
    "        #print(f'min_bound = {min_bound}, max_bound = {max_bound}')\n",
    "\n",
    "        preds_nb3 = np.append(preds_nb3, np.median(np.mean([model_NB2(df_np[min_bound:max_bound+1, :].reshape(-1, 130, 1), training = False).numpy()],axis=0), axis=1), axis=0)\n",
    "\n",
    "    preds_xgb = xgb_test_predictions_folds[fold_indice]\n",
    "    \n",
    "    #preds = np.stack([preds_nb1, preds_nb2, preds_nb3, preds_xgb], axis=1)\n",
    "    preds = np.stack([preds_nb1, preds_nb3, preds_xgb], axis=1) # Remove preds_nb2 for submit speed constraint :(\n",
    "\n",
    "    # Train logistic regression\n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(preds, df.loc[folds_list_test[fold_indice], 'resp_positive'])\n",
    "\n",
    "    print(f'Coefficients of logistic regression for fold {fold_indice}:')\n",
    "    print(logreg.coef_)\n",
    "    logreg_coefs.append(logreg.coef_[0])\n",
    "    \n",
    "    print(f'Intercept for fold {fold_indice}')\n",
    "    print(logreg.intercept_)\n",
    "    logreg_intercepts.append(logreg.intercept_[0])\n",
    "\n",
    "    print('Manually calculating proba for instance 0:')\n",
    "    print(1/(1 + np.exp(-(np.dot(logreg.coef_[0], preds[0]) + logreg.intercept_))) )\n",
    "\n",
    "    print('Predict proba by scikit learn for instance 0:')\n",
    "    print(logreg.predict_proba(preds[0:1]))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.73505422,  1.14404314,  0.88368141],\n",
       "       [ 9.28878195, -1.66280907, -1.49119587],\n",
       "       [11.08829042, -2.55277271, -3.14534251],\n",
       "       [11.01947756, -1.93777247, -3.87512962],\n",
       "       [ 9.69298325, -2.21555271, -1.55129055]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(logreg_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.400177560491695,\n",
       " -3.067424002194023,\n",
       " -2.705747816224323,\n",
       " -2.6185738946627026,\n",
       " -2.995802423049205]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.36491748, -1.44497276, -1.83585543])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.stack(logreg_coefs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.5575451393243895"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logreg_intercepts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model retrain on maximum data possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to retrain meta model on all data except data used to train base model (to avoid base model providing overfitted predictions),  \n",
    "and retrain base models (resp n-1 prediction and fold prediction) on all data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain meta model (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for fitting:\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.6, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.02, max_delta_step=None, max_depth=10,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=42, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=0.5, tree_method='gpu_hist',\n",
      "              validate_parameters=None, verbosity=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:40:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier_wrapper(params=None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapped_final = XGBClassifier_wrapper({\n",
    "   #'features': ['feature_'+str(i) for i in range(130)] + [0,1,2,3,4] + ['resp_n1_predict'], \n",
    "    'features': ['feature_'+str(i) for i in range(130)] + [0,3,4] + ['resp_n1_predict'], \n",
    "    'random_state': 42,\n",
    "    'max_depth': 10,\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.02,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'gamma': None,\n",
    "    'tree_method': 'gpu_hist'        \n",
    "    #'tree_method': 'hist' # CPU\n",
    "    })\n",
    "\n",
    "model_wrapped_final.fit(\n",
    "    df, \n",
    "    (df['resp'] > 0).astype(np.byte)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapped_final.model_internal.save_model(MODEL_FILE_META)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featimportance_final = pd.DataFrame(model_wrapped.model_internal.feature_importances_, index=FEATURES_LIST_TOTRAIN + [0,3,4] + ['resp_n1_predict'], columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "df_featimportance_cumulated_final = pd.concat([df_featimportance, pd.DataFrame({'% feat importance cumulé' : (df_featimportance['Importance'] / df_featimportance['Importance'].sum()).cumsum()})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "      <th>% feat importance cumulé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature_42</th>\n",
       "      <td>0.013739</td>\n",
       "      <td>0.013739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_43</th>\n",
       "      <td>0.013437</td>\n",
       "      <td>0.027176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_45</th>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.040407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_41</th>\n",
       "      <td>0.013018</td>\n",
       "      <td>0.053426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_44</th>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.066004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_63</th>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.077999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_39</th>\n",
       "      <td>0.011919</td>\n",
       "      <td>0.089918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_61</th>\n",
       "      <td>0.011712</td>\n",
       "      <td>0.101630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_5</th>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.112847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_6</th>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.123941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_27</th>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.134846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_62</th>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.145697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_60</th>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.156432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_107</th>\n",
       "      <td>0.010168</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_83</th>\n",
       "      <td>0.010065</td>\n",
       "      <td>0.176665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_3</th>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.186597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_4</th>\n",
       "      <td>0.009886</td>\n",
       "      <td>0.196483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_38</th>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.206292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.216053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_40</th>\n",
       "      <td>0.009753</td>\n",
       "      <td>0.225806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009349</td>\n",
       "      <td>0.235155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_119</th>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.244474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_120</th>\n",
       "      <td>0.009314</td>\n",
       "      <td>0.253788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_37</th>\n",
       "      <td>0.009265</td>\n",
       "      <td>0.263052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.272078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_77</th>\n",
       "      <td>0.008942</td>\n",
       "      <td>0.281019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_114</th>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.289883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_55</th>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.298714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_64</th>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.307474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_113</th>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.316028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_95</th>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.324546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_121</th>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.333041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_20</th>\n",
       "      <td>0.008483</td>\n",
       "      <td>0.341525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_124</th>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.349997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_90</th>\n",
       "      <td>0.008446</td>\n",
       "      <td>0.358443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_102</th>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.366827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_68</th>\n",
       "      <td>0.008243</td>\n",
       "      <td>0.375070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_66</th>\n",
       "      <td>0.008105</td>\n",
       "      <td>0.383175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_125</th>\n",
       "      <td>0.008020</td>\n",
       "      <td>0.391196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_89</th>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.399203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_57</th>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.407159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_101</th>\n",
       "      <td>0.007893</td>\n",
       "      <td>0.415053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_126</th>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.422901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_28</th>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.430683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_71</th>\n",
       "      <td>0.007502</td>\n",
       "      <td>0.438185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_65</th>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.445682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_58</th>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.453176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_108</th>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.460642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_78</th>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.468035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_84</th>\n",
       "      <td>0.007372</td>\n",
       "      <td>0.475407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_67</th>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.482719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_92</th>\n",
       "      <td>0.007264</td>\n",
       "      <td>0.489983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_96</th>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.497177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_127</th>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.504360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_8</th>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_69</th>\n",
       "      <td>0.007081</td>\n",
       "      <td>0.518581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_31</th>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.525644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_26</th>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.532660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_18</th>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.539654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_110</th>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.546603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_70</th>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.553552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_104</th>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_17</th>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.567399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_116</th>\n",
       "      <td>0.006904</td>\n",
       "      <td>0.574303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_59</th>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.581155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_36</th>\n",
       "      <td>0.006824</td>\n",
       "      <td>0.587979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_53</th>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.594766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_22</th>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.601532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_50</th>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.608286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_32</th>\n",
       "      <td>0.006746</td>\n",
       "      <td>0.615032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_128</th>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.621734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_7</th>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.628413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_49</th>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.635031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_21</th>\n",
       "      <td>0.006617</td>\n",
       "      <td>0.641648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_72</th>\n",
       "      <td>0.006609</td>\n",
       "      <td>0.648257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_33</th>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.654857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_23</th>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.661453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_86</th>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.668042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_98</th>\n",
       "      <td>0.006502</td>\n",
       "      <td>0.674543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_34</th>\n",
       "      <td>0.006396</td>\n",
       "      <td>0.680939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_10</th>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.687306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_24</th>\n",
       "      <td>0.006362</td>\n",
       "      <td>0.693668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_111</th>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.700020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_47</th>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.706307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_129</th>\n",
       "      <td>0.006281</td>\n",
       "      <td>0.712588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_117</th>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.718857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_56</th>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.725127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_48</th>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.731395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_30</th>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.737659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_25</th>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.743923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_12</th>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.750165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_35</th>\n",
       "      <td>0.006240</td>\n",
       "      <td>0.756405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_122</th>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.762628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_54</th>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.768843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_46</th>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.775003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_123</th>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.781153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_105</th>\n",
       "      <td>0.006142</td>\n",
       "      <td>0.787295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_51</th>\n",
       "      <td>0.006141</td>\n",
       "      <td>0.793436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_29</th>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.799571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_19</th>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.805655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_11</th>\n",
       "      <td>0.006058</td>\n",
       "      <td>0.811713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_93</th>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.817768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_2</th>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.823799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_9</th>\n",
       "      <td>0.005997</td>\n",
       "      <td>0.829796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_115</th>\n",
       "      <td>0.005977</td>\n",
       "      <td>0.835773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_118</th>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.841740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_94</th>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.847701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_80</th>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.853660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_1</th>\n",
       "      <td>0.005943</td>\n",
       "      <td>0.859603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_99</th>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.865529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_109</th>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.871418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_74</th>\n",
       "      <td>0.005883</td>\n",
       "      <td>0.877300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_14</th>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.883182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_87</th>\n",
       "      <td>0.005864</td>\n",
       "      <td>0.889046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_112</th>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.894896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_106</th>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.900742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_97</th>\n",
       "      <td>0.005805</td>\n",
       "      <td>0.906547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_82</th>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.912277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_81</th>\n",
       "      <td>0.005720</td>\n",
       "      <td>0.917997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_91</th>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.923714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_85</th>\n",
       "      <td>0.005716</td>\n",
       "      <td>0.929430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_76</th>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.935113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_16</th>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.940775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_100</th>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.946409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_15</th>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.952032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_52</th>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.957627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_79</th>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.963222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_73</th>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.968747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_13</th>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.974229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_88</th>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.979681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_103</th>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.985125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_75</th>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.990532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resp_n1_predict</th>\n",
       "      <td>0.005354</td>\n",
       "      <td>0.995886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_0</th>\n",
       "      <td>0.004114</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Importance  % feat importance cumulé\n",
       "feature_42         0.013739                  0.013739\n",
       "feature_43         0.013437                  0.027176\n",
       "feature_45         0.013231                  0.040407\n",
       "feature_41         0.013018                  0.053426\n",
       "feature_44         0.012578                  0.066004\n",
       "feature_63         0.011994                  0.077999\n",
       "feature_39         0.011919                  0.089918\n",
       "feature_61         0.011712                  0.101630\n",
       "feature_5          0.011217                  0.112847\n",
       "feature_6          0.011094                  0.123941\n",
       "feature_27         0.010905                  0.134846\n",
       "feature_62         0.010851                  0.145697\n",
       "feature_60         0.010735                  0.156432\n",
       "feature_107        0.010168                  0.166600\n",
       "feature_83         0.010065                  0.176665\n",
       "feature_3          0.009933                  0.186597\n",
       "feature_4          0.009886                  0.196483\n",
       "feature_38         0.009809                  0.206292\n",
       "4                  0.009761                  0.216053\n",
       "feature_40         0.009753                  0.225806\n",
       "0                  0.009349                  0.235155\n",
       "feature_119        0.009319                  0.244474\n",
       "feature_120        0.009314                  0.253788\n",
       "feature_37         0.009265                  0.263052\n",
       "3                  0.009025                  0.272078\n",
       "feature_77         0.008942                  0.281019\n",
       "feature_114        0.008864                  0.289883\n",
       "feature_55         0.008831                  0.298714\n",
       "feature_64         0.008761                  0.307474\n",
       "feature_113        0.008554                  0.316028\n",
       "feature_95         0.008518                  0.324546\n",
       "feature_121        0.008495                  0.333041\n",
       "feature_20         0.008483                  0.341525\n",
       "feature_124        0.008472                  0.349997\n",
       "feature_90         0.008446                  0.358443\n",
       "feature_102        0.008384                  0.366827\n",
       "feature_68         0.008243                  0.375070\n",
       "feature_66         0.008105                  0.383175\n",
       "feature_125        0.008020                  0.391196\n",
       "feature_89         0.008008                  0.399203\n",
       "feature_57         0.007956                  0.407159\n",
       "feature_101        0.007893                  0.415053\n",
       "feature_126        0.007848                  0.422901\n",
       "feature_28         0.007782                  0.430683\n",
       "feature_71         0.007502                  0.438185\n",
       "feature_65         0.007497                  0.445682\n",
       "feature_58         0.007494                  0.453176\n",
       "feature_108        0.007466                  0.460642\n",
       "feature_78         0.007393                  0.468035\n",
       "feature_84         0.007372                  0.475407\n",
       "feature_67         0.007312                  0.482719\n",
       "feature_92         0.007264                  0.489983\n",
       "feature_96         0.007194                  0.497177\n",
       "feature_127        0.007183                  0.504360\n",
       "feature_8          0.007140                  0.511500\n",
       "feature_69         0.007081                  0.518581\n",
       "feature_31         0.007063                  0.525644\n",
       "feature_26         0.007016                  0.532660\n",
       "feature_18         0.006994                  0.539654\n",
       "feature_110        0.006950                  0.546603\n",
       "feature_70         0.006949                  0.553552\n",
       "feature_104        0.006927                  0.560480\n",
       "feature_17         0.006919                  0.567399\n",
       "feature_116        0.006904                  0.574303\n",
       "feature_59         0.006853                  0.581155\n",
       "feature_36         0.006824                  0.587979\n",
       "feature_53         0.006787                  0.594766\n",
       "feature_22         0.006766                  0.601532\n",
       "feature_50         0.006754                  0.608286\n",
       "feature_32         0.006746                  0.615032\n",
       "feature_128        0.006702                  0.621734\n",
       "feature_7          0.006679                  0.628413\n",
       "feature_49         0.006618                  0.635031\n",
       "feature_21         0.006617                  0.641648\n",
       "feature_72         0.006609                  0.648257\n",
       "feature_33         0.006599                  0.654857\n",
       "feature_23         0.006596                  0.661453\n",
       "feature_86         0.006589                  0.668042\n",
       "feature_98         0.006502                  0.674543\n",
       "feature_34         0.006396                  0.680939\n",
       "feature_10         0.006367                  0.687306\n",
       "feature_24         0.006362                  0.693668\n",
       "feature_111        0.006352                  0.700020\n",
       "feature_47         0.006287                  0.706307\n",
       "feature_129        0.006281                  0.712588\n",
       "feature_117        0.006270                  0.718857\n",
       "feature_56         0.006269                  0.725127\n",
       "feature_48         0.006269                  0.731395\n",
       "feature_30         0.006264                  0.737659\n",
       "feature_25         0.006264                  0.743923\n",
       "feature_12         0.006243                  0.750165\n",
       "feature_35         0.006240                  0.756405\n",
       "feature_122        0.006222                  0.762628\n",
       "feature_54         0.006216                  0.768843\n",
       "feature_46         0.006160                  0.775003\n",
       "feature_123        0.006150                  0.781153\n",
       "feature_105        0.006142                  0.787295\n",
       "feature_51         0.006141                  0.793436\n",
       "feature_29         0.006135                  0.799571\n",
       "feature_19         0.006083                  0.805655\n",
       "feature_11         0.006058                  0.811713\n",
       "feature_93         0.006055                  0.817768\n",
       "feature_2          0.006030                  0.823799\n",
       "feature_9          0.005997                  0.829796\n",
       "feature_115        0.005977                  0.835773\n",
       "feature_118        0.005967                  0.841740\n",
       "feature_94         0.005961                  0.847701\n",
       "feature_80         0.005959                  0.853660\n",
       "feature_1          0.005943                  0.859603\n",
       "feature_99         0.005926                  0.865529\n",
       "feature_109        0.005889                  0.871418\n",
       "feature_74         0.005883                  0.877300\n",
       "feature_14         0.005882                  0.883182\n",
       "feature_87         0.005864                  0.889046\n",
       "feature_112        0.005850                  0.894896\n",
       "feature_106        0.005846                  0.900742\n",
       "feature_97         0.005805                  0.906547\n",
       "feature_82         0.005729                  0.912277\n",
       "feature_81         0.005720                  0.917997\n",
       "feature_91         0.005717                  0.923714\n",
       "feature_85         0.005716                  0.929430\n",
       "feature_76         0.005683                  0.935113\n",
       "feature_16         0.005662                  0.940775\n",
       "feature_100        0.005634                  0.946409\n",
       "feature_15         0.005623                  0.952032\n",
       "feature_52         0.005595                  0.957627\n",
       "feature_79         0.005595                  0.963222\n",
       "feature_73         0.005525                  0.968747\n",
       "feature_13         0.005482                  0.974229\n",
       "feature_88         0.005451                  0.979681\n",
       "feature_103        0.005444                  0.985125\n",
       "feature_75         0.005408                  0.990532\n",
       "resp_n1_predict    0.005354                  0.995886\n",
       "feature_0          0.004114                  1.000000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_featimportance_cumulated_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain resp n-1 model (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/janestreet/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:46:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.2, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.9,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate label of current step\n",
    "y_train1_resp_positive = (df.loc[:, 'resp'] > 0).astype(np.byte)\n",
    "\n",
    "# Shift values of resp to get resp of step n-1\n",
    "y_train1_resp_n1_positive = y_train1_resp_positive.shift(1, fill_value=0)\n",
    "\n",
    "\n",
    "model_n1_final = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 12,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.01,\n",
    "    subsample= 0.9,\n",
    "    colsample_bytree= 0.2,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    )\n",
    "\n",
    "model_n1_final.fit(df.loc[:, FEATURES_LIST_TOTRAIN], y_train1_resp_n1_positive, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n1_final.save_model(MODEL_FILE_RESPN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain fold model (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:48:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=0,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=24, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.5,\n",
       "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb_final = XGBClassifier(\n",
    "    random_state= 42,\n",
    "    max_depth= 10,\n",
    "    n_estimators= 500,\n",
    "    learning_rate= 0.02,\n",
    "    subsample= 0.5,\n",
    "    colsample_bytree= 0.6,\n",
    "    tree_method= 'gpu_hist',\n",
    "    gamma = None,\n",
    "    #objective= 'binary:logistic',\n",
    "    #disable_default_eval_metric=True,\n",
    "    )\n",
    "\n",
    "#model_xgb.fit(df.loc[folds_list_train1_unique, FEATURES_LIST_TOTRAIN], df.loc[folds_list_train1_unique, ['fold_'+str(i) for i in range(NB_FOLDS)]])\n",
    "model_xgb_final.fit(df.loc[:, FEATURES_LIST_TOTRAIN], df.loc[:, 'fold_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_final.save_model(MODEL_FILE_FOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check possible return values for fold 3 that does not perform well :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fold3_resp_positive = df.loc[folds_list_test[3]].query('resp > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2201.1234249072004"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_fold3_resp_positive['weight'] * df_fold3_resp_positive['resp']).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
