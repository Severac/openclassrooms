{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openclassrooms PJ4 : transats dataset : modelisation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import qgrid\n",
    "\n",
    "import glob\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "SAMPLED_DATA = False  # If True : data is sampled (1000 instances only) for faster testing purposes\n",
    "\n",
    "DATA_PATH = os.path.join(\"datasets\", \"transats\")\n",
    "DATA_PATH = os.path.join(DATA_PATH, \"out\")\n",
    "\n",
    "DATA_PATH_FILE_INPUT = os.path.join(DATA_PATH, \"transats_metadata_transformed.csv\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9] # Taille par d√©faut des figures de matplotlib\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#import common_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qgrid_show(df):\n",
    "    display(qgrid.show_grid(df, grid_options={'forceFitColumns': False, 'defaultColumnWidth': 170}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_column_information(df, column_name):\n",
    "    column_type = df.dtypes[column_name]\n",
    "    print(f'Column {column_name}, type {column_type}\\n')\n",
    "    print('--------------------------')\n",
    "\n",
    "    print(df[[column_name]].groupby(column_name).size().sort_values(ascending=False))\n",
    "    print(df[column_name].unique())    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_percent_complete(df):\n",
    "    not_na = 100 - (df.isnull().sum() * 100 / len(df))\n",
    "    not_na_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                     'percent_complete': not_na}).sort_values(by='percent_complete', ascending=False)\n",
    "    display(not_na_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_features(df, all_features):\n",
    "    quantitative_features = []\n",
    "    qualitative_features = []\n",
    "    features_todrop = []\n",
    "\n",
    "    for feature_name in all_features:\n",
    "        if (df[feature_name].dtype == 'object'):\n",
    "            qualitative_features.append(feature_name)\n",
    "\n",
    "        else:\n",
    "            quantitative_features.append(feature_name)\n",
    "\n",
    "    print(f'Quantitative features : {quantitative_features} \\n')\n",
    "    print(f'Qualitative features : {qualitative_features} \\n')  \n",
    "    \n",
    "    return quantitative_features, qualitative_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhmm timed features formatted\n",
    "feats_hhmm = ['CRS_DEP_TIME',  'CRS_ARR_TIME']\n",
    "\n",
    "df = pd.read_csv(DATA_PATH_FILE_INPUT, sep=',', header=0, encoding='utf-8', low_memory=False, parse_dates=feats_hhmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5547828, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ORIGIN</td>\n",
       "      <td>ORIGIN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MONTH</td>\n",
       "      <td>MONTH</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DAY_OF_MONTH</td>\n",
       "      <td>DAY_OF_MONTH</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>UNIQUE_CARRIER</td>\n",
       "      <td>UNIQUE_CARRIER</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DEST</td>\n",
       "      <td>DEST</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DISTANCE</td>\n",
       "      <td>DISTANCE</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARR_DELAY</td>\n",
       "      <td>ARR_DELAY</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DEP_DELAY</td>\n",
       "      <td>DEP_DELAY</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TAXI_OUT</td>\n",
       "      <td>TAXI_OUT</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TAIL_NUM</td>\n",
       "      <td>TAIL_NUM</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       column_name  percent_complete\n",
       "ORIGIN                      ORIGIN             100.0\n",
       "CRS_DEP_TIME          CRS_DEP_TIME             100.0\n",
       "MONTH                        MONTH             100.0\n",
       "DAY_OF_MONTH          DAY_OF_MONTH             100.0\n",
       "DAY_OF_WEEK            DAY_OF_WEEK             100.0\n",
       "UNIQUE_CARRIER      UNIQUE_CARRIER             100.0\n",
       "DEST                          DEST             100.0\n",
       "CRS_ARR_TIME          CRS_ARR_TIME             100.0\n",
       "DISTANCE                  DISTANCE             100.0\n",
       "CRS_ELAPSED_TIME  CRS_ELAPSED_TIME             100.0\n",
       "ARR_DELAY                ARR_DELAY             100.0\n",
       "DEP_DELAY                DEP_DELAY             100.0\n",
       "TAXI_OUT                  TAXI_OUT             100.0\n",
       "TAIL_NUM                  TAIL_NUM             100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_percent_complete(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor column_name in df.columns:\\n    print_column_information(df, column_name)\\n    \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for column_name in df.columns:\n",
    "    print_column_information(df, column_name)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantitative features : ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'] \n",
      "\n",
      "Qualitative features : ['ORIGIN', 'CRS_DEP_TIME', 'UNIQUE_CARRIER', 'DEST', 'CRS_ARR_TIME', 'TAIL_NUM'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below are feature from dataset that we decided to keep: \n",
    "all_features = ['ORIGIN','CRS_DEP_TIME','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','UNIQUE_CARRIER','DEST','CRS_ARR_TIME','DISTANCE','CRS_ELAPSED_TIME','ARR_DELAY','DEP_DELAY', 'TAXI_OUT', 'TAIL_NUM']\n",
    "\n",
    "model1_features = ['ORIGIN','CRS_DEP_TIME','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','UNIQUE_CARRIER','DEST','CRS_ARR_TIME','DISTANCE','CRS_ELAPSED_TIME']\n",
    "model1_label = 'ARR_DELAY'\n",
    "\n",
    "quantitative_features = []\n",
    "qualitative_features = []\n",
    "features_todrop = []\n",
    "\n",
    "for feature_name in all_features:\n",
    "    if (df[feature_name].dtype == 'object'):\n",
    "        qualitative_features.append(feature_name)\n",
    "        \n",
    "    else:\n",
    "        quantitative_features.append(feature_name)\n",
    "\n",
    "print(f'Quantitative features : {quantitative_features} \\n')\n",
    "print(f'Qualitative features : {qualitative_features} \\n')        \n",
    "        \n",
    "\n",
    "#Commented out : no drop of features\n",
    "#for df_column in df.columns:\n",
    "#    if df_column not in all_features:\n",
    "#        features_todrop.append(df_column)\n",
    "#        \n",
    "#print(f'Features to drop : {features_todrop} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train set, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "df_train = df_train.copy()\n",
    "df_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SAMPLED_DATA == True):\n",
    "    df_train = df_train.sample(1200000).copy(deep=True)\n",
    "    df = df.loc[df_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3352894</td>\n",
       "      <td>HPN</td>\n",
       "      <td>1537</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>B6</td>\n",
       "      <td>PBI</td>\n",
       "      <td>1829</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>N339JB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268114</td>\n",
       "      <td>ORD</td>\n",
       "      <td>1220</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>UA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1509</td>\n",
       "      <td>925.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>N79402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2193866</td>\n",
       "      <td>SAN</td>\n",
       "      <td>1335</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>UA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1703</td>\n",
       "      <td>853.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>N17244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3874513</td>\n",
       "      <td>HDN</td>\n",
       "      <td>1620</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>OO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>1738</td>\n",
       "      <td>763.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>N785SK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2213800</td>\n",
       "      <td>ORD</td>\n",
       "      <td>0730</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>UA</td>\n",
       "      <td>BOS</td>\n",
       "      <td>1044</td>\n",
       "      <td>867.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>N801UA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570006</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1355</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>WN</td>\n",
       "      <td>LAX</td>\n",
       "      <td>1520</td>\n",
       "      <td>862.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>N455WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2234489</td>\n",
       "      <td>SFO</td>\n",
       "      <td>1725</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>VX</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2250</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>N625VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4926484</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1015</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>UA</td>\n",
       "      <td>SJU</td>\n",
       "      <td>1630</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>N67812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4304572</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1950</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>UA</td>\n",
       "      <td>DFW</td>\n",
       "      <td>2115</td>\n",
       "      <td>224.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>N834UA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692743</td>\n",
       "      <td>CHA</td>\n",
       "      <td>1621</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>OO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>1716</td>\n",
       "      <td>106.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>N427SW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4993045 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ORIGIN CRS_DEP_TIME  MONTH  DAY_OF_MONTH  DAY_OF_WEEK UNIQUE_CARRIER  \\\n",
       "3352894    HPN         1537      3            29            2             B6   \n",
       "1268114    ORD         1220      6            22            3             UA   \n",
       "2193866    SAN         1335     12            26            1             UA   \n",
       "3874513    HDN         1620      1            22            5             OO   \n",
       "2213800    ORD         0730     12            12            1             UA   \n",
       "...        ...          ...    ...           ...          ...            ...   \n",
       "1570006    DEN         1355      5             7            6             WN   \n",
       "2234489    SFO         1725     12            13            2             VX   \n",
       "4926484    IAH         1015     11            11            5             UA   \n",
       "4304572    IAH         1950     10             4            2             UA   \n",
       "1692743    CHA         1621      5            15            7             OO   \n",
       "\n",
       "        DEST CRS_ARR_TIME  DISTANCE  CRS_ELAPSED_TIME  ARR_DELAY  DEP_DELAY  \\\n",
       "3352894  PBI         1829    1056.0             172.0       10.0        1.0   \n",
       "1268114  IAH         1509     925.0             169.0      175.0      184.0   \n",
       "2193866  DEN         1703     853.0             148.0      -12.0        5.0   \n",
       "3874513  LAX         1738     763.0             138.0      -16.0      -10.0   \n",
       "2213800  BOS         1044     867.0             134.0       53.0       52.0   \n",
       "...      ...          ...       ...               ...        ...        ...   \n",
       "1570006  LAX         1520     862.0             145.0       13.0        5.0   \n",
       "2234489  DAL         2250    1476.0             205.0      -12.0       -5.0   \n",
       "4926484  SJU         1630    2007.0             255.0       -5.0        1.0   \n",
       "4304572  DFW         2115     224.0              85.0      -15.0       -2.0   \n",
       "1692743  ATL         1716     106.0              55.0       64.0       61.0   \n",
       "\n",
       "         TAXI_OUT TAIL_NUM  \n",
       "3352894       9.0   N339JB  \n",
       "1268114      23.0   N79402  \n",
       "2193866      20.0   N17244  \n",
       "3874513       9.0   N785SK  \n",
       "2213800      31.0   N801UA  \n",
       "...           ...      ...  \n",
       "1570006      16.0   N455WN  \n",
       "2234489      11.0   N625VA  \n",
       "4926484      20.0   N67812  \n",
       "4304572      18.0   N834UA  \n",
       "1692743      28.0   N427SW  \n",
       "\n",
       "[4993045 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.loc[df_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nColumnTransformer([\\n        ('standardscaler_specific', StandardScaler(), ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'])\\n    ], remainder='passthrough')\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import statistics\n",
    "\n",
    "\n",
    "'''\n",
    "Cette fonction fait un 1 hot encoding des features qui sont des cat√©gories\n",
    "Old function not used anymore\n",
    "'''\n",
    "    \n",
    "def add_categorical_features_1hot(df, categorical_features_totransform):\n",
    "    #df.drop(labels=categorical_features_totransform, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #df_encoded = pd.get_dummies(df, columns=categorical_features_totransform, sparse=True)\n",
    "    \n",
    "    for feature_totransform in categorical_features_totransform:\n",
    "        print(f'Adding 1hot Feature : {feature_totransform}')\n",
    "        \n",
    "        print('First')\n",
    "        df_transformed = df[feature_totransform].str.get_dummies().add_prefix(feature_totransform +'_')   \n",
    "        \n",
    "        #df_new = pd.get_dummies(df, columns=['ORIGIN'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #df.drop(labels=feature_totransform, axis=1, inplace=True)\n",
    "        print('Second')\n",
    "        del df[feature_totransform]\n",
    "        \n",
    "        print('Third')\n",
    "        df = pd.concat([df, df_transformed], axis=1)\n",
    "        \n",
    "    return(df)\n",
    "\n",
    "\n",
    "class HHMM_to_Minutes(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_toconvert = ['CRS_DEP_TIME', 'CRS_ARR_TIME']):\n",
    "        self.features_toconvert = features_toconvert\n",
    "        return None\n",
    "    \n",
    "    def fit(self, df):      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):       \n",
    "        for feature_toconvert in self.features_toconvert:\n",
    "            print(f'Converting feature {feature_toconvert}\\n')\n",
    "            #print('1\\n')\n",
    "            df_concat = pd.concat([df[feature_toconvert].str.slice(start=0,stop=2, step=1),df[feature_toconvert].str.slice(start=2,stop=4, step=1)], axis=1).astype(int).copy(deep=True)\n",
    "                    \n",
    "            #print('2\\n')\n",
    "            df[feature_toconvert] = (df_concat.iloc[:, [0]] * 60 + df_concat.iloc[:, [1]])[feature_toconvert]\n",
    "            \n",
    "            #print('3\\n')\n",
    "        \n",
    "        return(df)\n",
    "\n",
    "    \n",
    "'''\n",
    "class CategoricalFeatures1HotEncoder_old(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_features_totransform=['ORIGIN', 'UNIQUE_CARRIER', 'DEST']):\n",
    "        self.categorical_features_totransform = categorical_features_totransform\n",
    "    \n",
    "    def fit(self, df, labels=None):      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        # /!\\ Array will not have the same shape if we fit an ensemble of samples that have less values than total dataset\n",
    "        df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=True)  # Sparse allows to gain memory. But then, standardscale must be with_mean=False\n",
    "        #df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=False)\n",
    "        print('type of df : ' + str(type(df_encoded)))\n",
    "        return(df_encoded)\n",
    "'''\n",
    "\n",
    "class CategoricalFeatures1HotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_features_totransform=['ORIGIN', 'UNIQUE_CARRIER', 'DEST']):\n",
    "        self.categorical_features_totransform = categorical_features_totransform\n",
    "        self.fitted = False\n",
    "        self.all_feature_values = {}\n",
    "        #self.df_encoded = None\n",
    "    \n",
    "    def fit(self, df, labels=None):      \n",
    "        print('Fit data')\n",
    "        for feature_name in self.categorical_features_totransform:\n",
    "            self.all_feature_values[feature_name] = feature_name + '_' + df[feature_name].unique()\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if (self.fitted == False):\n",
    "            self.fit(df)\n",
    "        \n",
    "        print('Transform data')\n",
    "        print('1hot encode categorical features...')\n",
    "        df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=True)  # Sparse allows to gain memory. But then, standardscale must be with_mean=False\n",
    "        #df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=False)\n",
    "        \n",
    "        # Get category values that were in fitted data, but that are not in data to transform \n",
    "        for feature_name, feature_values in self.all_feature_values.items():\n",
    "            diff_columns = list(set(feature_values) - set(df_encoded.columns.tolist()))\n",
    "            print(f'Column values that were in fitted data but not in current data: {diff_columns}')\n",
    "        \n",
    "            if (len(diff_columns) > 0):\n",
    "                print('Adding those column with 0 values to the DataFrme...')\n",
    "                # Create columns with 0 for the above categories, in order to preserve same matrix shape between train et test set\n",
    "                zeros_dict = dict.fromkeys(diff_columns, 0)\n",
    "                df_encoded.assign(**zeros_dict)\n",
    "        \n",
    "        print('type of df : ' + str(type(df_encoded)))\n",
    "        return(df_encoded)\n",
    "    \n",
    "    \n",
    "class FeaturesSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_toselect = None):  # If None : every column is kept, nothing is done\n",
    "        self.features_toselect = features_toselect\n",
    "    \n",
    "    def fit(self, df, labels=None):      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):       \n",
    "        if (self.features_toselect != None):\n",
    "            filter_cols = [col for col in df if (col.startswith(tuple(self.features_toselect)))]\n",
    "            return(df[filter_cols])    \n",
    "\n",
    "        else:\n",
    "            return(df)\n",
    "\n",
    "'''\n",
    "In order have less features globally: we Keep only features_tofilter that represent percent_tokeep% of total values\n",
    "Features which values represent less than percent_tokeep% will be set \"OTHERS\" value instead of their real value\n",
    "'''\n",
    "\n",
    "class Filter_High_Percentile(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_tofilter = ['ORIGIN', 'DEST'], percent_tokeep = 80):\n",
    "        self.features_tofilter = features_tofilter\n",
    "        self.percent_tokeep = percent_tokeep\n",
    "        self.high_percentile = None\n",
    "        self.low_percentile = None\n",
    "    \n",
    "    def fit(self, df, labels=None): \n",
    "        print('Fit high percentile filter...')\n",
    "        for feature_tofilter in self.features_tofilter:\n",
    "            # Get feature_tofilter values that represent 80% of data\n",
    "            self.high_percentile = ((((df[[feature_tofilter]].groupby(feature_tofilter).size() / len(df)).sort_values(ascending=False)) * 100).cumsum() < self.percent_tokeep).where(lambda x : x == True).dropna().index.values.tolist()\n",
    "            self.low_percentile = ((((df[[feature_tofilter]].groupby(feature_tofilter).size() / len(df)).sort_values(ascending=False)) * 100).cumsum() >= self.percent_tokeep).where(lambda x : x == True).dropna().index.values.tolist()\n",
    "\n",
    "            total = len(df[feature_tofilter].unique())\n",
    "            high_percentile_sum = len(self.high_percentile)\n",
    "            low_percentile_sum = len(self.low_percentile)\n",
    "            high_low_sum = high_percentile_sum + low_percentile_sum\n",
    "\n",
    "            print(f'Total number of {feature_tofilter} values : {total}')\n",
    "            print(f'Number of {feature_tofilter} high percentile (> {self.percent_tokeep}%) values : {high_percentile_sum}')\n",
    "            print(f'Number of {feature_tofilter} low percentile values : {low_percentile_sum}')\n",
    "            print(f'Sum of high percentile + low percentile values : {high_low_sum}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):       \n",
    "        if (self.features_tofilter != None):\n",
    "            print('Apply high percentile filter...')\n",
    "            \n",
    "            for feature_tofilter in self.features_tofilter:\n",
    "                df.loc[df[feature_tofilter].isin(self.low_percentile), feature_tofilter] = 'OTHERS'   \n",
    "            \n",
    "            return(df)    \n",
    "\n",
    "        else:\n",
    "            return(df)\n",
    "        \n",
    "'''\n",
    "conversion_pipeline = Pipeline([\n",
    "    ('data_converter', HHMM_to_Minutes()),\n",
    "    #('categoricalfeatures_1hotencoder', CategoricalFeatures1HotEncoder()),\n",
    "    #('standardscaler', preprocessing.StandardScaler()),\n",
    "])\n",
    "'''\n",
    "\n",
    "preparation_pipeline = Pipeline([\n",
    "    ('filter_highpercentile', Filter_High_Percentile()),\n",
    "    ('data_converter', HHMM_to_Minutes()),\n",
    "    ('categoricalfeatures_1hotencoder', CategoricalFeatures1HotEncoder()),\n",
    "    #('standardscaler', preprocessing.StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "# If matrix is sparse, with_mean=False must be passed to StandardScaler\n",
    "prediction_pipeline = Pipeline([\n",
    "    ('features_selector', FeaturesSelector(features_toselect=['ORIGIN','CRS_DEP_TIME','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','UNIQUE_CARRIER','DEST','CRS_ARR_TIME','DISTANCE','CRS_ELAPSED_TIME'])),\n",
    "    ('standardscaler', ColumnTransformer([\n",
    "        ('standardscaler_specific', StandardScaler(), ['CRS_DEP_TIME','MONTH','DAY_OF_MONTH', 'DAY_OF_WEEK', 'CRS_ARR_TIME', 'DISTANCE', 'CRS_ELAPSED_TIME'])\n",
    "    ], remainder='passthrough', sparse_threshold=1)),\n",
    "    #('predictor', To_Complete(predictor_params =  {'n_neighbors':6, 'algorithm':'ball_tree', 'metric':'minkowski'})),\n",
    "])\n",
    "#copy=False passed to StandardScaler() allows to gain memory\n",
    "\n",
    "'''\n",
    "# Old code that used scikit learn OneHotEncoder (which does not keep DataFrame type) instead of Pandas\n",
    "preparation_pipeline2 = Pipeline([\n",
    "    ('data_converter', HHMM_to_Minutes()),\n",
    "    ('multiple_encoder', ColumnTransformer([\n",
    "        ('categoricalfeatures_1hotencoder', OneHotEncoder(), ['ORIGIN', 'UNIQUE_CARRIER', 'DEST'])\n",
    "    ], remainder='passthrough')),\n",
    "    #('standardscaler', preprocessing.StandardScaler()),\n",
    "])\n",
    "'''\n",
    "\n",
    "'''\n",
    "ColumnTransformer([\n",
    "        ('standardscaler_specific', StandardScaler(), ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'])\n",
    "    ], remainder='passthrough')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit high percentile filter...\n",
      "Total number of ORIGIN values : 308\n",
      "Number of ORIGIN high percentile (> 80%) values : 45\n",
      "Number of ORIGIN low percentile values : 263\n",
      "Sum of high percentile + low percentile values : 308\n",
      "Total number of DEST values : 308\n",
      "Number of DEST high percentile (> 80%) values : 45\n",
      "Number of DEST low percentile values : 263\n",
      "Sum of high percentile + low percentile values : 308\n",
      "Apply high percentile filter...\n",
      "Converting feature CRS_DEP_TIME\n",
      "\n",
      "Converting feature CRS_ARR_TIME\n",
      "\n",
      "Fit data\n",
      "Transform data\n",
      "1hot encode categorical features...\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "type of df : <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_train_transformed = preparation_pipeline.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045, 115)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4993045 entries, 3352894 to 1692743\n",
      "Columns: 115 entries, CRS_DEP_TIME to DEST_TPA\n",
      "dtypes: Sparse[uint8, 0](104), float64(5), int64(5), object(1)\n",
      "memory usage: 528.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transformed = prediction_pipeline.fit_transform(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045, 111)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "sparse.issparse(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantitative features : ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'] \n",
      "\n",
      "Qualitative features : ['ORIGIN', 'CRS_DEP_TIME', 'UNIQUE_CARRIER', 'DEST', 'CRS_ARR_TIME', 'TAIL_NUM'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantitative_features, qualitative_features = identify_features(df, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045, 111)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[model1_label].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "df_train_transformed_sparse = sparse.csr_matrix(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4993045x111 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 49930450 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(df_train_transformed_sparse, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5547828, 14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (4993045, 111) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-fc51d3fcb729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlin_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlin_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel1_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_residues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/linalg/basic.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0mlwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_lwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlapack_lwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 x, s, rank, info = lapack_func(a1, b1, lwork,\n\u001b[0;32m-> 1211\u001b[0;31m                                                iwork, cond, False, False)\n\u001b[0m\u001b[1;32m   1212\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# complex data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 lwork, rwork, iwork = _compute_lwork(lapack_lwork, m, n,\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (4993045, 111) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(df_train_transformed, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply high percentile filter...\n",
      "Converting feature CRS_DEP_TIME\n",
      "\n",
      "Converting feature CRS_ARR_TIME\n",
      "\n",
      "Transform data\n",
      "1hot encode categorical features...\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "type of df : <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_test_transformed = preparation_pipeline.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_transformed = prediction_pipeline.transform(df_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_transformed_sparse = sparse.csr_matrix(df_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.16679389006135"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df_test_predictions = lin_reg.predict(df_test_transformed_sparse)\n",
    "lin_mse = mean_squared_error(df_test[model1_label], df_test_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df_test_predictions = lin_reg.predict(df_test_transformed)\n",
    "lin_mse = mean_squared_error(df_test[model1_label], df_test_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> With all samples and 70% most represented features, without StandardScale :  on test set : lin_rmse = 42.17  \n",
    "=> With all samples and 80% most represented features, without StandardScale :  on test set : lin_rmse = 42.16  \n",
    "=> With all samples and 80% most represented features, with StandardScale :  on test set : lin_rmse = 42.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn import linear_model\n",
    "\n",
    "regressor = linear_model.SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "regressor.fit(df_transformed, df_train[model1_label])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
