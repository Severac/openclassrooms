{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openclassrooms PJ4 : transats dataset : modelisation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import qgrid\n",
    "\n",
    "import glob\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "SAMPLED_DATA = False  # If True : data is sampled (1000 instances only) for faster testing purposes\n",
    "\n",
    "DATA_PATH = os.path.join(\"datasets\", \"transats\")\n",
    "DATA_PATH = os.path.join(DATA_PATH, \"out\")\n",
    "\n",
    "DATA_PATH_FILE_INPUT = os.path.join(DATA_PATH, \"transats_metadata_transformed.csv\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9] # Taille par défaut des figures de matplotlib\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#import common_functions\n",
    "\n",
    "####### Paramètres pour sauver et restaurer les modèles :\n",
    "import pickle\n",
    "####### Paramètres à changer par l'utilisateur selon son besoin :\n",
    "RECOMPUTE_GRIDSEARCH = True  # CAUTION : computation is several hours long\n",
    "SAVE_GRID_RESULTS = False # If True : grid results object will be saved to pickle files that have GRIDSEARCH_FILE_PREFIX\n",
    "LOAD_GRID_RESULTS = True # If True : grid results object will be loaded from pickle files that have GRIDSEARCH_FILE_PREFIX\n",
    "\n",
    "#GRIDSEARCH_CSV_FILE = 'grid_search_results.csv'\n",
    "\n",
    "GRIDSEARCH_FILE_PREFIX = 'grid_search_results_'\n",
    "\n",
    "EXECUTE_INTERMEDIATE_MODELS = False # If True: every intermediate model (which results are manually analyzed in the notebook) will be executed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qgrid_show(df):\n",
    "    display(qgrid.show_grid(df, grid_options={'forceFitColumns': False, 'defaultColumnWidth': 170}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_column_information(df, column_name):\n",
    "    column_type = df.dtypes[column_name]\n",
    "    print(f'Column {column_name}, type {column_type}\\n')\n",
    "    print('--------------------------')\n",
    "\n",
    "    print(df[[column_name]].groupby(column_name).size().sort_values(ascending=False))\n",
    "    print(df[column_name].unique())    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_percent_complete(df):\n",
    "    not_na = 100 - (df.isnull().sum() * 100 / len(df))\n",
    "    not_na_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                     'percent_complete': not_na}).sort_values(by='percent_complete', ascending=False)\n",
    "    display(not_na_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_features(df, all_features):\n",
    "    quantitative_features = []\n",
    "    qualitative_features = []\n",
    "    features_todrop = []\n",
    "\n",
    "    for feature_name in all_features:\n",
    "        if (df[feature_name].dtype == 'object'):\n",
    "            qualitative_features.append(feature_name)\n",
    "\n",
    "        else:\n",
    "            quantitative_features.append(feature_name)\n",
    "\n",
    "    print(f'Quantitative features : {quantitative_features} \\n')\n",
    "    print(f'Qualitative features : {qualitative_features} \\n')  \n",
    "    \n",
    "    return quantitative_features, qualitative_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_or_load_search_params(grid_search, save_file_suffix):\n",
    "    if (SAVE_GRID_RESULTS == True):\n",
    "        #df_grid_search_results = pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "        #df_grid_search_results.to_csv(GRIDSEARCH_CSV_FILE)\n",
    "\n",
    "        df_grid_search_results = pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"mean_test_score\"])],axis=1)\n",
    "        df_grid_search_results = pd.concat([df_grid_search_results,pd.DataFrame(grid_search.cv_results_[\"std_test_score\"], columns=[\"std_test_score\"])],axis=1)\n",
    "        df_grid_search_results = pd.concat([df_grid_search_results,pd.DataFrame(grid_search.cv_results_[\"mean_fit_time\"], columns=[\"mean_fit_time\"])],axis=1)\n",
    "        df_grid_search_results = pd.concat([df_grid_search_results,pd.DataFrame(grid_search.cv_results_[\"mean_score_time\"], columns=[\"mean_score_time\"])],axis=1)\n",
    "        df_grid_search_results.to_csv(GRIDSEARCH_FILE_PREFIX + save_file_suffix + '.csv')\n",
    "\n",
    "        with open(GRIDSEARCH_FILE_PREFIX + save_file_suffix + '.pickle', 'wb') as f:\n",
    "            pickle.dump(grid_search, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return(grid_search, df_grid_search_results)\n",
    "\n",
    "    if (LOAD_GRID_RESULTS == True):\n",
    "        if ((SAVE_GRID_RESULTS == True) or (RECOMPUTE_GRIDSEARCH == True)):\n",
    "            print('Error : if want to load grid results, you should not have saved them or recomputed them before, or you will loose all your training data')\n",
    "\n",
    "        else:\n",
    "            with open(GRIDSEARCH_FILE_PREFIX + save_file_suffix + '.pickle', 'rb') as f:\n",
    "                grid_search = pickle.load(f)\n",
    "\n",
    "            df_grid_search_results = pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"mean_test_score\"])],axis=1)\n",
    "            df_grid_search_results = pd.concat([df_grid_search_results,pd.DataFrame(grid_search.cv_results_[\"std_test_score\"], columns=[\"std_test_score\"])],axis=1)\n",
    "            df_grid_search_results = pd.concat([df_grid_search_results,pd.DataFrame(grid_search.cv_results_[\"mean_fit_time\"], columns=[\"mean_fit_time\"])],axis=1)\n",
    "            df_grid_search_results = pd.concat([df_grid_search_results,pd.DataFrame(grid_search.cv_results_[\"mean_score_time\"], columns=[\"mean_score_time\"])],axis=1)\n",
    "            \n",
    "            return(grid_search, df_grid_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test):\n",
    "    Y_predict = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, Y_predict)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'RMSE : {rmse}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhmm timed features formatted\n",
    "feats_hhmm = ['CRS_DEP_TIME',  'CRS_ARR_TIME']\n",
    "\n",
    "df = pd.read_csv(DATA_PATH_FILE_INPUT, sep=',', header=0, encoding='utf-8', low_memory=False, parse_dates=feats_hhmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5547828, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ORIGIN</td>\n",
       "      <td>ORIGIN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MONTH</td>\n",
       "      <td>MONTH</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DAY_OF_MONTH</td>\n",
       "      <td>DAY_OF_MONTH</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>UNIQUE_CARRIER</td>\n",
       "      <td>UNIQUE_CARRIER</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DEST</td>\n",
       "      <td>DEST</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DISTANCE</td>\n",
       "      <td>DISTANCE</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARR_DELAY</td>\n",
       "      <td>ARR_DELAY</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DEP_DELAY</td>\n",
       "      <td>DEP_DELAY</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TAXI_OUT</td>\n",
       "      <td>TAXI_OUT</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TAIL_NUM</td>\n",
       "      <td>TAIL_NUM</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       column_name  percent_complete\n",
       "ORIGIN                      ORIGIN             100.0\n",
       "CRS_DEP_TIME          CRS_DEP_TIME             100.0\n",
       "MONTH                        MONTH             100.0\n",
       "DAY_OF_MONTH          DAY_OF_MONTH             100.0\n",
       "DAY_OF_WEEK            DAY_OF_WEEK             100.0\n",
       "UNIQUE_CARRIER      UNIQUE_CARRIER             100.0\n",
       "DEST                          DEST             100.0\n",
       "CRS_ARR_TIME          CRS_ARR_TIME             100.0\n",
       "DISTANCE                  DISTANCE             100.0\n",
       "CRS_ELAPSED_TIME  CRS_ELAPSED_TIME             100.0\n",
       "ARR_DELAY                ARR_DELAY             100.0\n",
       "DEP_DELAY                DEP_DELAY             100.0\n",
       "TAXI_OUT                  TAXI_OUT             100.0\n",
       "TAIL_NUM                  TAIL_NUM             100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_percent_complete(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor column_name in df.columns:\\n    print_column_information(df, column_name)\\n    \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for column_name in df.columns:\n",
    "    print_column_information(df, column_name)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantitative features : ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'] \n",
      "\n",
      "Qualitative features : ['ORIGIN', 'CRS_DEP_TIME', 'UNIQUE_CARRIER', 'DEST', 'CRS_ARR_TIME', 'TAIL_NUM'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below are feature from dataset that we decided to keep: \n",
    "all_features = ['ORIGIN','CRS_DEP_TIME','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','UNIQUE_CARRIER','DEST','CRS_ARR_TIME','DISTANCE','CRS_ELAPSED_TIME','ARR_DELAY','DEP_DELAY', 'TAXI_OUT', 'TAIL_NUM']\n",
    "\n",
    "model1_features = ['ORIGIN','CRS_DEP_TIME','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','UNIQUE_CARRIER','DEST','CRS_ARR_TIME','DISTANCE','CRS_ELAPSED_TIME']\n",
    "model1_label = 'ARR_DELAY'\n",
    "\n",
    "quantitative_features = []\n",
    "qualitative_features = []\n",
    "features_todrop = []\n",
    "\n",
    "for feature_name in all_features:\n",
    "    if (df[feature_name].dtype == 'object'):\n",
    "        qualitative_features.append(feature_name)\n",
    "        \n",
    "    else:\n",
    "        quantitative_features.append(feature_name)\n",
    "\n",
    "print(f'Quantitative features : {quantitative_features} \\n')\n",
    "print(f'Qualitative features : {qualitative_features} \\n')        \n",
    "        \n",
    "\n",
    "#Commented out : no drop of features\n",
    "#for df_column in df.columns:\n",
    "#    if df_column not in all_features:\n",
    "#        features_todrop.append(df_column)\n",
    "#        \n",
    "#print(f'Features to drop : {features_todrop} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train set, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "df_train = df_train.copy()\n",
    "df_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SAMPLED_DATA == True):\n",
    "    df_train = df_train.sample(1200000).copy(deep=True)\n",
    "    df = df.loc[df_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3352894</td>\n",
       "      <td>HPN</td>\n",
       "      <td>1537</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>B6</td>\n",
       "      <td>PBI</td>\n",
       "      <td>1829</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>N339JB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268114</td>\n",
       "      <td>ORD</td>\n",
       "      <td>1220</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>UA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1509</td>\n",
       "      <td>925.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>N79402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2193866</td>\n",
       "      <td>SAN</td>\n",
       "      <td>1335</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>UA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1703</td>\n",
       "      <td>853.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>N17244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3874513</td>\n",
       "      <td>HDN</td>\n",
       "      <td>1620</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>OO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>1738</td>\n",
       "      <td>763.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>N785SK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2213800</td>\n",
       "      <td>ORD</td>\n",
       "      <td>0730</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>UA</td>\n",
       "      <td>BOS</td>\n",
       "      <td>1044</td>\n",
       "      <td>867.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>N801UA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570006</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1355</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>WN</td>\n",
       "      <td>LAX</td>\n",
       "      <td>1520</td>\n",
       "      <td>862.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>N455WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2234489</td>\n",
       "      <td>SFO</td>\n",
       "      <td>1725</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>VX</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2250</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>N625VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4926484</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1015</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>UA</td>\n",
       "      <td>SJU</td>\n",
       "      <td>1630</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>N67812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4304572</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1950</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>UA</td>\n",
       "      <td>DFW</td>\n",
       "      <td>2115</td>\n",
       "      <td>224.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>N834UA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692743</td>\n",
       "      <td>CHA</td>\n",
       "      <td>1621</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>OO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>1716</td>\n",
       "      <td>106.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>N427SW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4993045 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ORIGIN CRS_DEP_TIME  MONTH  DAY_OF_MONTH  DAY_OF_WEEK UNIQUE_CARRIER  \\\n",
       "3352894    HPN         1537      3            29            2             B6   \n",
       "1268114    ORD         1220      6            22            3             UA   \n",
       "2193866    SAN         1335     12            26            1             UA   \n",
       "3874513    HDN         1620      1            22            5             OO   \n",
       "2213800    ORD         0730     12            12            1             UA   \n",
       "...        ...          ...    ...           ...          ...            ...   \n",
       "1570006    DEN         1355      5             7            6             WN   \n",
       "2234489    SFO         1725     12            13            2             VX   \n",
       "4926484    IAH         1015     11            11            5             UA   \n",
       "4304572    IAH         1950     10             4            2             UA   \n",
       "1692743    CHA         1621      5            15            7             OO   \n",
       "\n",
       "        DEST CRS_ARR_TIME  DISTANCE  CRS_ELAPSED_TIME  ARR_DELAY  DEP_DELAY  \\\n",
       "3352894  PBI         1829    1056.0             172.0       10.0        1.0   \n",
       "1268114  IAH         1509     925.0             169.0      175.0      184.0   \n",
       "2193866  DEN         1703     853.0             148.0      -12.0        5.0   \n",
       "3874513  LAX         1738     763.0             138.0      -16.0      -10.0   \n",
       "2213800  BOS         1044     867.0             134.0       53.0       52.0   \n",
       "...      ...          ...       ...               ...        ...        ...   \n",
       "1570006  LAX         1520     862.0             145.0       13.0        5.0   \n",
       "2234489  DAL         2250    1476.0             205.0      -12.0       -5.0   \n",
       "4926484  SJU         1630    2007.0             255.0       -5.0        1.0   \n",
       "4304572  DFW         2115     224.0              85.0      -15.0       -2.0   \n",
       "1692743  ATL         1716     106.0              55.0       64.0       61.0   \n",
       "\n",
       "         TAXI_OUT TAIL_NUM  \n",
       "3352894       9.0   N339JB  \n",
       "1268114      23.0   N79402  \n",
       "2193866      20.0   N17244  \n",
       "3874513       9.0   N785SK  \n",
       "2213800      31.0   N801UA  \n",
       "...           ...      ...  \n",
       "1570006      16.0   N455WN  \n",
       "2234489      11.0   N625VA  \n",
       "4926484      20.0   N67812  \n",
       "4304572      18.0   N834UA  \n",
       "1692743      28.0   N427SW  \n",
       "\n",
       "[4993045 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.loc[df_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nColumnTransformer([\\n        ('standardscaler_specific', StandardScaler(), ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'])\\n    ], remainder='passthrough')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import statistics\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "'''\n",
    "Cette fonction fait un 1 hot encoding des features qui sont des catégories\n",
    "Old function not used anymore\n",
    "'''\n",
    "    \n",
    "def add_categorical_features_1hot(df, categorical_features_totransform):\n",
    "    #df.drop(labels=categorical_features_totransform, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #df_encoded = pd.get_dummies(df, columns=categorical_features_totransform, sparse=True)\n",
    "    \n",
    "    for feature_totransform in categorical_features_totransform:\n",
    "        print(f'Adding 1hot Feature : {feature_totransform}')\n",
    "        \n",
    "        print('First')\n",
    "        df_transformed = df[feature_totransform].str.get_dummies().add_prefix(feature_totransform +'_')   \n",
    "        \n",
    "        #df_new = pd.get_dummies(df, columns=['ORIGIN'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #df.drop(labels=feature_totransform, axis=1, inplace=True)\n",
    "        print('Second')\n",
    "        del df[feature_totransform]\n",
    "        \n",
    "        print('Third')\n",
    "        df = pd.concat([df, df_transformed], axis=1)\n",
    "        \n",
    "    return(df)\n",
    "\n",
    "\n",
    "class HHMM_to_Minutes(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_toconvert = ['CRS_DEP_TIME', 'CRS_ARR_TIME']):\n",
    "        self.features_toconvert = features_toconvert\n",
    "        return None\n",
    "    \n",
    "    def fit(self, df):      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):       \n",
    "        for feature_toconvert in self.features_toconvert:\n",
    "            print(f'Converting feature {feature_toconvert}\\n')\n",
    "            #print('1\\n')\n",
    "            df_concat = pd.concat([df[feature_toconvert].str.slice(start=0,stop=2, step=1),df[feature_toconvert].str.slice(start=2,stop=4, step=1)], axis=1).astype(int).copy(deep=True)\n",
    "                    \n",
    "            #print('2\\n')\n",
    "            df[feature_toconvert] = (df_concat.iloc[:, [0]] * 60 + df_concat.iloc[:, [1]])[feature_toconvert]\n",
    "            \n",
    "            #print('3\\n')\n",
    "        \n",
    "        return(df)\n",
    "\n",
    "    \n",
    "'''\n",
    "class CategoricalFeatures1HotEncoder_old(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_features_totransform=['ORIGIN', 'UNIQUE_CARRIER', 'DEST']):\n",
    "        self.categorical_features_totransform = categorical_features_totransform\n",
    "    \n",
    "    def fit(self, df, labels=None):      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        # /!\\ Array will not have the same shape if we fit an ensemble of samples that have less values than total dataset\n",
    "        df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=True)  # Sparse allows to gain memory. But then, standardscale must be with_mean=False\n",
    "        #df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=False)\n",
    "        print('type of df : ' + str(type(df_encoded)))\n",
    "        return(df_encoded)\n",
    "'''\n",
    "\n",
    "class CategoricalFeatures1HotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_features_totransform=['ORIGIN', 'UNIQUE_CARRIER', 'DEST']):\n",
    "        self.categorical_features_totransform = categorical_features_totransform\n",
    "        self.fitted = False\n",
    "        self.all_feature_values = {}\n",
    "        #self.df_encoded = None\n",
    "    \n",
    "    def fit(self, df, labels=None):      \n",
    "        print('Fit data')\n",
    "        for feature_name in self.categorical_features_totransform:\n",
    "            self.all_feature_values[feature_name] = feature_name + '_' + df[feature_name].unique()\n",
    "        \n",
    "        self.fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if (self.fitted == False):\n",
    "            self.fit(df)\n",
    "        \n",
    "        print('Transform data')\n",
    "        print('1hot encode categorical features...')\n",
    "        df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=True)  # Sparse allows to gain memory. But then, standardscale must be with_mean=False\n",
    "        #df_encoded = pd.get_dummies(df, columns=self.categorical_features_totransform, sparse=False)\n",
    "        \n",
    "        # Get category values that were in fitted data, but that are not in data to transform \n",
    "        for feature_name, feature_values in self.all_feature_values.items():\n",
    "            diff_columns = list(set(feature_values) - set(df_encoded.columns.tolist()))\n",
    "            print(f'Column values that were in fitted data but not in current data: {diff_columns}')\n",
    "        \n",
    "            if (len(diff_columns) > 0):\n",
    "                print('Adding those column with 0 values to the DataFrme...')\n",
    "                # Create columns with 0 for the above categories, in order to preserve same matrix shape between train et test set\n",
    "                zeros_dict = dict.fromkeys(diff_columns, 0)\n",
    "                df_encoded.assign(**zeros_dict)\n",
    "        \n",
    "        print('type of df : ' + str(type(df_encoded)))\n",
    "        return(df_encoded)\n",
    "    \n",
    "    \n",
    "class FeaturesSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_toselect = None):  # If None : every column is kept, nothing is done\n",
    "        self.features_toselect = features_toselect\n",
    "    \n",
    "    def fit(self, df, labels=None):      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):       \n",
    "        if (self.features_toselect != None):\n",
    "            filter_cols = [col for col in df if (col.startswith(tuple(self.features_toselect)))]\n",
    "            return(df[filter_cols])    \n",
    "\n",
    "        else:\n",
    "            return(df)\n",
    "\n",
    "'''\n",
    "In order have less features globally: we Keep only features_tofilter that represent percent_tokeep% of total values\n",
    "Features which values represent less than percent_tokeep% will be set \"OTHERS\" value instead of their real value\n",
    "'''\n",
    "\n",
    "class Filter_High_Percentile(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_tofilter = ['ORIGIN', 'DEST'], percent_tokeep = 80):\n",
    "        self.features_tofilter = features_tofilter\n",
    "        self.percent_tokeep = percent_tokeep\n",
    "        self.high_percentile = None\n",
    "        self.low_percentile = None\n",
    "    \n",
    "    def fit(self, df, labels=None): \n",
    "        print('Fit high percentile filter...')\n",
    "        for feature_tofilter in self.features_tofilter:\n",
    "            # Get feature_tofilter values that represent 80% of data\n",
    "            self.high_percentile = ((((df[[feature_tofilter]].groupby(feature_tofilter).size() / len(df)).sort_values(ascending=False)) * 100).cumsum() < self.percent_tokeep).where(lambda x : x == True).dropna().index.values.tolist()\n",
    "            self.low_percentile = ((((df[[feature_tofilter]].groupby(feature_tofilter).size() / len(df)).sort_values(ascending=False)) * 100).cumsum() >= self.percent_tokeep).where(lambda x : x == True).dropna().index.values.tolist()\n",
    "\n",
    "            total = len(df[feature_tofilter].unique())\n",
    "            high_percentile_sum = len(self.high_percentile)\n",
    "            low_percentile_sum = len(self.low_percentile)\n",
    "            high_low_sum = high_percentile_sum + low_percentile_sum\n",
    "\n",
    "            print(f'Total number of {feature_tofilter} values : {total}')\n",
    "            print(f'Number of {feature_tofilter} high percentile (> {self.percent_tokeep}%) values : {high_percentile_sum}')\n",
    "            print(f'Number of {feature_tofilter} low percentile values : {low_percentile_sum}')\n",
    "            print(f'Sum of high percentile + low percentile values : {high_low_sum}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):       \n",
    "        if (self.features_tofilter != None):\n",
    "            print('Apply high percentile filter...')\n",
    "            \n",
    "            for feature_tofilter in self.features_tofilter:\n",
    "                df.loc[df[feature_tofilter].isin(self.low_percentile), feature_tofilter] = 'OTHERS'   \n",
    "            \n",
    "            return(df)    \n",
    "\n",
    "        else:\n",
    "            return(df)\n",
    "        \n",
    "\n",
    "    \n",
    "class DenseToSparseConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):  # If None : every column is kept, nothing is done\n",
    "        return None\n",
    "    \n",
    "    def fit(self, matrix, labels=None):      \n",
    "        return self\n",
    "    \n",
    "    def transform(self, matrix):   \n",
    "        return(sparse.csr_matrix(matrix))\n",
    "\n",
    "        \n",
    "        \n",
    "'''\n",
    "conversion_pipeline = Pipeline([\n",
    "    ('data_converter', HHMM_to_Minutes()),\n",
    "    #('categoricalfeatures_1hotencoder', CategoricalFeatures1HotEncoder()),\n",
    "    #('standardscaler', preprocessing.StandardScaler()),\n",
    "])\n",
    "'''\n",
    "\n",
    "preparation_pipeline = Pipeline([\n",
    "    ('filter_highpercentile', Filter_High_Percentile()),\n",
    "    ('data_converter', HHMM_to_Minutes()),\n",
    "    ('categoricalfeatures_1hotencoder', CategoricalFeatures1HotEncoder()),\n",
    "    #('standardscaler', preprocessing.StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "# If matrix is sparse, with_mean=False must be passed to StandardScaler\n",
    "prediction_pipeline = Pipeline([\n",
    "    ('features_selector', FeaturesSelector(features_toselect=['ORIGIN','CRS_DEP_TIME','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','UNIQUE_CARRIER','DEST','CRS_ARR_TIME','DISTANCE','CRS_ELAPSED_TIME'])),\n",
    "    ('standardscaler', ColumnTransformer([\n",
    "        ('standardscaler_specific', StandardScaler(), ['CRS_DEP_TIME','MONTH','DAY_OF_MONTH', 'DAY_OF_WEEK', 'CRS_ARR_TIME', 'DISTANCE', 'CRS_ELAPSED_TIME'])\n",
    "    ], remainder='passthrough', sparse_threshold=1)),\n",
    "    \n",
    "    ('dense_to_sparse_converter', DenseToSparseConverter()),\n",
    "    #('predictor', To_Complete(predictor_params =  {'n_neighbors':6, 'algorithm':'ball_tree', 'metric':'minkowski'})),\n",
    "])\n",
    "#copy=False passed to StandardScaler() allows to gain memory\n",
    "\n",
    "'''\n",
    "# Old code that used scikit learn OneHotEncoder (which does not keep DataFrame type) instead of Pandas\n",
    "preparation_pipeline2 = Pipeline([\n",
    "    ('data_converter', HHMM_to_Minutes()),\n",
    "    ('multiple_encoder', ColumnTransformer([\n",
    "        ('categoricalfeatures_1hotencoder', OneHotEncoder(), ['ORIGIN', 'UNIQUE_CARRIER', 'DEST'])\n",
    "    ], remainder='passthrough')),\n",
    "    #('standardscaler', preprocessing.StandardScaler()),\n",
    "])\n",
    "'''\n",
    "\n",
    "'''\n",
    "ColumnTransformer([\n",
    "        ('standardscaler_specific', StandardScaler(), ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'])\n",
    "    ], remainder='passthrough')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit high percentile filter...\n",
      "Total number of ORIGIN values : 308\n",
      "Number of ORIGIN high percentile (> 80%) values : 45\n",
      "Number of ORIGIN low percentile values : 263\n",
      "Sum of high percentile + low percentile values : 308\n",
      "Total number of DEST values : 308\n",
      "Number of DEST high percentile (> 80%) values : 45\n",
      "Number of DEST low percentile values : 263\n",
      "Sum of high percentile + low percentile values : 308\n",
      "Apply high percentile filter...\n",
      "Converting feature CRS_DEP_TIME\n",
      "\n",
      "Converting feature CRS_ARR_TIME\n",
      "\n",
      "Fit data\n",
      "Transform data\n",
      "1hot encode categorical features...\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "type of df : <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_train_transformed = preparation_pipeline.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045, 115)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4993045 entries, 3352894 to 1692743\n",
      "Columns: 115 entries, CRS_DEP_TIME to DEST_TPA\n",
      "dtypes: Sparse[uint8, 0](104), float64(5), int64(5), object(1)\n",
      "memory usage: 528.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transformed = prediction_pipeline.fit_transform(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045, 111)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "sparse.issparse(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantitative features : ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'DISTANCE', 'CRS_ELAPSED_TIME', 'ARR_DELAY', 'DEP_DELAY', 'TAXI_OUT'] \n",
      "\n",
      "Qualitative features : ['ORIGIN', 'CRS_DEP_TIME', 'UNIQUE_CARRIER', 'DEST', 'CRS_ARR_TIME', 'TAIL_NUM'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantitative_features, qualitative_features = identify_features(df, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply high percentile filter...\n",
      "Converting feature CRS_DEP_TIME\n",
      "\n",
      "Converting feature CRS_ARR_TIME\n",
      "\n",
      "Transform data\n",
      "1hot encode categorical features...\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "Column values that were in fitted data but not in current data: []\n",
      "type of df : <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(554783, 111)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_transformed = preparation_pipeline.transform(df_test)\n",
    "df_test_transformed = prediction_pipeline.transform(df_test_transformed)\n",
    "df_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045, 111)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4993045,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[model1_label].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(df_train_transformed, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    df_test_predictions = lin_reg.predict(df_test_transformed)\n",
    "    lin_mse = mean_squared_error(df_test[model1_label], df_test_predictions)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    lin_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 42.17  (42.16679389006135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Comparison actual values / predict values')\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.scatter(df_test[model1_label], df_test_predictions, color='coral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#scores = cross_validate(lin_reg, df_train_transformed, df_train[model1_label], scoring='neg_root_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNET regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "shuffled_split_train = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "eNet = ElasticNet()\n",
    "\n",
    "grid_search = GridSearchCV(eNet, param_grid = {\"max_iter\": [1, 5, 10],\n",
    "                      \"alpha\": [10, 100],\n",
    "                      \"l1_ratio\": np.arange(0.0, 1.0, 0.4)},cv=shuffled_split_train, scoring='neg_mean_squared_error', error_score=np.nan, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\n\\neNet = ElasticNet()\\n\\ngrid_search = GridSearchCV(eNet, param_grid = {\"max_iter\": [1, 5, 10],\\n                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\\n                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)},cv=shuffled_split_train, scoring=\\'neg_mean_squared_error\\', error_score=np.nan, verbose=2)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "eNet = ElasticNet()\n",
    "\n",
    "grid_search = GridSearchCV(eNet, param_grid = {\"max_iter\": [1, 5, 10],\n",
    "                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)},cv=shuffled_split_train, scoring='neg_mean_squared_error', error_score=np.nan, verbose=2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (RECOMPUTE_GRIDSEARCH == True):\n",
    "    grid_search.fit(df_train_transformed, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search, df_grid_search_results = save_or_load_search_params(grid_search, 'eNet_20200319')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>l1_ratio</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1741.474494</td>\n",
       "      <td>11.553554</td>\n",
       "      <td>8.451748</td>\n",
       "      <td>0.021589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>-1741.606988</td>\n",
       "      <td>11.553293</td>\n",
       "      <td>19.897743</td>\n",
       "      <td>0.017433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1741.606988</td>\n",
       "      <td>11.553293</td>\n",
       "      <td>11.345686</td>\n",
       "      <td>0.018125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1746.266246</td>\n",
       "      <td>11.555358</td>\n",
       "      <td>3.253050</td>\n",
       "      <td>0.018168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1746.268263</td>\n",
       "      <td>11.555355</td>\n",
       "      <td>10.535562</td>\n",
       "      <td>0.017086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>-1746.268263</td>\n",
       "      <td>11.555355</td>\n",
       "      <td>19.410727</td>\n",
       "      <td>0.016518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>-1746.670284</td>\n",
       "      <td>11.555964</td>\n",
       "      <td>3.243109</td>\n",
       "      <td>0.018928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>-1746.670284</td>\n",
       "      <td>11.555964</td>\n",
       "      <td>3.316887</td>\n",
       "      <td>0.018247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1746.670284</td>\n",
       "      <td>11.555964</td>\n",
       "      <td>2.742426</td>\n",
       "      <td>0.017680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.613709</td>\n",
       "      <td>0.016329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.716009</td>\n",
       "      <td>0.018829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.635636</td>\n",
       "      <td>0.016907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.718752</td>\n",
       "      <td>0.017407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.728197</td>\n",
       "      <td>0.017755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.657087</td>\n",
       "      <td>0.016606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.710231</td>\n",
       "      <td>0.016306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.742407</td>\n",
       "      <td>0.016709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>-1746.913947</td>\n",
       "      <td>11.555634</td>\n",
       "      <td>2.641766</td>\n",
       "      <td>0.016149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  l1_ratio  max_iter  mean_test_score  std_test_score  mean_fit_time  \\\n",
       "0      10       0.0         1     -1741.474494       11.553554       8.451748   \n",
       "2      10       0.0        10     -1741.606988       11.553293      19.897743   \n",
       "1      10       0.0         5     -1741.606988       11.553293      11.345686   \n",
       "9     100       0.0         1     -1746.266246       11.555358       3.253050   \n",
       "10    100       0.0         5     -1746.268263       11.555355      10.535562   \n",
       "11    100       0.0        10     -1746.268263       11.555355      19.410727   \n",
       "4      10       0.4         5     -1746.670284       11.555964       3.243109   \n",
       "5      10       0.4        10     -1746.670284       11.555964       3.316887   \n",
       "3      10       0.4         1     -1746.670284       11.555964       2.742426   \n",
       "6      10       0.8         1     -1746.913947       11.555634       2.613709   \n",
       "7      10       0.8         5     -1746.913947       11.555634       2.716009   \n",
       "8      10       0.8        10     -1746.913947       11.555634       2.635636   \n",
       "12    100       0.4         1     -1746.913947       11.555634       2.718752   \n",
       "13    100       0.4         5     -1746.913947       11.555634       2.728197   \n",
       "14    100       0.4        10     -1746.913947       11.555634       2.657087   \n",
       "15    100       0.8         1     -1746.913947       11.555634       2.710231   \n",
       "16    100       0.8         5     -1746.913947       11.555634       2.742407   \n",
       "17    100       0.8        10     -1746.913947       11.555634       2.641766   \n",
       "\n",
       "    mean_score_time  \n",
       "0          0.021589  \n",
       "2          0.017433  \n",
       "1          0.018125  \n",
       "9          0.018168  \n",
       "10         0.017086  \n",
       "11         0.016518  \n",
       "4          0.018928  \n",
       "5          0.018247  \n",
       "3          0.017680  \n",
       "6          0.016329  \n",
       "7          0.018829  \n",
       "8          0.016907  \n",
       "12         0.017407  \n",
       "13         0.017755  \n",
       "14         0.016606  \n",
       "15         0.016306  \n",
       "16         0.016709  \n",
       "17         0.016149  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_search_results.sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.73092378560532"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(1741.47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 41.73092378560532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=10, copy_X=True, fit_intercept=True, l1_ratio=0.0, max_iter=1,\n",
       "           normalize=False, positive=False, precompute=False, random_state=None,\n",
       "           selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    if (RECOMPUTE_GRIDSEARCH == True):\n",
    "        df_test_predictions = grid_search.best_estimator_.predict(df_test_transformed)\n",
    "        mse = mean_squared_error(df_test[model1_label], df_test_predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'brier_score_loss',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "sorted(metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random value between min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193.8307281262264"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_random = np.random.randint(df['ARR_DELAY'].min(), df['ARR_DELAY'].max(), df_test['ARR_DELAY'].shape)\n",
    "naive_mse = mean_squared_error(df_test[model1_label], y_pred_random)\n",
    "naive_rmse = np.sqrt(naive_mse)\n",
    "naive_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always mean naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 42.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn import dummy\n",
    "\n",
    "dum = dummy.DummyRegressor(strategy='mean')\n",
    "\n",
    "# Entraînement\n",
    "dum.fit(df_train_transformed, df_train[model1_label])\n",
    "\n",
    "# Prédiction sur le jeu de test\n",
    "y_pred_dum = dum.predict(df_test_transformed)\n",
    "\n",
    "# Evaluate\n",
    "print(\"RMSE : {:.2f}\".format(np.sqrt(mean_squared_error(df_test[model1_label], y_pred_dum)) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2e5b4625d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_test[model1_label], y_pred_dum, color='coral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3746453    62.0\n",
       "1300593   -22.0\n",
       "4707023   -11.0\n",
       "1752505   -38.0\n",
       "2152691    -1.0\n",
       "           ... \n",
       "2882585    -8.0\n",
       "298857     -7.0\n",
       "4799315     5.0\n",
       "5392696     7.0\n",
       "236730     -4.0\n",
       "Name: ARR_DELAY, Length: 554783, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[model1_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.45500812, 3.45500812, 3.45500812, ..., 3.45500812, 3.45500812,\n",
       "       3.45500812])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.15843930273253"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ARR_DELAY'].abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> With all samples and 70% most represented features, without StandardScale :  on test set : lin_rmse = 42.17  \n",
    "=> With all samples and 80% most represented features, without StandardScale :  on test set : lin_rmse = 42.16  \n",
    "=> With all samples and 80% most represented features, with StandardScale :  on test set : lin_rmse = 42.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    random_reg = RandomForestRegressor(n_estimators=10, max_depth=2, random_state=42)\n",
    "    random_reg.fit(df_train_transformed, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    df_test_predictions = random_reg.predict(df_test_transformed)\n",
    "    mse = mean_squared_error(df_test[model1_label], df_test_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 42.373691516139964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.svm import SVR\\n\\nsvm_reg = SVR(kernel=\"rbf\", verbose=True)\\nsvm_reg.fit(df_train_transformed, df_train[model1_label])\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel=\"rbf\", verbose=True)\n",
    "svm_reg.fit(df_train_transformed, df_train[model1_label])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=0, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    svm_reg = LinearSVR(random_state=42, tol=1e-5, verbose=True)\n",
    "    svm_reg.fit(df_train_transformed, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 43.45607643335432\n"
     ]
    }
   ],
   "source": [
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    evaluate_model(svm_reg, df_test_transformed, df_test[model1_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> RMSE : 43.45607643335432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_SVR = GridSearchCV(svm_reg, param_grid = {\"epsilon\": [0, 0.5],\n",
    "                      \"C\": [1, 5, 10, 100, 1000],\n",
    "                      \"loss\": ['epsilon_insensitive', 'squared_epsilon_insensitive'],},cv=shuffled_split_train, scoring='neg_mean_squared_error', error_score=np.nan, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=1, epsilon=0, loss=epsilon_insensitive ........................\n",
      "[LibLinear][CV] ......... C=1, epsilon=0, loss=epsilon_insensitive, total=  44.4s\n",
      "[CV] C=1, epsilon=0, loss=epsilon_insensitive ........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   44.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][CV] ......... C=1, epsilon=0, loss=epsilon_insensitive, total=  45.2s\n",
      "[CV] C=1, epsilon=0, loss=epsilon_insensitive ........................\n",
      "[LibLinear][CV] ......... C=1, epsilon=0, loss=epsilon_insensitive, total=  45.1s\n",
      "[CV] C=1, epsilon=0, loss=epsilon_insensitive ........................\n",
      "[LibLinear][CV] ......... C=1, epsilon=0, loss=epsilon_insensitive, total=  42.1s\n",
      "[CV] C=1, epsilon=0, loss=epsilon_insensitive ........................\n",
      "[LibLinear][CV] ......... C=1, epsilon=0, loss=epsilon_insensitive, total=  41.5s\n",
      "[CV] C=1, epsilon=0, loss=squared_epsilon_insensitive ................\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francois/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=1, epsilon=0, loss=squared_epsilon_insensitive, total=18.0min\n",
      "[CV] C=1, epsilon=0, loss=squared_epsilon_insensitive ................\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francois/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=1, epsilon=0, loss=squared_epsilon_insensitive, total=18.7min\n",
      "[CV] C=1, epsilon=0, loss=squared_epsilon_insensitive ................\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francois/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=1, epsilon=0, loss=squared_epsilon_insensitive, total=19.2min\n",
      "[CV] C=1, epsilon=0, loss=squared_epsilon_insensitive ................\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francois/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=1, epsilon=0, loss=squared_epsilon_insensitive, total=19.2min\n",
      "[CV] C=1, epsilon=0, loss=squared_epsilon_insensitive ................\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francois/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . C=1, epsilon=0, loss=squared_epsilon_insensitive, total=18.4min\n",
      "[CV] C=1, epsilon=0.5, loss=epsilon_insensitive ......................\n",
      "[LibLinear][CV] ....... C=1, epsilon=0.5, loss=epsilon_insensitive, total=  42.3s\n",
      "[CV] C=1, epsilon=0.5, loss=epsilon_insensitive ......................\n",
      "[LibLinear][CV] ....... C=1, epsilon=0.5, loss=epsilon_insensitive, total=  43.1s\n",
      "[CV] C=1, epsilon=0.5, loss=epsilon_insensitive ......................\n",
      "[LibLinear][CV] ....... C=1, epsilon=0.5, loss=epsilon_insensitive, total=  42.4s\n",
      "[CV] C=1, epsilon=0.5, loss=epsilon_insensitive ......................\n",
      "[LibLinear][CV] ....... C=1, epsilon=0.5, loss=epsilon_insensitive, total=  43.1s\n",
      "[CV] C=1, epsilon=0.5, loss=epsilon_insensitive ......................\n",
      "[LibLinear][CV] ....... C=1, epsilon=0.5, loss=epsilon_insensitive, total=  43.7s\n",
      "[CV] C=1, epsilon=0.5, loss=squared_epsilon_insensitive ..............\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francois/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, epsilon=0.5, loss=squared_epsilon_insensitive, total=18.3min\n",
      "[CV] C=1, epsilon=0.5, loss=squared_epsilon_insensitive ..............\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francois/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, epsilon=0.5, loss=squared_epsilon_insensitive, total=18.4min\n",
      "[CV] C=1, epsilon=0.5, loss=squared_epsilon_insensitive ..............\n",
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "if (EXECUTE_INTERMEDIATE_MODELS == True):\n",
    "    if (RECOMPUTE_GRIDSEARCH == True):\n",
    "        grid_search_SVR.fit(df_train_transformed, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_SVR, df_grid_search_results = save_or_load_search_params(grid_search_SVR, 'LinearSVR_20200319')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid_search_results.sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(grid_search_SVR, df_test_transformed, df_test[model1_label])\n",
    "grid_search_SVR.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial features + linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_reg = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
    "                          ('linear', LinearRegression(fit_intercept=False))])\n",
    "\n",
    "polynomial_reg.fit(df_train_transformed, df_train[model1_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(polynomial_reg, df_test_transformed, df_test[model1_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex : unused code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn import linear_model\\n\\nregressor = linear_model.SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\\nregressor.fit(df_transformed, df_train[model1_label])\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn import linear_model\n",
    "\n",
    "regressor = linear_model.SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "regressor.fit(df_transformed, df_train[model1_label])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.svm import SVR\\n\\nsvm_reg = SVR(kernel=\"linear\")\\nsvm_reg.fit(df_train_transformed, df_train[model1_label])\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(df_train_transformed, df_train[model1_label])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_test_predictions = svm_reg.predict(df_test_transformed)\\nsvm_mse = mean_squared_error(df_test[model1_label], df_test_predictions)\\nsvm_rmse = np.sqrt(svm_mse)\\nsvm_rmse\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_test_predictions = svm_reg.predict(df_test_transformed)\n",
    "svm_mse = mean_squared_error(df_test[model1_label], df_test_predictions)\n",
    "svm_rmse = np.sqrt(svm_mse)\n",
    "svm_rmse\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import StratifiedShuffleSplit\\n\\nstratified_split_train = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "stratified_split_train = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
